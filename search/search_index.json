{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An open source framework for research in Embodied AI AllenAct is a modular and flexible learning framework designed with a focus on the unique requirements of Embodied-AI research. It provides first-class support for a growing collection of embodied environments, tasks and algorithms, provides reproductions of state-of-the-art models and includes extensive documentation, tutorials, start-up code, and pre-trained models. AllenAct is built and backed by the Allen Institute for AI (AI2) . AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. Quick Links # Website & Docs Github Install Tutorials AllenAct Paper Citation Features & Highlights # Support for multiple environments : Support for the iTHOR , RoboTHOR and Habitat embodied environments as well as for grid-worlds including MiniGrid . Task Abstraction : Tasks and environments are decoupled in AllenAct, enabling researchers to easily implement a large variety of tasks in the same environment. Algorithms : Support for a variety of on-policy algorithms including PPO , DD-PPO , A2C , Imitation Learning and DAgger as well as offline training such as offline IL. Sequential Algorithms : It is trivial to experiment with different sequences of training routines, which are often the key to successful policies. Simultaneous Losses : Easily combine various losses while training models (e.g. use an external self-supervised loss while optimizing a PPO loss). Multi-agent support : Support for multi-agent algorithms and tasks. Visualizations : Out of the box support to easily visualize first and third person views for agents as well as intermediate model tensors, integrated into Tensorboard. Pre-trained models : Code and models for a number of standard Embodied AI tasks. Tutorials : Start-up code and extensive tutorials to help ramp up to Embodied AI. First-class PyTorch support : One of the few RL frameworks to target PyTorch. Arbitrary action spaces : Supporting both discrete and continuous actions. Environments Tasks Algorithms iTHOR , RoboTHOR , Habitat , MiniGrid , OpenAI Gym PointNav , ObjectNav , MiniGrid tasks , Gym Box2D tasks A2C , PPO , DD-PPO , DAgger , Off-policy Imitation Contributions # We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in this repository is subject to formatting, documentation, and type-annotation guidelines. For more details, please see the our contribution guidelines . Acknowledgments # This work builds upon the pytorch-a2c-ppo-acktr library of Ilya Kostrikov and uses some data structures from FAIR's habitat-lab . We would like to thank Dustin Schwenk for his help for the public release of the framework. License # AllenAct is MIT licensed, as found in the LICENSE file. Team # AllenAct is an open-source project built by members of the PRIOR research group at the Allen Institute for Artificial Intelligence (AI2). Citation # If you use this work, please cite our paper : @article { AllenAct , author = {Luca Weihs and Jordi Salvador and Klemen Kotar and Unnat Jain and Kuo-Hao Zeng and Roozbeh Mottaghi and Aniruddha Kembhavi} , title = {AllenAct: A Framework for Embodied AI Research} , year = {2020} , journal = {arXiv preprint arXiv:2008.12760} , }","title":"Overview"},{"location":"#quick-links","text":"Website & Docs Github Install Tutorials AllenAct Paper Citation","title":"Quick Links"},{"location":"#features-highlights","text":"Support for multiple environments : Support for the iTHOR , RoboTHOR and Habitat embodied environments as well as for grid-worlds including MiniGrid . Task Abstraction : Tasks and environments are decoupled in AllenAct, enabling researchers to easily implement a large variety of tasks in the same environment. Algorithms : Support for a variety of on-policy algorithms including PPO , DD-PPO , A2C , Imitation Learning and DAgger as well as offline training such as offline IL. Sequential Algorithms : It is trivial to experiment with different sequences of training routines, which are often the key to successful policies. Simultaneous Losses : Easily combine various losses while training models (e.g. use an external self-supervised loss while optimizing a PPO loss). Multi-agent support : Support for multi-agent algorithms and tasks. Visualizations : Out of the box support to easily visualize first and third person views for agents as well as intermediate model tensors, integrated into Tensorboard. Pre-trained models : Code and models for a number of standard Embodied AI tasks. Tutorials : Start-up code and extensive tutorials to help ramp up to Embodied AI. First-class PyTorch support : One of the few RL frameworks to target PyTorch. Arbitrary action spaces : Supporting both discrete and continuous actions. Environments Tasks Algorithms iTHOR , RoboTHOR , Habitat , MiniGrid , OpenAI Gym PointNav , ObjectNav , MiniGrid tasks , Gym Box2D tasks A2C , PPO , DD-PPO , DAgger , Off-policy Imitation","title":"Features &amp; Highlights"},{"location":"#contributions","text":"We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in this repository is subject to formatting, documentation, and type-annotation guidelines. For more details, please see the our contribution guidelines .","title":"Contributions"},{"location":"#acknowledgments","text":"This work builds upon the pytorch-a2c-ppo-acktr library of Ilya Kostrikov and uses some data structures from FAIR's habitat-lab . We would like to thank Dustin Schwenk for his help for the public release of the framework.","title":"Acknowledgments"},{"location":"#license","text":"AllenAct is MIT licensed, as found in the LICENSE file.","title":"License"},{"location":"#team","text":"AllenAct is an open-source project built by members of the PRIOR research group at the Allen Institute for Artificial Intelligence (AI2).","title":"Team"},{"location":"#citation","text":"If you use this work, please cite our paper : @article { AllenAct , author = {Luca Weihs and Jordi Salvador and Klemen Kotar and Unnat Jain and Kuo-Hao Zeng and Roozbeh Mottaghi and Aniruddha Kembhavi} , title = {AllenAct: A Framework for Embodied AI Research} , year = {2020} , journal = {arXiv preprint arXiv:2008.12760} , }","title":"Citation"},{"location":"CONTRIBUTING/","text":"Contributing # We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in pull requests should adhere to the following guidelines. Found a bug or want to suggest an enhancement? # Please submit an issue in which you note the steps to reproduce the bug or in which you detail the enhancement. Making a pull request? # When making a pull request we require that any code respects several guidelines detailed below. Auto-formatting # All python code in this repository should be formatted using black . To use black auto-formatting across all files, simply run bash scripts/auto_format.sh which will run black auto-formatting as well as docformatter (used to auto-format documentation strings). Type-checking # Our code makes liberal use of type hints. If you have not had experience with type hinting in python we recommend reading the documentation of the typing python module or the simplified introduction to type hints found here . All methods should have typed arguments and output. Furthermore we use mypy to perform basic static type checking. Before making a pull request, there should be no warnings or errors when running dmypy run -- --follow-imports = skip . Explicitly ignoring type checking (for instance using # type: ignore ) should be only be done when it would otherwise be an extensive burden. Setting up pre-commit hooks (optional) # Pre-commit hooks check that, when you attempt to commit changes, your code adheres a number of formatting and type-checking guidelines. Pull requests containing code not adhering to these guidelines will not be accepted and thus we recommend installing these pre-commit hooks. Assuming you have installed all of the project requirements, you can install our recommended pre-commit hooks by running (from this project's root directory) pre-commit install After running the above, each time you run git commit ... a set of pre-commit checks will be run.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in pull requests should adhere to the following guidelines.","title":"Contributing"},{"location":"CONTRIBUTING/#found-a-bug-or-want-to-suggest-an-enhancement","text":"Please submit an issue in which you note the steps to reproduce the bug or in which you detail the enhancement.","title":"Found a bug or want to suggest an enhancement?"},{"location":"CONTRIBUTING/#making-a-pull-request","text":"When making a pull request we require that any code respects several guidelines detailed below.","title":"Making a pull request?"},{"location":"CONTRIBUTING/#auto-formatting","text":"All python code in this repository should be formatted using black . To use black auto-formatting across all files, simply run bash scripts/auto_format.sh which will run black auto-formatting as well as docformatter (used to auto-format documentation strings).","title":"Auto-formatting"},{"location":"CONTRIBUTING/#type-checking","text":"Our code makes liberal use of type hints. If you have not had experience with type hinting in python we recommend reading the documentation of the typing python module or the simplified introduction to type hints found here . All methods should have typed arguments and output. Furthermore we use mypy to perform basic static type checking. Before making a pull request, there should be no warnings or errors when running dmypy run -- --follow-imports = skip . Explicitly ignoring type checking (for instance using # type: ignore ) should be only be done when it would otherwise be an extensive burden.","title":"Type-checking"},{"location":"CONTRIBUTING/#setting-up-pre-commit-hooks-optional","text":"Pre-commit hooks check that, when you attempt to commit changes, your code adheres a number of formatting and type-checking guidelines. Pull requests containing code not adhering to these guidelines will not be accepted and thus we recommend installing these pre-commit hooks. Assuming you have installed all of the project requirements, you can install our recommended pre-commit hooks by running (from this project's root directory) pre-commit install After running the above, each time you run git commit ... a set of pre-commit checks will be run.","title":"Setting up pre-commit hooks (optional)"},{"location":"FAQ/","text":"FAQ # How do I file a bug regarding the code or documentation? # Please file bugs by submitting an issue . We also welcome contributions from the community, including new features and bugfixes on existing functionality. Please refer to our contribution guidelines . How do I generate documentation? # Documentation is generated using mkdoc and pydoc-markdown . Building documentation locally # The mkdocs command used to build our documentation relies on all documentation existing as subdirectories of the docs folder. To ensure that all relevant markdown files are placed into this directory, you should always run bash scripts/build_docs.sh from the top-level project directory before running any of the mkdocs commands below. If you have made no changes to the documentation and only wish to build documentation on your local machine, run the following from within the allenact root directory. Note: This will generate HTML documentation within the site folder mkdocs build Serving documentation locally # If you have made no changes to the documentation and only wish to serve documentation on your local machine (with live reloading of modified documentation), run the following from within the allenact root directory. mkdocs serve Then navigate to http://127.0.0.1:8000/ Modifying and serving documentation locally # If you have made changes to the documentation, you will need to run a documentation builder script before you serve it on your local machine. bash scripts/build_docs.sh mkdocs serve Then navigate to http://127.0.0.1:8000/ Alternatively, the site directory (once built) can be served as a static webpage on your local machine without installing any dependencies by running python -m http.server 8000 from within the site directory.","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#how-do-i-file-a-bug-regarding-the-code-or-documentation","text":"Please file bugs by submitting an issue . We also welcome contributions from the community, including new features and bugfixes on existing functionality. Please refer to our contribution guidelines .","title":"How do I file a bug regarding the code or documentation?"},{"location":"FAQ/#how-do-i-generate-documentation","text":"Documentation is generated using mkdoc and pydoc-markdown .","title":"How do I generate documentation?"},{"location":"FAQ/#building-documentation-locally","text":"The mkdocs command used to build our documentation relies on all documentation existing as subdirectories of the docs folder. To ensure that all relevant markdown files are placed into this directory, you should always run bash scripts/build_docs.sh from the top-level project directory before running any of the mkdocs commands below. If you have made no changes to the documentation and only wish to build documentation on your local machine, run the following from within the allenact root directory. Note: This will generate HTML documentation within the site folder mkdocs build","title":"Building documentation locally"},{"location":"FAQ/#serving-documentation-locally","text":"If you have made no changes to the documentation and only wish to serve documentation on your local machine (with live reloading of modified documentation), run the following from within the allenact root directory. mkdocs serve Then navigate to http://127.0.0.1:8000/","title":"Serving documentation locally"},{"location":"FAQ/#modifying-and-serving-documentation-locally","text":"If you have made changes to the documentation, you will need to run a documentation builder script before you serve it on your local machine. bash scripts/build_docs.sh mkdocs serve Then navigate to http://127.0.0.1:8000/ Alternatively, the site directory (once built) can be served as a static webpage on your local machine without installing any dependencies by running python -m http.server 8000 from within the site directory.","title":"Modifying and serving documentation locally"},{"location":"LICENSE/","text":"MIT License Original work Copyright (c) 2017 Ilya Kostrikov Original work Copyright (c) Facebook, Inc. and its affiliates. Modified work Copyright (c) 2020 Allen Institute for Artificial Intelligence Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Licence"},{"location":"api/constants/","text":"constants # [view_source]","title":"constants"},{"location":"api/constants/#constants","text":"[view_source]","title":"constants"},{"location":"api/allenact/_constants/","text":"allenact._constants # [view_source]","title":"_constants"},{"location":"api/allenact/_constants/#allenact_constants","text":"[view_source]","title":"allenact._constants"},{"location":"api/allenact/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/","text":"allenact.algorithms.offpolicy_sync.losses.abstract_offpolicy_loss # [view_source] Defining abstract loss classes for actor critic models. AbstractOffPolicyLoss # class AbstractOffPolicyLoss ( Generic [ ModelType ], Loss ) [view_source] Abstract class representing an off-policy loss function used to train a model. AbstractOffPolicyLoss.loss # | @abc . abstractmethod | loss ( step_count : int , model : ModelType , batch : ObservationType , memory : Memory , * args , ** kwargs , * , ,) -> Tuple [ torch . FloatTensor , Dict [ str , float ], Memory , int ] [view_source] Computes the loss. Loss after processing a batch of data with (part of) a model (possibly with memory). Parameters model : model to run on data batch (both assumed to be on the same device) batch : data to use as input for model (already on the same device as model) memory : model memory before processing current data batch Returns A tuple with : current_loss : total loss current_info : additional information about the current loss memory : model memory after processing current data batch bsize : batch size","title":"abstract_offpolicy_loss"},{"location":"api/allenact/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/#allenactalgorithmsoffpolicy_synclossesabstract_offpolicy_loss","text":"[view_source] Defining abstract loss classes for actor critic models.","title":"allenact.algorithms.offpolicy_sync.losses.abstract_offpolicy_loss"},{"location":"api/allenact/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/#abstractoffpolicyloss","text":"class AbstractOffPolicyLoss ( Generic [ ModelType ], Loss ) [view_source] Abstract class representing an off-policy loss function used to train a model.","title":"AbstractOffPolicyLoss"},{"location":"api/allenact/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/#abstractoffpolicylossloss","text":"| @abc . abstractmethod | loss ( step_count : int , model : ModelType , batch : ObservationType , memory : Memory , * args , ** kwargs , * , ,) -> Tuple [ torch . FloatTensor , Dict [ str , float ], Memory , int ] [view_source] Computes the loss. Loss after processing a batch of data with (part of) a model (possibly with memory). Parameters model : model to run on data batch (both assumed to be on the same device) batch : data to use as input for model (already on the same device as model) memory : model memory before processing current data batch Returns A tuple with : current_loss : total loss current_info : additional information about the current loss memory : model memory after processing current data batch bsize : batch size","title":"AbstractOffPolicyLoss.loss"},{"location":"api/allenact/algorithms/onpolicy_sync/engine/","text":"allenact.algorithms.onpolicy_sync.engine # [view_source] Defines the reinforcement learning OnPolicyRLEngine . OnPolicyRLEngine # class OnPolicyRLEngine ( object ) [view_source] The reinforcement learning primary controller. This OnPolicyRLEngine class handles all training, validation, and testing as well as logging and checkpointing. You are not expected to instantiate this class yourself, instead you should define an experiment which will then be used to instantiate an OnPolicyRLEngine and perform any desired tasks. OnPolicyRLEngine.__init__ # | __init__ ( experiment_name : str , config : ExperimentConfig , results_queue : mp . Queue , checkpoints_queue : Optional [ | mp . Queue | ], checkpoints_dir : str , mode : str = \"train\" , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , mp_ctx : Optional [ BaseContext ] = None , worker_id : int = 0 , num_workers : int = 1 , device : Union [ str , torch . device , int ] = \"cpu\" , distributed_port : int = 0 , deterministic_agents : bool = False , max_sampler_processes_per_worker : Optional [ int ] = None , initial_model_state_dict : Optional [ Union [ Dict [ str , Any ], int ]] = None , ** kwargs , ,) [view_source] Initializer. Parameters config : The ExperimentConfig defining the experiment to run. output_dir : Root directory at which checkpoints and logs should be saved. seed : Seed used to encourage deterministic behavior (it is difficult to ensure completely deterministic behavior due to CUDA issues and nondeterminism in environments). mode : \"train\", \"valid\", or \"test\". deterministic_cudnn : Whether or not to use deterministic cudnn. If True this may lower training performance this is necessary (but not sufficient) if you desire deterministic behavior. extra_tag : An additional label to add to the experiment when saving tensorboard logs. OnPolicyRLEngine.worker_seeds # | @staticmethod | worker_seeds ( nprocesses : int , initial_seed : Optional [ int ]) -> List [ int ] [view_source] Create a collection of seeds for workers without modifying the RNG state. OnPolicyRLEngine.probe # | probe ( dones : List [ bool ], npaused , period = 100000 ) [view_source] Debugging util. When called from self.collect_rollout_step(...), calls render for the 0-th task sampler of the 0-th distributed worker for the first beginning episode spaced at least period steps from the beginning of the previous one. For valid, train, it currently renders all episodes for the 0-th task sampler of the 0-th distributed worker. If this is not wanted, it must be hard-coded for now below. Arguments : dones : dones list from self.collect_rollout_step(...) npaused : number of newly paused tasks returned by self.removed_paused(...) period : minimal spacing in sampled steps between the beginning of episodes to be shown.","title":"engine"},{"location":"api/allenact/algorithms/onpolicy_sync/engine/#allenactalgorithmsonpolicy_syncengine","text":"[view_source] Defines the reinforcement learning OnPolicyRLEngine .","title":"allenact.algorithms.onpolicy_sync.engine"},{"location":"api/allenact/algorithms/onpolicy_sync/engine/#onpolicyrlengine","text":"class OnPolicyRLEngine ( object ) [view_source] The reinforcement learning primary controller. This OnPolicyRLEngine class handles all training, validation, and testing as well as logging and checkpointing. You are not expected to instantiate this class yourself, instead you should define an experiment which will then be used to instantiate an OnPolicyRLEngine and perform any desired tasks.","title":"OnPolicyRLEngine"},{"location":"api/allenact/algorithms/onpolicy_sync/engine/#onpolicyrlengine__init__","text":"| __init__ ( experiment_name : str , config : ExperimentConfig , results_queue : mp . Queue , checkpoints_queue : Optional [ | mp . Queue | ], checkpoints_dir : str , mode : str = \"train\" , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , mp_ctx : Optional [ BaseContext ] = None , worker_id : int = 0 , num_workers : int = 1 , device : Union [ str , torch . device , int ] = \"cpu\" , distributed_port : int = 0 , deterministic_agents : bool = False , max_sampler_processes_per_worker : Optional [ int ] = None , initial_model_state_dict : Optional [ Union [ Dict [ str , Any ], int ]] = None , ** kwargs , ,) [view_source] Initializer. Parameters config : The ExperimentConfig defining the experiment to run. output_dir : Root directory at which checkpoints and logs should be saved. seed : Seed used to encourage deterministic behavior (it is difficult to ensure completely deterministic behavior due to CUDA issues and nondeterminism in environments). mode : \"train\", \"valid\", or \"test\". deterministic_cudnn : Whether or not to use deterministic cudnn. If True this may lower training performance this is necessary (but not sufficient) if you desire deterministic behavior. extra_tag : An additional label to add to the experiment when saving tensorboard logs.","title":"OnPolicyRLEngine.__init__"},{"location":"api/allenact/algorithms/onpolicy_sync/engine/#onpolicyrlengineworker_seeds","text":"| @staticmethod | worker_seeds ( nprocesses : int , initial_seed : Optional [ int ]) -> List [ int ] [view_source] Create a collection of seeds for workers without modifying the RNG state.","title":"OnPolicyRLEngine.worker_seeds"},{"location":"api/allenact/algorithms/onpolicy_sync/engine/#onpolicyrlengineprobe","text":"| probe ( dones : List [ bool ], npaused , period = 100000 ) [view_source] Debugging util. When called from self.collect_rollout_step(...), calls render for the 0-th task sampler of the 0-th distributed worker for the first beginning episode spaced at least period steps from the beginning of the previous one. For valid, train, it currently renders all episodes for the 0-th task sampler of the 0-th distributed worker. If this is not wanted, it must be hard-coded for now below. Arguments : dones : dones list from self.collect_rollout_step(...) npaused : number of newly paused tasks returned by self.removed_paused(...) period : minimal spacing in sampled steps between the beginning of episodes to be shown.","title":"OnPolicyRLEngine.probe"},{"location":"api/allenact/algorithms/onpolicy_sync/policy/","text":"allenact.algorithms.onpolicy_sync.policy # [view_source] ActorCriticModel # class ActorCriticModel ( Generic [ DistributionType ], nn . Module ) [view_source] Abstract class defining a deep (recurrent) actor critic agent. When defining a new agent, you should subclass this class and implement the abstract methods. Attributes action_space : The space of actions available to the agent. This is of type gym.spaces.Space . observation_space : The observation space expected by the agent. This is of type gym.spaces.dict . ActorCriticModel.__init__ # | __init__ ( action_space : gym . Space , observation_space : SpaceDict ) [view_source] Initializer. Parameters action_space : The space of actions available to the agent. observation_space : The observation space expected by the agent. ActorCriticModel.recurrent_memory_specification # | @property | recurrent_memory_specification () -> Optional [ FullMemorySpecType ] [view_source] The memory specification for the ActorCriticModel . See docs for _recurrent_memory_shape Returns The memory specification from _recurrent_memory_shape . ActorCriticModel.forward # | @abc . abstractmethod | forward ( observations : ObservationType , memory : Memory , prev_actions : ActionType , masks : torch . FloatTensor ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]] [view_source] Transforms input observations (& previous hidden state) into action probabilities and the state value. Parameters observations : Multi-level map from key strings to tensors of shape [steps, samplers, (agents,) ...] with the current observations. memory : Memory object with recurrent memory. The shape of each tensor is determined by the corresponding entry in _recurrent_memory_specification . prev_actions : ActionType with tensors of shape [steps, samplers, ...] with the previous actions. masks : tensor of shape [steps, samplers, agents, 1] with zeros indicating steps where a new episode/task starts. Returns A tuple whose first element is an object of class ActorCriticOutput which stores the agents' probability distribution over possible actions (shape [steps, samplers, ...]), the agents' value for the state (shape [steps, samplers, ..., 1]), and any extra information needed for loss computations. The second element is an optional Memory , which is only used in models with recurrent memory.","title":"policy"},{"location":"api/allenact/algorithms/onpolicy_sync/policy/#allenactalgorithmsonpolicy_syncpolicy","text":"[view_source]","title":"allenact.algorithms.onpolicy_sync.policy"},{"location":"api/allenact/algorithms/onpolicy_sync/policy/#actorcriticmodel","text":"class ActorCriticModel ( Generic [ DistributionType ], nn . Module ) [view_source] Abstract class defining a deep (recurrent) actor critic agent. When defining a new agent, you should subclass this class and implement the abstract methods. Attributes action_space : The space of actions available to the agent. This is of type gym.spaces.Space . observation_space : The observation space expected by the agent. This is of type gym.spaces.dict .","title":"ActorCriticModel"},{"location":"api/allenact/algorithms/onpolicy_sync/policy/#actorcriticmodel__init__","text":"| __init__ ( action_space : gym . Space , observation_space : SpaceDict ) [view_source] Initializer. Parameters action_space : The space of actions available to the agent. observation_space : The observation space expected by the agent.","title":"ActorCriticModel.__init__"},{"location":"api/allenact/algorithms/onpolicy_sync/policy/#actorcriticmodelrecurrent_memory_specification","text":"| @property | recurrent_memory_specification () -> Optional [ FullMemorySpecType ] [view_source] The memory specification for the ActorCriticModel . See docs for _recurrent_memory_shape Returns The memory specification from _recurrent_memory_shape .","title":"ActorCriticModel.recurrent_memory_specification"},{"location":"api/allenact/algorithms/onpolicy_sync/policy/#actorcriticmodelforward","text":"| @abc . abstractmethod | forward ( observations : ObservationType , memory : Memory , prev_actions : ActionType , masks : torch . FloatTensor ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]] [view_source] Transforms input observations (& previous hidden state) into action probabilities and the state value. Parameters observations : Multi-level map from key strings to tensors of shape [steps, samplers, (agents,) ...] with the current observations. memory : Memory object with recurrent memory. The shape of each tensor is determined by the corresponding entry in _recurrent_memory_specification . prev_actions : ActionType with tensors of shape [steps, samplers, ...] with the previous actions. masks : tensor of shape [steps, samplers, agents, 1] with zeros indicating steps where a new episode/task starts. Returns A tuple whose first element is an object of class ActorCriticOutput which stores the agents' probability distribution over possible actions (shape [steps, samplers, ...]), the agents' value for the state (shape [steps, samplers, ..., 1]), and any extra information needed for loss computations. The second element is an optional Memory , which is only used in models with recurrent memory.","title":"ActorCriticModel.forward"},{"location":"api/allenact/algorithms/onpolicy_sync/runner/","text":"allenact.algorithms.onpolicy_sync.runner # [view_source] Defines the reinforcement learning OnPolicyRunner .","title":"runner"},{"location":"api/allenact/algorithms/onpolicy_sync/runner/#allenactalgorithmsonpolicy_syncrunner","text":"[view_source] Defines the reinforcement learning OnPolicyRunner .","title":"allenact.algorithms.onpolicy_sync.runner"},{"location":"api/allenact/algorithms/onpolicy_sync/storage/","text":"allenact.algorithms.onpolicy_sync.storage # [view_source] RolloutStorage # class RolloutStorage ( object ) [view_source] Class for storing rollout information for RL trainers. RolloutStorage.narrow # | narrow () [view_source] This function is used by the training engine (in decentralized distributed settings) to temporarily narrow the step dimension in the storage. The reverse operation, unnarrow , is automatically called by after_update .","title":"storage"},{"location":"api/allenact/algorithms/onpolicy_sync/storage/#allenactalgorithmsonpolicy_syncstorage","text":"[view_source]","title":"allenact.algorithms.onpolicy_sync.storage"},{"location":"api/allenact/algorithms/onpolicy_sync/storage/#rolloutstorage","text":"class RolloutStorage ( object ) [view_source] Class for storing rollout information for RL trainers.","title":"RolloutStorage"},{"location":"api/allenact/algorithms/onpolicy_sync/storage/#rolloutstoragenarrow","text":"| narrow () [view_source] This function is used by the training engine (in decentralized distributed settings) to temporarily narrow the step dimension in the storage. The reverse operation, unnarrow , is automatically called by after_update .","title":"RolloutStorage.narrow"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/","text":"allenact.algorithms.onpolicy_sync.vector_sampled_tasks # [view_source] VectorSampledTasks # class VectorSampledTasks ( object ) [view_source] Vectorized collection of tasks. Creates multiple processes where each process runs its own TaskSampler. Each process generates one Task from its TaskSampler at a time and this class allows for interacting with these tasks in a vectorized manner. When a task on a process completes, the process samples another task from its task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks. multiprocessing_start_method : the multiprocessing method used to spawn worker processes. Valid methods are {'spawn', 'forkserver', 'fork'} 'forkserver' is the recommended method as it works well with CUDA. If 'fork' is used, the subproccess must be started before any other GPU useage. VectorSampledTasks.is_closed # | @property | is_closed () -> bool [view_source] Has the vector task been closed. VectorSampledTasks.num_unpaused_tasks # | @property | num_unpaused_tasks () -> int [view_source] Number of unpaused processes. Returns Number of unpaused processes. VectorSampledTasks.mp_ctx # | @property | mp_ctx () [view_source] Get the multiprocessing process used by the vector task. Returns The multiprocessing context. VectorSampledTasks.next_task # | next_task ( ** kwargs ) [view_source] Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks. VectorSampledTasks.get_observations # | get_observations () [view_source] Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks. VectorSampledTasks.command_at # | command_at ( sampler_index : int , command : str , data : Optional [ Any ] = None ) -> Any [view_source] Runs the command on the selected task and returns the result. Parameters Returns Result of the command. VectorSampledTasks.call_at # | call_at ( sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function. VectorSampledTasks.next_task_at # | next_task_at ( sampler_index : int ) -> List [ RLStepResult ] [view_source] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the process to be reset. Returns List of length one containing the observations the newly sampled task. VectorSampledTasks.step_at # | step_at ( sampler_index : int , action : Any ) -> List [ RLStepResult ] [view_source] Step in the index_process task in the vector. Parameters sampler_index : Index of the sampler to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process. VectorSampledTasks.async_step # | async_step ( actions : Sequence [ Any ]) -> None [view_source] Asynchronously step in the vectorized Tasks. Parameters actions : actions to be performed in the vectorized Tasks. VectorSampledTasks.wait_step # | wait_step () -> List [ Dict [ str , Any ]] [view_source] Wait until all the asynchronized processes have synchronized. VectorSampledTasks.step # | step ( actions : Sequence [ Any ]) [view_source] Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks. VectorSampledTasks.reset_all # | reset_all () [view_source] Reset all task samplers to their initial state (except for the RNG seed). VectorSampledTasks.set_seeds # | set_seeds ( seeds : List [ int ]) [view_source] Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds. VectorSampledTasks.pause_at # | pause_at ( sampler_index : int ) -> None [view_source] Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one. VectorSampledTasks.resume_all # | resume_all () -> None [view_source] Resumes any paused processes. VectorSampledTasks.call # | call ( function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None ) -> List [ Any ] [view_source] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions. VectorSampledTasks.attr_at # | attr_at ( sampler_index : int , attr_name : str ) -> Any [view_source] Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function. VectorSampledTasks.attr # | attr ( attr_names : Union [ List [ str ], str ]) -> List [ Any ] [view_source] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions. VectorSampledTasks.render # | render ( mode : str = \"human\" , * args , ** kwargs ) -> Union [ np . ndarray , None , List [ np . ndarray ]] [view_source] Render observations from all Tasks in a tiled image or list of images. SingleProcessVectorSampledTasks # class SingleProcessVectorSampledTasks ( object ) [view_source] Vectorized collection of tasks. Simultaneously handles the state of multiple TaskSamplers and their associated tasks. Allows for interacting with these tasks in a vectorized manner. When a task completes, another task is sampled from the appropriate task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks. SingleProcessVectorSampledTasks.is_closed # | @property | is_closed () -> bool [view_source] Has the vector task been closed. SingleProcessVectorSampledTasks.num_unpaused_tasks # | @property | num_unpaused_tasks () -> int [view_source] Number of unpaused processes. Returns Number of unpaused processes. SingleProcessVectorSampledTasks.next_task # | next_task ( ** kwargs ) [view_source] Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks. SingleProcessVectorSampledTasks.get_observations # | get_observations () [view_source] Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks. SingleProcessVectorSampledTasks.next_task_at # | next_task_at ( index_process : int ) -> List [ RLStepResult ] [view_source] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the generator to be reset. Returns List of length one containing the observations the newly sampled task. SingleProcessVectorSampledTasks.step_at # | step_at ( index_process : int , action : int ) -> List [ RLStepResult ] [view_source] Step in the index_process task in the vector. Parameters index_process : Index of the process to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process. SingleProcessVectorSampledTasks.step # | step ( actions : List [ List [ int ]]) [view_source] Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks. SingleProcessVectorSampledTasks.reset_all # | reset_all () [view_source] Reset all task samplers to their initial state (except for the RNG seed). SingleProcessVectorSampledTasks.set_seeds # | set_seeds ( seeds : List [ int ]) [view_source] Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds. SingleProcessVectorSampledTasks.pause_at # | pause_at ( sampler_index : int ) -> None [view_source] Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one. SingleProcessVectorSampledTasks.resume_all # | resume_all () -> None [view_source] Resumes any paused processes. SingleProcessVectorSampledTasks.command_at # | command_at ( sampler_index : int , command : str , data : Optional [ Any ] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function. SingleProcessVectorSampledTasks.call_at # | call_at ( sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function. SingleProcessVectorSampledTasks.call # | call ( function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None ) -> List [ Any ] [view_source] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions. SingleProcessVectorSampledTasks.attr_at # | attr_at ( sampler_index : int , attr_name : str ) -> Any [view_source] Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function. SingleProcessVectorSampledTasks.attr # | attr ( attr_names : Union [ List [ str ], str ]) -> List [ Any ] [view_source] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions. SingleProcessVectorSampledTasks.render # | render ( mode : str = \"human\" , * args , ** kwargs ) -> Union [ np . ndarray , None , List [ np . ndarray ]] [view_source] Render observations from all Tasks in a tiled image or a list of images.","title":"vector_sampled_tasks"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#allenactalgorithmsonpolicy_syncvector_sampled_tasks","text":"[view_source]","title":"allenact.algorithms.onpolicy_sync.vector_sampled_tasks"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasks","text":"class VectorSampledTasks ( object ) [view_source] Vectorized collection of tasks. Creates multiple processes where each process runs its own TaskSampler. Each process generates one Task from its TaskSampler at a time and this class allows for interacting with these tasks in a vectorized manner. When a task on a process completes, the process samples another task from its task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks. multiprocessing_start_method : the multiprocessing method used to spawn worker processes. Valid methods are {'spawn', 'forkserver', 'fork'} 'forkserver' is the recommended method as it works well with CUDA. If 'fork' is used, the subproccess must be started before any other GPU useage.","title":"VectorSampledTasks"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksis_closed","text":"| @property | is_closed () -> bool [view_source] Has the vector task been closed.","title":"VectorSampledTasks.is_closed"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksnum_unpaused_tasks","text":"| @property | num_unpaused_tasks () -> int [view_source] Number of unpaused processes. Returns Number of unpaused processes.","title":"VectorSampledTasks.num_unpaused_tasks"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksmp_ctx","text":"| @property | mp_ctx () [view_source] Get the multiprocessing process used by the vector task. Returns The multiprocessing context.","title":"VectorSampledTasks.mp_ctx"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksnext_task","text":"| next_task ( ** kwargs ) [view_source] Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks.","title":"VectorSampledTasks.next_task"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksget_observations","text":"| get_observations () [view_source] Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks.","title":"VectorSampledTasks.get_observations"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtaskscommand_at","text":"| command_at ( sampler_index : int , command : str , data : Optional [ Any ] = None ) -> Any [view_source] Runs the command on the selected task and returns the result. Parameters Returns Result of the command.","title":"VectorSampledTasks.command_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtaskscall_at","text":"| call_at ( sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function.","title":"VectorSampledTasks.call_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksnext_task_at","text":"| next_task_at ( sampler_index : int ) -> List [ RLStepResult ] [view_source] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the process to be reset. Returns List of length one containing the observations the newly sampled task.","title":"VectorSampledTasks.next_task_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksstep_at","text":"| step_at ( sampler_index : int , action : Any ) -> List [ RLStepResult ] [view_source] Step in the index_process task in the vector. Parameters sampler_index : Index of the sampler to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process.","title":"VectorSampledTasks.step_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksasync_step","text":"| async_step ( actions : Sequence [ Any ]) -> None [view_source] Asynchronously step in the vectorized Tasks. Parameters actions : actions to be performed in the vectorized Tasks.","title":"VectorSampledTasks.async_step"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtaskswait_step","text":"| wait_step () -> List [ Dict [ str , Any ]] [view_source] Wait until all the asynchronized processes have synchronized.","title":"VectorSampledTasks.wait_step"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksstep","text":"| step ( actions : Sequence [ Any ]) [view_source] Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks.","title":"VectorSampledTasks.step"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksreset_all","text":"| reset_all () [view_source] Reset all task samplers to their initial state (except for the RNG seed).","title":"VectorSampledTasks.reset_all"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksset_seeds","text":"| set_seeds ( seeds : List [ int ]) [view_source] Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds.","title":"VectorSampledTasks.set_seeds"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtaskspause_at","text":"| pause_at ( sampler_index : int ) -> None [view_source] Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one.","title":"VectorSampledTasks.pause_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksresume_all","text":"| resume_all () -> None [view_source] Resumes any paused processes.","title":"VectorSampledTasks.resume_all"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtaskscall","text":"| call ( function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None ) -> List [ Any ] [view_source] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions.","title":"VectorSampledTasks.call"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksattr_at","text":"| attr_at ( sampler_index : int , attr_name : str ) -> Any [view_source] Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function.","title":"VectorSampledTasks.attr_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksattr","text":"| attr ( attr_names : Union [ List [ str ], str ]) -> List [ Any ] [view_source] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions.","title":"VectorSampledTasks.attr"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksrender","text":"| render ( mode : str = \"human\" , * args , ** kwargs ) -> Union [ np . ndarray , None , List [ np . ndarray ]] [view_source] Render observations from all Tasks in a tiled image or list of images.","title":"VectorSampledTasks.render"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasks","text":"class SingleProcessVectorSampledTasks ( object ) [view_source] Vectorized collection of tasks. Simultaneously handles the state of multiple TaskSamplers and their associated tasks. Allows for interacting with these tasks in a vectorized manner. When a task completes, another task is sampled from the appropriate task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks.","title":"SingleProcessVectorSampledTasks"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksis_closed","text":"| @property | is_closed () -> bool [view_source] Has the vector task been closed.","title":"SingleProcessVectorSampledTasks.is_closed"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksnum_unpaused_tasks","text":"| @property | num_unpaused_tasks () -> int [view_source] Number of unpaused processes. Returns Number of unpaused processes.","title":"SingleProcessVectorSampledTasks.num_unpaused_tasks"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksnext_task","text":"| next_task ( ** kwargs ) [view_source] Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks.","title":"SingleProcessVectorSampledTasks.next_task"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksget_observations","text":"| get_observations () [view_source] Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks.","title":"SingleProcessVectorSampledTasks.get_observations"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksnext_task_at","text":"| next_task_at ( index_process : int ) -> List [ RLStepResult ] [view_source] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the generator to be reset. Returns List of length one containing the observations the newly sampled task.","title":"SingleProcessVectorSampledTasks.next_task_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksstep_at","text":"| step_at ( index_process : int , action : int ) -> List [ RLStepResult ] [view_source] Step in the index_process task in the vector. Parameters index_process : Index of the process to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process.","title":"SingleProcessVectorSampledTasks.step_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksstep","text":"| step ( actions : List [ List [ int ]]) [view_source] Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks.","title":"SingleProcessVectorSampledTasks.step"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksreset_all","text":"| reset_all () [view_source] Reset all task samplers to their initial state (except for the RNG seed).","title":"SingleProcessVectorSampledTasks.reset_all"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksset_seeds","text":"| set_seeds ( seeds : List [ int ]) [view_source] Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds.","title":"SingleProcessVectorSampledTasks.set_seeds"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtaskspause_at","text":"| pause_at ( sampler_index : int ) -> None [view_source] Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one.","title":"SingleProcessVectorSampledTasks.pause_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksresume_all","text":"| resume_all () -> None [view_source] Resumes any paused processes.","title":"SingleProcessVectorSampledTasks.resume_all"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtaskscommand_at","text":"| command_at ( sampler_index : int , command : str , data : Optional [ Any ] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function.","title":"SingleProcessVectorSampledTasks.command_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtaskscall_at","text":"| call_at ( sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function.","title":"SingleProcessVectorSampledTasks.call_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtaskscall","text":"| call ( function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None ) -> List [ Any ] [view_source] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions.","title":"SingleProcessVectorSampledTasks.call"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksattr_at","text":"| attr_at ( sampler_index : int , attr_name : str ) -> Any [view_source] Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function.","title":"SingleProcessVectorSampledTasks.attr_at"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksattr","text":"| attr ( attr_names : Union [ List [ str ], str ]) -> List [ Any ] [view_source] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions.","title":"SingleProcessVectorSampledTasks.attr"},{"location":"api/allenact/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksrender","text":"| render ( mode : str = \"human\" , * args , ** kwargs ) -> Union [ np . ndarray , None , List [ np . ndarray ]] [view_source] Render observations from all Tasks in a tiled image or a list of images.","title":"SingleProcessVectorSampledTasks.render"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/a2cacktr/","text":"allenact.algorithms.onpolicy_sync.losses.a2cacktr # [view_source] Implementation of A2C and ACKTR losses. A2CACKTR # class A2CACKTR ( AbstractActorCriticLoss ) [view_source] Class implementing A2C and ACKTR losses. Attributes acktr : True if should use ACKTR loss (currently not supported), otherwise uses A2C loss. value_loss_coef : Weight of value loss. entropy_coef : Weight of entropy (encouraging) loss. entropy_method_name : Name of Distr's entropy method name. Default is entropy , but we might use conditional_entropy for SequentialDistr . A2CACKTR.__init__ # | __init__ ( value_loss_coef , entropy_coef , acktr = False , entropy_method_name : str = \"entropy\" , * args , ** kwargs , * , ,) [view_source] Initializer. See class documentation for parameter definitions. A2C # class A2C ( A2CACKTR ) [view_source] A2C Loss. ACKTR # class ACKTR ( A2CACKTR ) [view_source] ACKTR Loss. This code is not supported as it currently lacks an implementation for recurrent models.","title":"a2cacktr"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/a2cacktr/#allenactalgorithmsonpolicy_synclossesa2cacktr","text":"[view_source] Implementation of A2C and ACKTR losses.","title":"allenact.algorithms.onpolicy_sync.losses.a2cacktr"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/a2cacktr/#a2cacktr","text":"class A2CACKTR ( AbstractActorCriticLoss ) [view_source] Class implementing A2C and ACKTR losses. Attributes acktr : True if should use ACKTR loss (currently not supported), otherwise uses A2C loss. value_loss_coef : Weight of value loss. entropy_coef : Weight of entropy (encouraging) loss. entropy_method_name : Name of Distr's entropy method name. Default is entropy , but we might use conditional_entropy for SequentialDistr .","title":"A2CACKTR"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/a2cacktr/#a2cacktr__init__","text":"| __init__ ( value_loss_coef , entropy_coef , acktr = False , entropy_method_name : str = \"entropy\" , * args , ** kwargs , * , ,) [view_source] Initializer. See class documentation for parameter definitions.","title":"A2CACKTR.__init__"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/a2cacktr/#a2c","text":"class A2C ( A2CACKTR ) [view_source] A2C Loss.","title":"A2C"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/a2cacktr/#acktr","text":"class ACKTR ( A2CACKTR ) [view_source] ACKTR Loss. This code is not supported as it currently lacks an implementation for recurrent models.","title":"ACKTR"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/abstract_loss/","text":"allenact.algorithms.onpolicy_sync.losses.abstract_loss # [view_source] Defining abstract loss classes for actor critic models. AbstractActorCriticLoss # class AbstractActorCriticLoss ( Loss ) [view_source] Abstract class representing a loss function used to train an ActorCriticModel. AbstractActorCriticLoss.loss # | @abc . abstractmethod | loss ( step_count : int , batch : ObservationType , actor_critic_output : ActorCriticOutput [ CategoricalDistr ], * args , ** kwargs , * , ,) -> Tuple [ torch . FloatTensor , Dict [ str , float ]] [view_source] Computes the loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. kwargs : Extra kwargs. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"abstract_loss"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/abstract_loss/#allenactalgorithmsonpolicy_synclossesabstract_loss","text":"[view_source] Defining abstract loss classes for actor critic models.","title":"allenact.algorithms.onpolicy_sync.losses.abstract_loss"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/abstract_loss/#abstractactorcriticloss","text":"class AbstractActorCriticLoss ( Loss ) [view_source] Abstract class representing a loss function used to train an ActorCriticModel.","title":"AbstractActorCriticLoss"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/abstract_loss/#abstractactorcriticlossloss","text":"| @abc . abstractmethod | loss ( step_count : int , batch : ObservationType , actor_critic_output : ActorCriticOutput [ CategoricalDistr ], * args , ** kwargs , * , ,) -> Tuple [ torch . FloatTensor , Dict [ str , float ]] [view_source] Computes the loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. kwargs : Extra kwargs. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"AbstractActorCriticLoss.loss"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/grouped_action_imitation/","text":"allenact.algorithms.onpolicy_sync.losses.grouped_action_imitation # [view_source]","title":"grouped_action_imitation"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/grouped_action_imitation/#allenactalgorithmsonpolicy_synclossesgrouped_action_imitation","text":"[view_source]","title":"allenact.algorithms.onpolicy_sync.losses.grouped_action_imitation"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/imitation/","text":"allenact.algorithms.onpolicy_sync.losses.imitation # [view_source] Defining imitation losses for actor critic type models. Imitation # class Imitation ( AbstractActorCriticLoss ) [view_source] Expert imitation loss. Imitation.loss # | loss ( step_count : int , batch : ObservationType , actor_critic_output : ActorCriticOutput [ Distr ], * args , ** kwargs , * , ,) [view_source] Computes the imitation loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . Here batch[\"observations\"] must contain \"expert_action\" observations or \"expert_policy\" observations. See ExpertActionSensor (or ExpertPolicySensor ) for an example of a sensor producing such observations. actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. Ignored. kwargs : Extra kwargs. Ignored. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"imitation"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/imitation/#allenactalgorithmsonpolicy_synclossesimitation","text":"[view_source] Defining imitation losses for actor critic type models.","title":"allenact.algorithms.onpolicy_sync.losses.imitation"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/imitation/#imitation","text":"class Imitation ( AbstractActorCriticLoss ) [view_source] Expert imitation loss.","title":"Imitation"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/imitation/#imitationloss","text":"| loss ( step_count : int , batch : ObservationType , actor_critic_output : ActorCriticOutput [ Distr ], * args , ** kwargs , * , ,) [view_source] Computes the imitation loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . Here batch[\"observations\"] must contain \"expert_action\" observations or \"expert_policy\" observations. See ExpertActionSensor (or ExpertPolicySensor ) for an example of a sensor producing such observations. actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. Ignored. kwargs : Extra kwargs. Ignored. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"Imitation.loss"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/kfac/","text":"allenact.algorithms.onpolicy_sync.losses.kfac # [view_source] Implementation of the KFAC optimizer. TODO: this code is not supported as it currently lacks an implementation for recurrent models.","title":"kfac"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/kfac/#allenactalgorithmsonpolicy_synclosseskfac","text":"[view_source] Implementation of the KFAC optimizer. TODO: this code is not supported as it currently lacks an implementation for recurrent models.","title":"allenact.algorithms.onpolicy_sync.losses.kfac"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/ppo/","text":"allenact.algorithms.onpolicy_sync.losses.ppo # [view_source] Defining the PPO loss for actor critic type models. PPO # class PPO ( AbstractActorCriticLoss ) [view_source] Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. value_loss_coef : Weight of the value loss. entropy_coef : Weight of the entropy (encouraging) loss. use_clipped_value_loss : Whether or not to also clip the value loss. clip_decay : Callable for clip param decay factor (function of the current number of steps) entropy_method_name : Name of Distr's entropy method name. Default is entropy , but we might use conditional_entropy for SequentialDistr PPO.__init__ # | __init__ ( clip_param : float , value_loss_coef : float , entropy_coef : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , entropy_method_name : str = \"entropy\" , * args , ** kwargs ) [view_source] Initializer. See the class documentation for parameter definitions. PPOValue # class PPOValue ( AbstractActorCriticLoss ) [view_source] Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. use_clipped_value_loss : Whether or not to also clip the value loss. PPOValue.__init__ # | __init__ ( clip_param : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , * args , ** kwargs ) [view_source] Initializer. See the class documentation for parameter definitions.","title":"ppo"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/ppo/#allenactalgorithmsonpolicy_synclossesppo","text":"[view_source] Defining the PPO loss for actor critic type models.","title":"allenact.algorithms.onpolicy_sync.losses.ppo"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/ppo/#ppo","text":"class PPO ( AbstractActorCriticLoss ) [view_source] Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. value_loss_coef : Weight of the value loss. entropy_coef : Weight of the entropy (encouraging) loss. use_clipped_value_loss : Whether or not to also clip the value loss. clip_decay : Callable for clip param decay factor (function of the current number of steps) entropy_method_name : Name of Distr's entropy method name. Default is entropy , but we might use conditional_entropy for SequentialDistr","title":"PPO"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/ppo/#ppo__init__","text":"| __init__ ( clip_param : float , value_loss_coef : float , entropy_coef : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , entropy_method_name : str = \"entropy\" , * args , ** kwargs ) [view_source] Initializer. See the class documentation for parameter definitions.","title":"PPO.__init__"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/ppo/#ppovalue","text":"class PPOValue ( AbstractActorCriticLoss ) [view_source] Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. use_clipped_value_loss : Whether or not to also clip the value loss.","title":"PPOValue"},{"location":"api/allenact/algorithms/onpolicy_sync/losses/ppo/#ppovalue__init__","text":"| __init__ ( clip_param : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , * args , ** kwargs ) [view_source] Initializer. See the class documentation for parameter definitions.","title":"PPOValue.__init__"},{"location":"api/allenact/base_abstractions/distributions/","text":"allenact.base_abstractions.distributions # [view_source] TeacherForcingAnnealingType # [view_source] Modify standard PyTorch distributions so they are compatible with this code. Distr # class Distr ( abc . ABC ) [view_source] Distr.log_prob # | @abc . abstractmethod | log_prob ( actions : Any ) [view_source] Return the log probability/ies of the provided action/s. Distr.entropy # | @abc . abstractmethod | entropy () [view_source] Return the entropy or entropies. Distr.sample # | @abc . abstractmethod | sample ( sample_shape = torch . Size ()) [view_source] Sample actions. Distr.mode # | mode () [view_source] If available, return the action(s) with highest probability. It will only be called if using deterministic agents. CategoricalDistr # class CategoricalDistr ( torch . distributions . Categorical , Distr ) [view_source] A categorical distribution extending PyTorch's Categorical. probs or logits are assumed to be passed with step and sampler dimensions as in: [step, samplers, ...] ConditionalDistr # class ConditionalDistr ( Distr ) [view_source] Action distribution conditional which is conditioned on other information (i.e. part of a hierarchical distribution) Attributes action_group_name : the identifier of the group of actions ( OrderedDict ) produced by this ConditionalDistr ConditionalDistr.__init__ # | __init__ ( distr_conditioned_on_input_fn_or_instance : Union [ Callable , Distr ], action_group_name : str , * distr_conditioned_on_input_args , ** distr_conditioned_on_input_kwargs , * , ,) [view_source] Initialize an ConditionalDistr Parameters distr_conditioned_on_input_fn_or_instance : Callable to generate ConditionalDistr given sampled actions, or given Distr . action_group_name : the identifier of the group of actions ( OrderedDict ) produced by this ConditionalDistr distr_conditioned_on_input_args : positional arguments for Callable distr_conditioned_on_input_fn_or_instance distr_conditioned_on_input_kwargs : keyword arguments for Callable distr_conditioned_on_input_fn_or_instance AddBias # class AddBias ( nn . Module ) [view_source] Adding bias parameters to input values. AddBias.__init__ # | __init__ ( bias : torch . FloatTensor ) [view_source] Initializer. Parameters bias : data to use as the initial values of the bias. AddBias.forward # | forward ( x : torch . FloatTensor ) -> torch . FloatTensor [view_source] Adds the stored bias parameters to x .","title":"distributions"},{"location":"api/allenact/base_abstractions/distributions/#allenactbase_abstractionsdistributions","text":"[view_source]","title":"allenact.base_abstractions.distributions"},{"location":"api/allenact/base_abstractions/distributions/#teacherforcingannealingtype","text":"[view_source] Modify standard PyTorch distributions so they are compatible with this code.","title":"TeacherForcingAnnealingType"},{"location":"api/allenact/base_abstractions/distributions/#distr","text":"class Distr ( abc . ABC ) [view_source]","title":"Distr"},{"location":"api/allenact/base_abstractions/distributions/#distrlog_prob","text":"| @abc . abstractmethod | log_prob ( actions : Any ) [view_source] Return the log probability/ies of the provided action/s.","title":"Distr.log_prob"},{"location":"api/allenact/base_abstractions/distributions/#distrentropy","text":"| @abc . abstractmethod | entropy () [view_source] Return the entropy or entropies.","title":"Distr.entropy"},{"location":"api/allenact/base_abstractions/distributions/#distrsample","text":"| @abc . abstractmethod | sample ( sample_shape = torch . Size ()) [view_source] Sample actions.","title":"Distr.sample"},{"location":"api/allenact/base_abstractions/distributions/#distrmode","text":"| mode () [view_source] If available, return the action(s) with highest probability. It will only be called if using deterministic agents.","title":"Distr.mode"},{"location":"api/allenact/base_abstractions/distributions/#categoricaldistr","text":"class CategoricalDistr ( torch . distributions . Categorical , Distr ) [view_source] A categorical distribution extending PyTorch's Categorical. probs or logits are assumed to be passed with step and sampler dimensions as in: [step, samplers, ...]","title":"CategoricalDistr"},{"location":"api/allenact/base_abstractions/distributions/#conditionaldistr","text":"class ConditionalDistr ( Distr ) [view_source] Action distribution conditional which is conditioned on other information (i.e. part of a hierarchical distribution) Attributes action_group_name : the identifier of the group of actions ( OrderedDict ) produced by this ConditionalDistr","title":"ConditionalDistr"},{"location":"api/allenact/base_abstractions/distributions/#conditionaldistr__init__","text":"| __init__ ( distr_conditioned_on_input_fn_or_instance : Union [ Callable , Distr ], action_group_name : str , * distr_conditioned_on_input_args , ** distr_conditioned_on_input_kwargs , * , ,) [view_source] Initialize an ConditionalDistr Parameters distr_conditioned_on_input_fn_or_instance : Callable to generate ConditionalDistr given sampled actions, or given Distr . action_group_name : the identifier of the group of actions ( OrderedDict ) produced by this ConditionalDistr distr_conditioned_on_input_args : positional arguments for Callable distr_conditioned_on_input_fn_or_instance distr_conditioned_on_input_kwargs : keyword arguments for Callable distr_conditioned_on_input_fn_or_instance","title":"ConditionalDistr.__init__"},{"location":"api/allenact/base_abstractions/distributions/#addbias","text":"class AddBias ( nn . Module ) [view_source] Adding bias parameters to input values.","title":"AddBias"},{"location":"api/allenact/base_abstractions/distributions/#addbias__init__","text":"| __init__ ( bias : torch . FloatTensor ) [view_source] Initializer. Parameters bias : data to use as the initial values of the bias.","title":"AddBias.__init__"},{"location":"api/allenact/base_abstractions/distributions/#addbiasforward","text":"| forward ( x : torch . FloatTensor ) -> torch . FloatTensor [view_source] Adds the stored bias parameters to x .","title":"AddBias.forward"},{"location":"api/allenact/base_abstractions/experiment_config/","text":"allenact.base_abstractions.experiment_config # [view_source] Defines the ExperimentConfig abstract class used as the basis of all experiments. FrozenClassVariables # class FrozenClassVariables ( abc . ABCMeta ) [view_source] Metaclass for ExperimentConfig. Ensures ExperimentConfig class-level attributes cannot be modified. ExperimentConfig attributes can still be modified at the object level. ExperimentConfig # class ExperimentConfig (, metaclass = FrozenClassVariables ) [view_source] Abstract class used to define experiments. Instead of using yaml or text files, experiments in our framework are defined as a class. In particular, to define an experiment one must define a new class inheriting from this class which implements all of the below methods. The below methods will then be called when running the experiment. ExperimentConfig.tag # | @abc . abstractmethod | tag () -> str [view_source] A string describing the experiment. ExperimentConfig.training_pipeline # | @abc . abstractmethod | training_pipeline ( ** kwargs ) -> TrainingPipeline [view_source] Creates the training pipeline. Parameters kwargs : Extra kwargs. Currently unused. Returns An instantiate TrainingPipeline object. ExperimentConfig.machine_params # | @abc . abstractmethod | machine_params ( mode = \"train\" , ** kwargs ) -> Union [ MachineParams , Dict [ str , Any ]] [view_source] Parameters used to specify machine information. Machine information includes at least (1) the number of processes to train with and (2) the gpu devices indices to use. mode : Whether or not the machine parameters should be those for \"train\", \"valid\", or \"test\". kwargs : Extra kwargs. Returns A dictionary of the form {\"nprocesses\" : ..., \"gpu_ids\": ..., ...} . Here nprocesses must be a non-negative integer, gpu_ids must be a sequence of non-negative integers (if empty, then everything will be run on the cpu). ExperimentConfig.create_model # | @abc . abstractmethod | create_model ( ** kwargs ) -> nn . Module [view_source] Create the neural model. ExperimentConfig.make_sampler_fn # | @abc . abstractmethod | make_sampler_fn ( ** kwargs ) -> TaskSampler [view_source] Create the TaskSampler given keyword arguments. These kwargs will be generated by one of ExperimentConfig.train_task_sampler_args , ExperimentConfig.valid_task_sampler_args , or ExperimentConfig.test_task_sampler_args depending on whether the user has chosen to train, validate, or test. ExperimentConfig.train_task_sampler_args # | train_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the training parameters for the process_ind th training process. These parameters are meant be passed as keyword arguments to ExperimentConfig.make_sampler_fn to generate a task sampler. Parameters process_ind : The unique index of the training process ( 0 \u2264 process_ind < total_processes ). total_processes : The total number of training processes. devices : Gpu devices (if any) to use. seeds : The seeds to use, if any. deterministic_cudnn : Whether or not to use deterministic cudnn. Returns The parameters for make_sampler_fn ExperimentConfig.valid_task_sampler_args # | valid_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the validation parameters for the process_ind th validation process. See ExperimentConfig.train_task_sampler_args for parameter definitions. ExperimentConfig.test_task_sampler_args # | test_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the test parameters for the process_ind th test process. See ExperimentConfig.train_task_sampler_args for parameter definitions.","title":"experiment_config"},{"location":"api/allenact/base_abstractions/experiment_config/#allenactbase_abstractionsexperiment_config","text":"[view_source] Defines the ExperimentConfig abstract class used as the basis of all experiments.","title":"allenact.base_abstractions.experiment_config"},{"location":"api/allenact/base_abstractions/experiment_config/#frozenclassvariables","text":"class FrozenClassVariables ( abc . ABCMeta ) [view_source] Metaclass for ExperimentConfig. Ensures ExperimentConfig class-level attributes cannot be modified. ExperimentConfig attributes can still be modified at the object level.","title":"FrozenClassVariables"},{"location":"api/allenact/base_abstractions/experiment_config/#experimentconfig","text":"class ExperimentConfig (, metaclass = FrozenClassVariables ) [view_source] Abstract class used to define experiments. Instead of using yaml or text files, experiments in our framework are defined as a class. In particular, to define an experiment one must define a new class inheriting from this class which implements all of the below methods. The below methods will then be called when running the experiment.","title":"ExperimentConfig"},{"location":"api/allenact/base_abstractions/experiment_config/#experimentconfigtag","text":"| @abc . abstractmethod | tag () -> str [view_source] A string describing the experiment.","title":"ExperimentConfig.tag"},{"location":"api/allenact/base_abstractions/experiment_config/#experimentconfigtraining_pipeline","text":"| @abc . abstractmethod | training_pipeline ( ** kwargs ) -> TrainingPipeline [view_source] Creates the training pipeline. Parameters kwargs : Extra kwargs. Currently unused. Returns An instantiate TrainingPipeline object.","title":"ExperimentConfig.training_pipeline"},{"location":"api/allenact/base_abstractions/experiment_config/#experimentconfigmachine_params","text":"| @abc . abstractmethod | machine_params ( mode = \"train\" , ** kwargs ) -> Union [ MachineParams , Dict [ str , Any ]] [view_source] Parameters used to specify machine information. Machine information includes at least (1) the number of processes to train with and (2) the gpu devices indices to use. mode : Whether or not the machine parameters should be those for \"train\", \"valid\", or \"test\". kwargs : Extra kwargs. Returns A dictionary of the form {\"nprocesses\" : ..., \"gpu_ids\": ..., ...} . Here nprocesses must be a non-negative integer, gpu_ids must be a sequence of non-negative integers (if empty, then everything will be run on the cpu).","title":"ExperimentConfig.machine_params"},{"location":"api/allenact/base_abstractions/experiment_config/#experimentconfigcreate_model","text":"| @abc . abstractmethod | create_model ( ** kwargs ) -> nn . Module [view_source] Create the neural model.","title":"ExperimentConfig.create_model"},{"location":"api/allenact/base_abstractions/experiment_config/#experimentconfigmake_sampler_fn","text":"| @abc . abstractmethod | make_sampler_fn ( ** kwargs ) -> TaskSampler [view_source] Create the TaskSampler given keyword arguments. These kwargs will be generated by one of ExperimentConfig.train_task_sampler_args , ExperimentConfig.valid_task_sampler_args , or ExperimentConfig.test_task_sampler_args depending on whether the user has chosen to train, validate, or test.","title":"ExperimentConfig.make_sampler_fn"},{"location":"api/allenact/base_abstractions/experiment_config/#experimentconfigtrain_task_sampler_args","text":"| train_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the training parameters for the process_ind th training process. These parameters are meant be passed as keyword arguments to ExperimentConfig.make_sampler_fn to generate a task sampler. Parameters process_ind : The unique index of the training process ( 0 \u2264 process_ind < total_processes ). total_processes : The total number of training processes. devices : Gpu devices (if any) to use. seeds : The seeds to use, if any. deterministic_cudnn : Whether or not to use deterministic cudnn. Returns The parameters for make_sampler_fn","title":"ExperimentConfig.train_task_sampler_args"},{"location":"api/allenact/base_abstractions/experiment_config/#experimentconfigvalid_task_sampler_args","text":"| valid_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the validation parameters for the process_ind th validation process. See ExperimentConfig.train_task_sampler_args for parameter definitions.","title":"ExperimentConfig.valid_task_sampler_args"},{"location":"api/allenact/base_abstractions/experiment_config/#experimentconfigtest_task_sampler_args","text":"| test_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the test parameters for the process_ind th test process. See ExperimentConfig.train_task_sampler_args for parameter definitions.","title":"ExperimentConfig.test_task_sampler_args"},{"location":"api/allenact/base_abstractions/misc/","text":"allenact.base_abstractions.misc # [view_source] Memory # class Memory ( Dict ) [view_source] Memory.check_append # | check_append ( key : str , tensor : torch . Tensor , sampler_dim : int ) -> \"Memory\" [view_source] Appends a new memory type given its identifier, its memory tensor and its sampler dim. Parameters key : string identifier of the memory type tensor : memory tensor sampler_dim : sampler dimension Returns Updated Memory Memory.tensor # | tensor ( key : str ) -> torch . Tensor [view_source] Returns the memory tensor for a given memory type. Parameters key : string identifier of the memory type Returns Memory tensor for type key Memory.sampler_dim # | sampler_dim ( key : str ) -> int [view_source] Returns the sampler dimension for the given memory type. Parameters key : string identifier of the memory type Returns The sampler dim Memory.sampler_select # | sampler_select ( keep : Sequence [ int ]) -> \"Memory\" [view_source] Equivalent to PyTorch index_select along the sampler_dim of each memory type. Parameters keep : a list of sampler indices to keep Returns Selected memory Memory.set_tensor # | set_tensor ( key : str , tensor : torch . Tensor ) -> \"Memory\" [view_source] Replaces tensor for given key with an updated version. Parameters key : memory type identifier to update tensor : updated tensor Returns Updated memory Memory.step_select # | step_select ( step : int ) -> \"Memory\" [view_source] Equivalent to slicing with length 1 for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step Memory.step_squeeze # | step_squeeze ( step : int ) -> \"Memory\" [view_source] Equivalent to simple indexing for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step (and squeezed step dimension) Memory.slice # | slice ( dim : int , start : Optional [ int ] = None , stop : Optional [ int ] = None , step : int = 1 ) -> \"Memory\" [view_source] Slicing for dimensions that have same extents in all memory types. It also accepts negative indices. Parameters dim : the dimension to slice start : the index of the first item to keep if given (default 0 if None) stop : the index of the first item to discard if given (default tensor size along dim if None) step : the increment between consecutive indices (default 1) Returns Sliced memory","title":"misc"},{"location":"api/allenact/base_abstractions/misc/#allenactbase_abstractionsmisc","text":"[view_source]","title":"allenact.base_abstractions.misc"},{"location":"api/allenact/base_abstractions/misc/#memory","text":"class Memory ( Dict ) [view_source]","title":"Memory"},{"location":"api/allenact/base_abstractions/misc/#memorycheck_append","text":"| check_append ( key : str , tensor : torch . Tensor , sampler_dim : int ) -> \"Memory\" [view_source] Appends a new memory type given its identifier, its memory tensor and its sampler dim. Parameters key : string identifier of the memory type tensor : memory tensor sampler_dim : sampler dimension Returns Updated Memory","title":"Memory.check_append"},{"location":"api/allenact/base_abstractions/misc/#memorytensor","text":"| tensor ( key : str ) -> torch . Tensor [view_source] Returns the memory tensor for a given memory type. Parameters key : string identifier of the memory type Returns Memory tensor for type key","title":"Memory.tensor"},{"location":"api/allenact/base_abstractions/misc/#memorysampler_dim","text":"| sampler_dim ( key : str ) -> int [view_source] Returns the sampler dimension for the given memory type. Parameters key : string identifier of the memory type Returns The sampler dim","title":"Memory.sampler_dim"},{"location":"api/allenact/base_abstractions/misc/#memorysampler_select","text":"| sampler_select ( keep : Sequence [ int ]) -> \"Memory\" [view_source] Equivalent to PyTorch index_select along the sampler_dim of each memory type. Parameters keep : a list of sampler indices to keep Returns Selected memory","title":"Memory.sampler_select"},{"location":"api/allenact/base_abstractions/misc/#memoryset_tensor","text":"| set_tensor ( key : str , tensor : torch . Tensor ) -> \"Memory\" [view_source] Replaces tensor for given key with an updated version. Parameters key : memory type identifier to update tensor : updated tensor Returns Updated memory","title":"Memory.set_tensor"},{"location":"api/allenact/base_abstractions/misc/#memorystep_select","text":"| step_select ( step : int ) -> \"Memory\" [view_source] Equivalent to slicing with length 1 for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step","title":"Memory.step_select"},{"location":"api/allenact/base_abstractions/misc/#memorystep_squeeze","text":"| step_squeeze ( step : int ) -> \"Memory\" [view_source] Equivalent to simple indexing for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step (and squeezed step dimension)","title":"Memory.step_squeeze"},{"location":"api/allenact/base_abstractions/misc/#memoryslice","text":"| slice ( dim : int , start : Optional [ int ] = None , stop : Optional [ int ] = None , step : int = 1 ) -> \"Memory\" [view_source] Slicing for dimensions that have same extents in all memory types. It also accepts negative indices. Parameters dim : the dimension to slice start : the index of the first item to keep if given (default 0 if None) stop : the index of the first item to discard if given (default tensor size along dim if None) step : the increment between consecutive indices (default 1) Returns Sliced memory","title":"Memory.slice"},{"location":"api/allenact/base_abstractions/preprocessor/","text":"allenact.base_abstractions.preprocessor # [view_source] Preprocessor # class Preprocessor ( abc . ABC ) [view_source] Represents a preprocessor that transforms data from a sensor or another preprocessor to the input of agents or other preprocessors. The user of this class needs to implement the process method and the user is also required to set the below attributes: Attributes: # input_uuids : List of input universally unique ids. uuid : Universally unique id. observation_space : gym.Space object corresponding to processed observation spaces. Preprocessor.process # | @abc . abstractmethod | process ( obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Any [view_source] Returns processed observations from sensors or other preprocessors. Parameters obs : Dict with available observations and processed observations. Returns Processed observation. SensorPreprocessorGraph # class SensorPreprocessorGraph () [view_source] Represents a graph of preprocessors, with each preprocessor being identified through a universally unique id. Allows for the construction of observations that are a function of sensor readings. For instance, perhaps rather than giving your agent a raw RGB image, you'd rather first pass that image through a pre-trained convolutional network and only give your agent the resulting features (see e.g. the ResNetPreprocessor class). Attributes preprocessors : List containing preprocessors with required input uuids, output uuid of each sensor must be unique. observation_spaces : The observation spaces of the values returned when calling get_observations . By default (see the additionally_exposed_uuids parameter to to change this default) the observations returned by the SensorPreprocessorGraph include only the sink nodes of the graph (i.e. those that are not used by any other preprocessor). Thus if one of the input preprocessors takes as input the 'YOUR_SENSOR_UUID' sensor, then 'YOUR_SENSOR_UUID' will not be returned when calling get_observations . device : The torch.device upon which the preprocessors are run. SensorPreprocessorGraph.__init__ # | __init__ ( source_observation_spaces : SpaceDict , preprocessors : Sequence [ Union [ Preprocessor , Builder [ Preprocessor ]]], additional_output_uuids : Sequence [ str ] = tuple ()) -> None [view_source] Initializer. Parameters source_observation_spaces : The observation spaces of all sensors before preprocessing. This generally should be the output of SensorSuite.observation_spaces . preprocessors : The preprocessors that will be included in the graph. additional_output_uuids : As described in the documentation for this class, the observations returned when calling get_observations only include, by default, those observations that are not processed by any preprocessor. If you'd like to include observations that would otherwise not be included, the uuids of these sensors should be included as a sequence of strings here. SensorPreprocessorGraph.get # | get ( uuid : str ) -> Preprocessor [view_source] Return preprocessor with the given uuid . Parameters uuid : The unique id of the preprocessor. Returns The preprocessor with unique id uuid . SensorPreprocessorGraph.get_observations # | get_observations ( obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Dict [ str , Any ] [view_source] Get processed observations. Returns Collect observations processed from all sensors and return them packaged inside a Dict.","title":"preprocessor"},{"location":"api/allenact/base_abstractions/preprocessor/#allenactbase_abstractionspreprocessor","text":"[view_source]","title":"allenact.base_abstractions.preprocessor"},{"location":"api/allenact/base_abstractions/preprocessor/#preprocessor","text":"class Preprocessor ( abc . ABC ) [view_source] Represents a preprocessor that transforms data from a sensor or another preprocessor to the input of agents or other preprocessors. The user of this class needs to implement the process method and the user is also required to set the below attributes:","title":"Preprocessor"},{"location":"api/allenact/base_abstractions/preprocessor/#attributes","text":"input_uuids : List of input universally unique ids. uuid : Universally unique id. observation_space : gym.Space object corresponding to processed observation spaces.","title":"Attributes:"},{"location":"api/allenact/base_abstractions/preprocessor/#preprocessorprocess","text":"| @abc . abstractmethod | process ( obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Any [view_source] Returns processed observations from sensors or other preprocessors. Parameters obs : Dict with available observations and processed observations. Returns Processed observation.","title":"Preprocessor.process"},{"location":"api/allenact/base_abstractions/preprocessor/#sensorpreprocessorgraph","text":"class SensorPreprocessorGraph () [view_source] Represents a graph of preprocessors, with each preprocessor being identified through a universally unique id. Allows for the construction of observations that are a function of sensor readings. For instance, perhaps rather than giving your agent a raw RGB image, you'd rather first pass that image through a pre-trained convolutional network and only give your agent the resulting features (see e.g. the ResNetPreprocessor class). Attributes preprocessors : List containing preprocessors with required input uuids, output uuid of each sensor must be unique. observation_spaces : The observation spaces of the values returned when calling get_observations . By default (see the additionally_exposed_uuids parameter to to change this default) the observations returned by the SensorPreprocessorGraph include only the sink nodes of the graph (i.e. those that are not used by any other preprocessor). Thus if one of the input preprocessors takes as input the 'YOUR_SENSOR_UUID' sensor, then 'YOUR_SENSOR_UUID' will not be returned when calling get_observations . device : The torch.device upon which the preprocessors are run.","title":"SensorPreprocessorGraph"},{"location":"api/allenact/base_abstractions/preprocessor/#sensorpreprocessorgraph__init__","text":"| __init__ ( source_observation_spaces : SpaceDict , preprocessors : Sequence [ Union [ Preprocessor , Builder [ Preprocessor ]]], additional_output_uuids : Sequence [ str ] = tuple ()) -> None [view_source] Initializer. Parameters source_observation_spaces : The observation spaces of all sensors before preprocessing. This generally should be the output of SensorSuite.observation_spaces . preprocessors : The preprocessors that will be included in the graph. additional_output_uuids : As described in the documentation for this class, the observations returned when calling get_observations only include, by default, those observations that are not processed by any preprocessor. If you'd like to include observations that would otherwise not be included, the uuids of these sensors should be included as a sequence of strings here.","title":"SensorPreprocessorGraph.__init__"},{"location":"api/allenact/base_abstractions/preprocessor/#sensorpreprocessorgraphget","text":"| get ( uuid : str ) -> Preprocessor [view_source] Return preprocessor with the given uuid . Parameters uuid : The unique id of the preprocessor. Returns The preprocessor with unique id uuid .","title":"SensorPreprocessorGraph.get"},{"location":"api/allenact/base_abstractions/preprocessor/#sensorpreprocessorgraphget_observations","text":"| get_observations ( obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Dict [ str , Any ] [view_source] Get processed observations. Returns Collect observations processed from all sensors and return them packaged inside a Dict.","title":"SensorPreprocessorGraph.get_observations"},{"location":"api/allenact/base_abstractions/sensor/","text":"allenact.base_abstractions.sensor # [view_source] Sensor # class Sensor ( Generic [ EnvType , SubTaskType ]) [view_source] Represents a sensor that provides data from the environment to agent. The user of this class needs to implement the get_observation method and the user is also required to set the below attributes: Attributes uuid : universally unique id. observation_space : gym.Space object corresponding to observation of sensor. Sensor.get_observation # | get_observation ( env : EnvType , task : Optional [ SubTaskType ], * args : Any , ** kwargs : Any ) -> Any [view_source] Returns observations from the environment (or task). Parameters env : The environment the sensor is used upon. task : (Optionally) a Task from which the sensor should get data. Returns Current observation for Sensor. SensorSuite # class SensorSuite ( Generic [ EnvType ]) [view_source] Represents a set of sensors, with each sensor being identified through a unique id. Attributes sensors : list containing sensors for the environment, uuid of each sensor must be unique. SensorSuite.__init__ # | __init__ ( sensors : Sequence [ Sensor ]) -> None [view_source] Initializer. Parameters param sensors : the sensors that will be included in the suite. SensorSuite.get # | get ( uuid : str ) -> Sensor [view_source] Return sensor with the given uuid . Parameters uuid : The unique id of the sensor Returns The sensor with unique id uuid . SensorSuite.get_observations # | get_observations ( env : EnvType , task : Optional [ SubTaskType ], ** kwargs : Any ) -> Dict [ str , Any ] [view_source] Get all observations corresponding to the sensors in the suite. Parameters env : The environment from which to get the observation. task : (Optionally) the task from which to get the observation. Returns Data from all sensors packaged inside a Dict. AbstractExpertSensor # class AbstractExpertSensor ( Sensor [ EnvType , SubTaskType ], abc . ABC ) [view_source] Base class for sensors that obtain the expert action for a given task (if available). AbstractExpertSensor.__init__ # | __init__ ( action_space : Optional [ Union [ gym . Space , int ]] = None , uuid : str = \"expert_sensor_type_uuid\" , expert_args : Optional [ Dict [ str , Any ]] = None , nactions : Optional [ int ] = None , use_dict_as_groups : bool = True , ** kwargs : Any , ,) -> None [view_source] Initialize an ExpertSensor . Parameters action_space : The action space of the agent. This is necessary in order for this sensor to know what its output observation space is. uuid : A string specifying the unique ID of this sensor. expert_args : This sensor obtains an expert action from the task by calling the query_expert method of the task. expert_args are any keyword arguments that should be passed to the query_expert method when called. nactions : [DEPRECATED] The number of actions available to the agent, corresponds to an action_space of gym.spaces.Discrete(nactions) . use_dict_as_groups : Whether to use the top-level action_space of type gym.spaces.Dict as action groups. AbstractExpertSensor.flagged_group_space # | @classmethod | @abc . abstractmethod | flagged_group_space ( cls , group_space : gym . spaces . Space ) -> gym . spaces . Dict [view_source] gym space resulting from wrapping the given action space (or a derived space, as in AbstractExpertPolicySensor ) together with a binary action space corresponding to an expert success flag, in a Dict space. Parameters group_space : The source action space to be (optionally used to derive a policy space,) flagged and wrapped AbstractExpertSensor.flagged_space # | @classmethod | flagged_space ( cls , action_space : gym . spaces . Space , use_dict_as_groups : bool = True ) -> gym . spaces . Dict [view_source] gym space resulting from wrapping the given action space (or every highest-level entry in a Dict action space), together with binary action space corresponding to an expert success flag, in a Dict space. Parameters action_space : The agent's action space (to be flagged and wrapped) use_dict_as_groups : Flag enabling every highest-level entry in a Dict action space to be independently flagged. AbstractExpertSensor.query_expert # | @abc . abstractmethod | query_expert ( task : SubTaskType , expert_sensor_group_name : Optional [ str ]) -> Tuple [ Any , bool ] [view_source] Query the expert for the given task (and optional group name). Returns A tuple (x, y) where x is the expert action or policy and y is False \\ if the expert could not determine the optimal action (otherwise True). Here y \\ is used for masking. Even when y is False, x should still lie in the space of \\ possible values (e.g. if x is the expert policy then x should be the correct length, \\ sum to 1, and have non-negative entries). AbstractExpertActionSensor # class AbstractExpertActionSensor ( AbstractExpertSensor , abc . ABC ) [view_source] AbstractExpertActionSensor.flagged_group_space # | @classmethod | flagged_group_space ( cls , group_space : gym . spaces . Space ) -> gym . spaces . Dict [view_source] gym space resulting from wrapping the given action space, together with a binary action space corresponding to an expert success flag, in a Dict space. Parameters group_space : The action space to be flagged and wrapped ExpertActionSensor # class ExpertActionSensor ( AbstractExpertActionSensor ) [view_source] (Deprecated) A sensor that obtains the expert action from a given task (if available). AbstractExpertPolicySensor # class AbstractExpertPolicySensor ( AbstractExpertSensor , abc . ABC ) [view_source] AbstractExpertPolicySensor.flagged_group_space # | @classmethod | flagged_group_space ( cls , group_space : gym . spaces . Space ) -> gym . spaces . Dict [view_source] gym space resulting from wrapping the policy space corresponding to allenact.utils.spaces_utils.policy_space(group_space) together with a binary action space corresponding to an expert success flag, in a Dict space. Parameters group_space : The source action space to be used to derive a policy space, flagged and wrapped ExpertPolicySensor # class ExpertPolicySensor ( AbstractExpertPolicySensor ) [view_source] (Deprecated) A sensor that obtains the expert policy from a given task (if available).","title":"sensor"},{"location":"api/allenact/base_abstractions/sensor/#allenactbase_abstractionssensor","text":"[view_source]","title":"allenact.base_abstractions.sensor"},{"location":"api/allenact/base_abstractions/sensor/#sensor","text":"class Sensor ( Generic [ EnvType , SubTaskType ]) [view_source] Represents a sensor that provides data from the environment to agent. The user of this class needs to implement the get_observation method and the user is also required to set the below attributes: Attributes uuid : universally unique id. observation_space : gym.Space object corresponding to observation of sensor.","title":"Sensor"},{"location":"api/allenact/base_abstractions/sensor/#sensorget_observation","text":"| get_observation ( env : EnvType , task : Optional [ SubTaskType ], * args : Any , ** kwargs : Any ) -> Any [view_source] Returns observations from the environment (or task). Parameters env : The environment the sensor is used upon. task : (Optionally) a Task from which the sensor should get data. Returns Current observation for Sensor.","title":"Sensor.get_observation"},{"location":"api/allenact/base_abstractions/sensor/#sensorsuite","text":"class SensorSuite ( Generic [ EnvType ]) [view_source] Represents a set of sensors, with each sensor being identified through a unique id. Attributes sensors : list containing sensors for the environment, uuid of each sensor must be unique.","title":"SensorSuite"},{"location":"api/allenact/base_abstractions/sensor/#sensorsuite__init__","text":"| __init__ ( sensors : Sequence [ Sensor ]) -> None [view_source] Initializer. Parameters param sensors : the sensors that will be included in the suite.","title":"SensorSuite.__init__"},{"location":"api/allenact/base_abstractions/sensor/#sensorsuiteget","text":"| get ( uuid : str ) -> Sensor [view_source] Return sensor with the given uuid . Parameters uuid : The unique id of the sensor Returns The sensor with unique id uuid .","title":"SensorSuite.get"},{"location":"api/allenact/base_abstractions/sensor/#sensorsuiteget_observations","text":"| get_observations ( env : EnvType , task : Optional [ SubTaskType ], ** kwargs : Any ) -> Dict [ str , Any ] [view_source] Get all observations corresponding to the sensors in the suite. Parameters env : The environment from which to get the observation. task : (Optionally) the task from which to get the observation. Returns Data from all sensors packaged inside a Dict.","title":"SensorSuite.get_observations"},{"location":"api/allenact/base_abstractions/sensor/#abstractexpertsensor","text":"class AbstractExpertSensor ( Sensor [ EnvType , SubTaskType ], abc . ABC ) [view_source] Base class for sensors that obtain the expert action for a given task (if available).","title":"AbstractExpertSensor"},{"location":"api/allenact/base_abstractions/sensor/#abstractexpertsensor__init__","text":"| __init__ ( action_space : Optional [ Union [ gym . Space , int ]] = None , uuid : str = \"expert_sensor_type_uuid\" , expert_args : Optional [ Dict [ str , Any ]] = None , nactions : Optional [ int ] = None , use_dict_as_groups : bool = True , ** kwargs : Any , ,) -> None [view_source] Initialize an ExpertSensor . Parameters action_space : The action space of the agent. This is necessary in order for this sensor to know what its output observation space is. uuid : A string specifying the unique ID of this sensor. expert_args : This sensor obtains an expert action from the task by calling the query_expert method of the task. expert_args are any keyword arguments that should be passed to the query_expert method when called. nactions : [DEPRECATED] The number of actions available to the agent, corresponds to an action_space of gym.spaces.Discrete(nactions) . use_dict_as_groups : Whether to use the top-level action_space of type gym.spaces.Dict as action groups.","title":"AbstractExpertSensor.__init__"},{"location":"api/allenact/base_abstractions/sensor/#abstractexpertsensorflagged_group_space","text":"| @classmethod | @abc . abstractmethod | flagged_group_space ( cls , group_space : gym . spaces . Space ) -> gym . spaces . Dict [view_source] gym space resulting from wrapping the given action space (or a derived space, as in AbstractExpertPolicySensor ) together with a binary action space corresponding to an expert success flag, in a Dict space. Parameters group_space : The source action space to be (optionally used to derive a policy space,) flagged and wrapped","title":"AbstractExpertSensor.flagged_group_space"},{"location":"api/allenact/base_abstractions/sensor/#abstractexpertsensorflagged_space","text":"| @classmethod | flagged_space ( cls , action_space : gym . spaces . Space , use_dict_as_groups : bool = True ) -> gym . spaces . Dict [view_source] gym space resulting from wrapping the given action space (or every highest-level entry in a Dict action space), together with binary action space corresponding to an expert success flag, in a Dict space. Parameters action_space : The agent's action space (to be flagged and wrapped) use_dict_as_groups : Flag enabling every highest-level entry in a Dict action space to be independently flagged.","title":"AbstractExpertSensor.flagged_space"},{"location":"api/allenact/base_abstractions/sensor/#abstractexpertsensorquery_expert","text":"| @abc . abstractmethod | query_expert ( task : SubTaskType , expert_sensor_group_name : Optional [ str ]) -> Tuple [ Any , bool ] [view_source] Query the expert for the given task (and optional group name). Returns A tuple (x, y) where x is the expert action or policy and y is False \\ if the expert could not determine the optimal action (otherwise True). Here y \\ is used for masking. Even when y is False, x should still lie in the space of \\ possible values (e.g. if x is the expert policy then x should be the correct length, \\ sum to 1, and have non-negative entries).","title":"AbstractExpertSensor.query_expert"},{"location":"api/allenact/base_abstractions/sensor/#abstractexpertactionsensor","text":"class AbstractExpertActionSensor ( AbstractExpertSensor , abc . ABC ) [view_source]","title":"AbstractExpertActionSensor"},{"location":"api/allenact/base_abstractions/sensor/#abstractexpertactionsensorflagged_group_space","text":"| @classmethod | flagged_group_space ( cls , group_space : gym . spaces . Space ) -> gym . spaces . Dict [view_source] gym space resulting from wrapping the given action space, together with a binary action space corresponding to an expert success flag, in a Dict space. Parameters group_space : The action space to be flagged and wrapped","title":"AbstractExpertActionSensor.flagged_group_space"},{"location":"api/allenact/base_abstractions/sensor/#expertactionsensor","text":"class ExpertActionSensor ( AbstractExpertActionSensor ) [view_source] (Deprecated) A sensor that obtains the expert action from a given task (if available).","title":"ExpertActionSensor"},{"location":"api/allenact/base_abstractions/sensor/#abstractexpertpolicysensor","text":"class AbstractExpertPolicySensor ( AbstractExpertSensor , abc . ABC ) [view_source]","title":"AbstractExpertPolicySensor"},{"location":"api/allenact/base_abstractions/sensor/#abstractexpertpolicysensorflagged_group_space","text":"| @classmethod | flagged_group_space ( cls , group_space : gym . spaces . Space ) -> gym . spaces . Dict [view_source] gym space resulting from wrapping the policy space corresponding to allenact.utils.spaces_utils.policy_space(group_space) together with a binary action space corresponding to an expert success flag, in a Dict space. Parameters group_space : The source action space to be used to derive a policy space, flagged and wrapped","title":"AbstractExpertPolicySensor.flagged_group_space"},{"location":"api/allenact/base_abstractions/sensor/#expertpolicysensor","text":"class ExpertPolicySensor ( AbstractExpertPolicySensor ) [view_source] (Deprecated) A sensor that obtains the expert policy from a given task (if available).","title":"ExpertPolicySensor"},{"location":"api/allenact/base_abstractions/task/","text":"allenact.base_abstractions.task # [view_source] Defines the primary data structures by which agents interact with their environment. Task # class Task ( Generic [ EnvType ]) [view_source] An abstract class defining a, goal directed, 'task.' Agents interact with their environment through a task by taking a step after which they receive new observations, rewards, and (potentially) other useful information. A Task is a helpful generalization of the OpenAI gym's Env class and allows for multiple tasks (e.g. point and object navigation) to be defined on a single environment (e.g. AI2-THOR). Attributes env : The environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : Dictionary of (k, v) pairs defining task goals and other task information. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors. Task.action_space # | @property | @abc . abstractmethod | action_space () -> gym . Space [view_source] Task's action space. Returns The action space for the task. Task.render # | @abc . abstractmethod | render ( mode : str = \"rgb\" , * args , ** kwargs ) -> np . ndarray [view_source] Render the current task state. Rendered task state can come in any supported modes. Parameters mode : The mode in which to render. For example, you might have a 'rgb' mode that renders the agent's egocentric viewpoint or a 'dev' mode returning additional information. args : Extra args. kwargs : Extra kwargs. Returns An numpy array corresponding to the requested render. Task.step # | step ( action : Any ) -> RLStepResult [view_source] Take an action in the environment (one per agent). Takes the action in the environment and returns observations (& rewards and any additional information) corresponding to the agent's new state. Note that this function should not be overwritten without care (instead implement the _step function). Parameters action : The action to take, should be of the same form as specified by self.action_space . Returns A RLStepResult object encoding the new observations, reward, and (possibly) additional information. Task.reached_max_steps # | reached_max_steps () -> bool [view_source] Has the agent reached the maximum number of steps. Task.reached_terminal_state # | @abc . abstractmethod | reached_terminal_state () -> bool [view_source] Has the agent reached a terminal state (excluding reaching the maximum number of steps). Task.is_done # | is_done () -> bool [view_source] Did the agent reach a terminal state or performed the maximum number of steps. Task.num_steps_taken # | num_steps_taken () -> int [view_source] Number of steps taken by the agent in the task so far. Task.action_names # | @deprecated | action_names () -> Tuple [ str , ... ] [view_source] Action names of the Task instance. This function has been deprecated and will be removed. This function is a hold-over from when the Task abstraction only considered gym.space.Discrete action spaces (in which case it makes sense name these actions). This implementation of action_names requires that a class_action_names method has been defined. This method should be overwritten if class_action_names requires key word arguments to determine the number of actions. Task.close # | @abc . abstractmethod | close () -> None [view_source] Closes the environment and any other files opened by the Task (if applicable). Task.metrics # | metrics () -> Dict [ str , Any ] [view_source] Computes metrics related to the task after the task's completion. By default this function is automatically called during training and the reported metrics logged to tensorboard. Returns A dictionary where every key is a string (the metric's name) and the value is the value of the metric. Task.query_expert # | query_expert ( ** kwargs ) -> Tuple [ Any , bool ] [view_source] (Deprecated) Query the expert policy for this task. The new correct way to include this functionality is through the definition of a class derived from allenact.base_abstractions.sensor.AbstractExpertActionSensor or allenact.base_abstractions.sensor.AbstractExpertPolicySensor , where a query_expert method must be defined. Returns A tuple (x, y) where x is the expert action (or policy) and y is False \\ if the expert could not determine the optimal action (otherwise True). Here y \\ is used for masking. Even when y is False, x should still lie in the space of \\ possible values (e.g. if x is the expert policy then x should be the correct length, \\ sum to 1, and have non-negative entries). Task.cumulative_reward # | @property | cumulative_reward () -> float [view_source] Mean per-agent total cumulative in the task so far. Returns Mean per-agent cumulative reward as a float. TaskSampler # class TaskSampler ( abc . ABC ) [view_source] Abstract class defining a how new tasks are sampled. TaskSampler.length # | @property | @abc . abstractmethod | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). TaskSampler.last_sampled_task # | @property | @abc . abstractmethod | last_sampled_task () -> Optional [ Task ] [view_source] Get the most recently sampled Task. Returns The most recently sampled Task. TaskSampler.next_task # | @abc . abstractmethod | next_task ( force_advance_scene : bool = False ) -> Optional [ Task ] [view_source] Get the next task in the sampler's stream. Parameters force_advance_scene : Used to (if applicable) force the task sampler to use a new scene for the next task. This is useful if, during training, you would like to train with one scene for some number of steps and then explicitly control when you begin training with the next scene. Returns The next Task in the sampler's stream if a next task exists. Otherwise None. TaskSampler.close # | @abc . abstractmethod | close () -> None [view_source] Closes any open environments or streams. Should be run when done sampling. TaskSampler.all_observation_spaces_equal # | @property | @abc . abstractmethod | all_observation_spaces_equal () -> bool [view_source] Checks if all observation spaces of tasks that can be sampled are equal. This will almost always simply return True . A case in which it should return False includes, for example, a setting where you design a TaskSampler that can generate different types of tasks, i.e. point navigation tasks and object navigation tasks. In this case, these different tasks may output different types of observations. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. TaskSampler.reset # | @abc . abstractmethod | reset () -> None [view_source] Resets task sampler to its original state (except for any seed). TaskSampler.set_seed # | @abc . abstractmethod | set_seed ( seed : int ) -> None [view_source] Sets new RNG seed. Parameters seed : New seed.","title":"task"},{"location":"api/allenact/base_abstractions/task/#allenactbase_abstractionstask","text":"[view_source] Defines the primary data structures by which agents interact with their environment.","title":"allenact.base_abstractions.task"},{"location":"api/allenact/base_abstractions/task/#task","text":"class Task ( Generic [ EnvType ]) [view_source] An abstract class defining a, goal directed, 'task.' Agents interact with their environment through a task by taking a step after which they receive new observations, rewards, and (potentially) other useful information. A Task is a helpful generalization of the OpenAI gym's Env class and allows for multiple tasks (e.g. point and object navigation) to be defined on a single environment (e.g. AI2-THOR). Attributes env : The environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : Dictionary of (k, v) pairs defining task goals and other task information. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors.","title":"Task"},{"location":"api/allenact/base_abstractions/task/#taskaction_space","text":"| @property | @abc . abstractmethod | action_space () -> gym . Space [view_source] Task's action space. Returns The action space for the task.","title":"Task.action_space"},{"location":"api/allenact/base_abstractions/task/#taskrender","text":"| @abc . abstractmethod | render ( mode : str = \"rgb\" , * args , ** kwargs ) -> np . ndarray [view_source] Render the current task state. Rendered task state can come in any supported modes. Parameters mode : The mode in which to render. For example, you might have a 'rgb' mode that renders the agent's egocentric viewpoint or a 'dev' mode returning additional information. args : Extra args. kwargs : Extra kwargs. Returns An numpy array corresponding to the requested render.","title":"Task.render"},{"location":"api/allenact/base_abstractions/task/#taskstep","text":"| step ( action : Any ) -> RLStepResult [view_source] Take an action in the environment (one per agent). Takes the action in the environment and returns observations (& rewards and any additional information) corresponding to the agent's new state. Note that this function should not be overwritten without care (instead implement the _step function). Parameters action : The action to take, should be of the same form as specified by self.action_space . Returns A RLStepResult object encoding the new observations, reward, and (possibly) additional information.","title":"Task.step"},{"location":"api/allenact/base_abstractions/task/#taskreached_max_steps","text":"| reached_max_steps () -> bool [view_source] Has the agent reached the maximum number of steps.","title":"Task.reached_max_steps"},{"location":"api/allenact/base_abstractions/task/#taskreached_terminal_state","text":"| @abc . abstractmethod | reached_terminal_state () -> bool [view_source] Has the agent reached a terminal state (excluding reaching the maximum number of steps).","title":"Task.reached_terminal_state"},{"location":"api/allenact/base_abstractions/task/#taskis_done","text":"| is_done () -> bool [view_source] Did the agent reach a terminal state or performed the maximum number of steps.","title":"Task.is_done"},{"location":"api/allenact/base_abstractions/task/#tasknum_steps_taken","text":"| num_steps_taken () -> int [view_source] Number of steps taken by the agent in the task so far.","title":"Task.num_steps_taken"},{"location":"api/allenact/base_abstractions/task/#taskaction_names","text":"| @deprecated | action_names () -> Tuple [ str , ... ] [view_source] Action names of the Task instance. This function has been deprecated and will be removed. This function is a hold-over from when the Task abstraction only considered gym.space.Discrete action spaces (in which case it makes sense name these actions). This implementation of action_names requires that a class_action_names method has been defined. This method should be overwritten if class_action_names requires key word arguments to determine the number of actions.","title":"Task.action_names"},{"location":"api/allenact/base_abstractions/task/#taskclose","text":"| @abc . abstractmethod | close () -> None [view_source] Closes the environment and any other files opened by the Task (if applicable).","title":"Task.close"},{"location":"api/allenact/base_abstractions/task/#taskmetrics","text":"| metrics () -> Dict [ str , Any ] [view_source] Computes metrics related to the task after the task's completion. By default this function is automatically called during training and the reported metrics logged to tensorboard. Returns A dictionary where every key is a string (the metric's name) and the value is the value of the metric.","title":"Task.metrics"},{"location":"api/allenact/base_abstractions/task/#taskquery_expert","text":"| query_expert ( ** kwargs ) -> Tuple [ Any , bool ] [view_source] (Deprecated) Query the expert policy for this task. The new correct way to include this functionality is through the definition of a class derived from allenact.base_abstractions.sensor.AbstractExpertActionSensor or allenact.base_abstractions.sensor.AbstractExpertPolicySensor , where a query_expert method must be defined. Returns A tuple (x, y) where x is the expert action (or policy) and y is False \\ if the expert could not determine the optimal action (otherwise True). Here y \\ is used for masking. Even when y is False, x should still lie in the space of \\ possible values (e.g. if x is the expert policy then x should be the correct length, \\ sum to 1, and have non-negative entries).","title":"Task.query_expert"},{"location":"api/allenact/base_abstractions/task/#taskcumulative_reward","text":"| @property | cumulative_reward () -> float [view_source] Mean per-agent total cumulative in the task so far. Returns Mean per-agent cumulative reward as a float.","title":"Task.cumulative_reward"},{"location":"api/allenact/base_abstractions/task/#tasksampler","text":"class TaskSampler ( abc . ABC ) [view_source] Abstract class defining a how new tasks are sampled.","title":"TaskSampler"},{"location":"api/allenact/base_abstractions/task/#tasksamplerlength","text":"| @property | @abc . abstractmethod | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"TaskSampler.length"},{"location":"api/allenact/base_abstractions/task/#tasksamplerlast_sampled_task","text":"| @property | @abc . abstractmethod | last_sampled_task () -> Optional [ Task ] [view_source] Get the most recently sampled Task. Returns The most recently sampled Task.","title":"TaskSampler.last_sampled_task"},{"location":"api/allenact/base_abstractions/task/#tasksamplernext_task","text":"| @abc . abstractmethod | next_task ( force_advance_scene : bool = False ) -> Optional [ Task ] [view_source] Get the next task in the sampler's stream. Parameters force_advance_scene : Used to (if applicable) force the task sampler to use a new scene for the next task. This is useful if, during training, you would like to train with one scene for some number of steps and then explicitly control when you begin training with the next scene. Returns The next Task in the sampler's stream if a next task exists. Otherwise None.","title":"TaskSampler.next_task"},{"location":"api/allenact/base_abstractions/task/#tasksamplerclose","text":"| @abc . abstractmethod | close () -> None [view_source] Closes any open environments or streams. Should be run when done sampling.","title":"TaskSampler.close"},{"location":"api/allenact/base_abstractions/task/#tasksamplerall_observation_spaces_equal","text":"| @property | @abc . abstractmethod | all_observation_spaces_equal () -> bool [view_source] Checks if all observation spaces of tasks that can be sampled are equal. This will almost always simply return True . A case in which it should return False includes, for example, a setting where you design a TaskSampler that can generate different types of tasks, i.e. point navigation tasks and object navigation tasks. In this case, these different tasks may output different types of observations. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"TaskSampler.all_observation_spaces_equal"},{"location":"api/allenact/base_abstractions/task/#tasksamplerreset","text":"| @abc . abstractmethod | reset () -> None [view_source] Resets task sampler to its original state (except for any seed).","title":"TaskSampler.reset"},{"location":"api/allenact/base_abstractions/task/#tasksamplerset_seed","text":"| @abc . abstractmethod | set_seed ( seed : int ) -> None [view_source] Sets new RNG seed. Parameters seed : New seed.","title":"TaskSampler.set_seed"},{"location":"api/allenact/embodiedai/mapping/mapping_losses/","text":"allenact.embodiedai.mapping.mapping_losses # [view_source] BinnedPointCloudMapLoss # class BinnedPointCloudMapLoss ( AbstractActorCriticLoss ) [view_source] A (binary cross entropy) loss for training metric maps for free space prediction. BinnedPointCloudMapLoss.__init__ # | __init__ ( binned_pc_uuid : str , map_logits_uuid : str ) [view_source] Initializer. Parameters binned_pc_uuid : The uuid of a sensor returning a dictionary with an \"egocentric_update\" key with the same format as returned by allenact.embodied_ai.mapping_utils.map_builders.BinnedPointCloudMapBuilder . Such a sensor can be found in the allenact_plugins library : see allenact_plugins.ithor_plugin.ithor_sensors.BinnedPointCloudMapTHORSensor . map_logits_uuid : key used to index into actor_critic_output.extras (returned by the model) whose value should be a tensor of the same shape as the tensor corresponding to the above \"egocentric_update\" key. SemanticMapFocalLoss # class SemanticMapFocalLoss ( AbstractActorCriticLoss ) [view_source] A (focal-loss based) loss for training metric maps for free space prediction. As semantic maps tend to be quite sparse this loss uses the focal loss (https://arxiv.org/abs/1708.02002) rather than binary cross entropy (BCE). If the gamma parameter is 0.0 then this is just the normal BCE, larger values of gamma result less and less emphasis being paid to examples that are already well classified. SemanticMapFocalLoss.__init__ # | __init__ ( semantic_map_uuid : str , map_logits_uuid : str , gamma : float = 2.0 ) [view_source] Initializer. Parameters semantic_map_uuid : The uuid of a sensor returning a dictionary with an \"egocentric_update\" key with the same format as returned by allenact.embodied_ai.mapping_utils.map_builders.SemanticMapBuilder . Such a sensor can be found in the allenact_plugins library : see allenact_plugins.ithor_plugin.ithor_sensors.SemanticMapTHORSensor . map_logits_uuid : key used to index into actor_critic_output.extras (returned by the model) whose value should be a tensor of the same shape as the tensor corresponding to the above \"egocentric_update\" key.","title":"mapping_losses"},{"location":"api/allenact/embodiedai/mapping/mapping_losses/#allenactembodiedaimappingmapping_losses","text":"[view_source]","title":"allenact.embodiedai.mapping.mapping_losses"},{"location":"api/allenact/embodiedai/mapping/mapping_losses/#binnedpointcloudmaploss","text":"class BinnedPointCloudMapLoss ( AbstractActorCriticLoss ) [view_source] A (binary cross entropy) loss for training metric maps for free space prediction.","title":"BinnedPointCloudMapLoss"},{"location":"api/allenact/embodiedai/mapping/mapping_losses/#binnedpointcloudmaploss__init__","text":"| __init__ ( binned_pc_uuid : str , map_logits_uuid : str ) [view_source] Initializer. Parameters binned_pc_uuid : The uuid of a sensor returning a dictionary with an \"egocentric_update\" key with the same format as returned by allenact.embodied_ai.mapping_utils.map_builders.BinnedPointCloudMapBuilder . Such a sensor can be found in the allenact_plugins library : see allenact_plugins.ithor_plugin.ithor_sensors.BinnedPointCloudMapTHORSensor . map_logits_uuid : key used to index into actor_critic_output.extras (returned by the model) whose value should be a tensor of the same shape as the tensor corresponding to the above \"egocentric_update\" key.","title":"BinnedPointCloudMapLoss.__init__"},{"location":"api/allenact/embodiedai/mapping/mapping_losses/#semanticmapfocalloss","text":"class SemanticMapFocalLoss ( AbstractActorCriticLoss ) [view_source] A (focal-loss based) loss for training metric maps for free space prediction. As semantic maps tend to be quite sparse this loss uses the focal loss (https://arxiv.org/abs/1708.02002) rather than binary cross entropy (BCE). If the gamma parameter is 0.0 then this is just the normal BCE, larger values of gamma result less and less emphasis being paid to examples that are already well classified.","title":"SemanticMapFocalLoss"},{"location":"api/allenact/embodiedai/mapping/mapping_losses/#semanticmapfocalloss__init__","text":"| __init__ ( semantic_map_uuid : str , map_logits_uuid : str , gamma : float = 2.0 ) [view_source] Initializer. Parameters semantic_map_uuid : The uuid of a sensor returning a dictionary with an \"egocentric_update\" key with the same format as returned by allenact.embodied_ai.mapping_utils.map_builders.SemanticMapBuilder . Such a sensor can be found in the allenact_plugins library : see allenact_plugins.ithor_plugin.ithor_sensors.SemanticMapTHORSensor . map_logits_uuid : key used to index into actor_critic_output.extras (returned by the model) whose value should be a tensor of the same shape as the tensor corresponding to the above \"egocentric_update\" key.","title":"SemanticMapFocalLoss.__init__"},{"location":"api/allenact/embodiedai/mapping/mapping_models/active_neural_slam/","text":"allenact.embodiedai.mapping.mapping_models.active_neural_slam # [view_source] ActiveNeuralSLAM # class ActiveNeuralSLAM ( nn . Module ) [view_source] Active Neural SLAM module. This is an implementation of the Active Neural SLAM module from: Chaplot, D.S., Gandhi, D., Gupta, S., Gupta, A. and Salakhutdinov, R., 2020. Learning To Explore Using Active Neural SLAM. In International Conference on Learning Representations (ICLR). Note that this is purely the mapping component and does not include the planning components from the above paper. This implementation is adapted from https://github.com/devendrachaplot/Neural-SLAM , we have extended this implementation to allow for an arbitrary number of output map channels (enabling semantic mapping). At a high level, this model takes as input RGB egocentric images and outputs metric map tensors of shape (# channels) x height x width where height/width correspond to the ground plane of the environment. ActiveNeuralSLAM.__init__ # | __init__ ( frame_height : int , frame_width : int , n_map_channels : int , resolution_in_cm : int = 5 , map_size_in_cm : int = 2400 , vision_range_in_cm : int = 300 , use_pose_estimation : bool = False , pretrained_resnet : bool = True , freeze_resnet_batchnorm : bool = True , use_resnet_layernorm : bool = False ) [view_source] Initialize an Active Neural SLAM module. Parameters frame_height : The height of the RGB images given to this module on calls to forward . frame_width : The width of the RGB images given to this module on calls to forward . n_map_channels : The number of output channels in the output maps. resolution_in_cm : The resolution of the output map, see map_size_in_cm . map_size_in_cm : The height & width of the map in centimeters. The size of the map tensor returned on calls to forward will be map_size_in_cm/resolution_in_cm . Note that map_size_in_cm must be an divisible by resolution_in_cm. vision_range_in_cm : Given an RGB image input, this module will transform this image into an \"egocentric map\" with height and width equaling vision_range_in_cm/resolution_in_cm . This egocentr map corresponds to the area of the world directly in front of the agent. This \"egocentric map\" will be rotated/translated into the allocentric reference frame and used to update the larger, allocentric, map whose height and width equal map_size_in_cm/resolution_in_cm . Thus this parameter controls how much of the map will be updated on every step. use_pose_estimation : Whether or not we should estimate the agent's change in position/rotation. If False , you'll need to provide the ground truth changes in position/rotation. pretrained_resnet : Whether or not to use ImageNet pre-trained model weights for the ResNet18 backbone. freeze_resnet_batchnorm : Whether or not the batch normalization layers in the ResNet18 backbone should be frozen and batchnorm updates disabled. You almost certainly want this to be True as using batch normalization during RL training results in all sorts of issues unless you're very careful. use_resnet_layernorm : If you've enabled freeze_resnet_batchnorm (recommended) you'll likely want to normalize the output from the ResNet18 model as we've found that these values can otherwise grow quite large harming learning. ActiveNeuralSLAM.forward # | forward ( images : Optional [ torch . Tensor ], last_map_probs_allocentric : Optional [ torch . Tensor ], last_xzrs_allocentric : Optional [ torch . Tensor ], dx_dz_drs_egocentric : Optional [ torch . Tensor ], last_map_logits_egocentric : Optional [ torch . Tensor ], return_allocentric_maps = True , resnet_image_features : Optional [ torch . Tensor ] = None ) [view_source] Create allocentric/egocentric maps predictions given RGB image inputs. Here it is assumed that last_xzrs_allocentric has been re-centered so that (x, z) == (0,0) corresponds to the top left of the returned map (with increasing x/z moving to the bottom right of the map). Note that all maps are oriented so that: * Increasing x values correspond to increasing columns in the map(s). * Increasing z values correspond to increasing rows in the map(s). Note that this may seem a bit weird as: * \"north\" is pointing downwards in the map, * if you picture yourself as the agent facing north (i.e. down) in the map, then moving to the right from the agent's perspective will correspond to increasing which column the agent is at: agent facing downwards - - > (dir. to the right of the agent, i.e. moving right corresponds to +cols) | | v (dir. agent faces, i.e. moving ahead corresponds to +rows) This may be the opposite of what you expect. Parameters images : A (# batches) x 3 x height x width tensor of RGB images. These should be __normalized for use with a resnet model. See here for information (see also the use_resnet_normalization parameter of the allenact.base_abstractions.sensor.RGBSensor sensor). last_map_probs_allocentric : A (# batches) x (map channels) x (map height) x (map width) tensor representing the colllection of allocentric maps to be updated. last_xzrs_allocentric : A (# batches) x 3 tensor where last_xzrs_allocentric[:, 0] are the agent's (allocentric) x-coordinates on the previous step, __ last_xzrs_allocentric[__:, 1] are the agent's (allocentric) z-coordinates from the previous __step, and last_xzrs_allocentric[__:, 2] are the agent's rotations (allocentric, in degrees) from the prevoius step. dx_dz_drs_egocentric : A (# batches) x 3 tensor representing the agent's change in x (in meters), z (in meters), and rotation (in degrees) from the previous step. Note that these changes are \"egocentric\" so that if the agent moved 1 meter ahead from it's perspective this should correspond to a dz of +1.0 regardless of the agent's orientation (similarly moving right would result in a dx of +1.0). This is ignored (and thus can be None ) if you are using pose estimation (i.e. self.use_pose_estimation is True ) or if return_allocentric_maps is False . last_map_logits_egocentric : The \"egocentric_update\" output when calling this function on the last agent's step. I.e. this should be the egocentric map view of the agent from the last step. This is used to compute the change in the agent's position rotation. This is ignored (and thus can be None ) if you do not wish to estimate the agent's pose (i.e. self.use_pose_estimation is False ). return_allocentric_maps : Whether or not to generate new allocentric maps given last_map_probs_allocentric and the new map estimates. Creating these new allocentric maps is expensive so better avoided when not needed. resnet_image_features : Sometimes you may wish to compute the ResNet image features yourself for use in another part of your model. Rather than having to recompute them multiple times, you can instead compute them once and pass them into this forward call (in this case the input images parameter is ignored). Note that if you're using the self.resnet_l5 module to compute these features, be sure to also normalize them with self.resnet_normalizer if you have opted to use_resnet_layernorm when initializing this module). Returns A dictionary with keys/values : * \"egocentric_update\" - The egocentric map view for the given RGB image. This is what should be used for computing losses in general. * \"map_logits_probs_update_no_grad\" - The egocentric map view after it has been rotated, translated, and moved into a full-sized allocentric map. This map has been detached from the computation graph and so should not be used for gradient computations. This will be None if return_allocentric_maps was False . * \"map_logits_probs_no_grad\" - The newly updated allocentric map, this corresponds to performing a pointwise maximum between last_map_probs_allocentric and the above returned map_probs_allocentric_update_no_grad . This will be None if return_allocentric_maps was False . * \"dx_dz_dr_egocentric_preds\" - The predicted change in x, z, and rotation of the agent (from the egocentric perspective of the agent). * \"xzr_allocentric_preds\" - The (predicted if self.use_pose_estimation == True ) allocentric (x, z) position and rotation of the agent. This will equal None if self.use_pose_estimation == False and dx_dz_drs_egocentric is None .","title":"active_neural_slam"},{"location":"api/allenact/embodiedai/mapping/mapping_models/active_neural_slam/#allenactembodiedaimappingmapping_modelsactive_neural_slam","text":"[view_source]","title":"allenact.embodiedai.mapping.mapping_models.active_neural_slam"},{"location":"api/allenact/embodiedai/mapping/mapping_models/active_neural_slam/#activeneuralslam","text":"class ActiveNeuralSLAM ( nn . Module ) [view_source] Active Neural SLAM module. This is an implementation of the Active Neural SLAM module from: Chaplot, D.S., Gandhi, D., Gupta, S., Gupta, A. and Salakhutdinov, R., 2020. Learning To Explore Using Active Neural SLAM. In International Conference on Learning Representations (ICLR). Note that this is purely the mapping component and does not include the planning components from the above paper. This implementation is adapted from https://github.com/devendrachaplot/Neural-SLAM , we have extended this implementation to allow for an arbitrary number of output map channels (enabling semantic mapping). At a high level, this model takes as input RGB egocentric images and outputs metric map tensors of shape (# channels) x height x width where height/width correspond to the ground plane of the environment.","title":"ActiveNeuralSLAM"},{"location":"api/allenact/embodiedai/mapping/mapping_models/active_neural_slam/#activeneuralslam__init__","text":"| __init__ ( frame_height : int , frame_width : int , n_map_channels : int , resolution_in_cm : int = 5 , map_size_in_cm : int = 2400 , vision_range_in_cm : int = 300 , use_pose_estimation : bool = False , pretrained_resnet : bool = True , freeze_resnet_batchnorm : bool = True , use_resnet_layernorm : bool = False ) [view_source] Initialize an Active Neural SLAM module. Parameters frame_height : The height of the RGB images given to this module on calls to forward . frame_width : The width of the RGB images given to this module on calls to forward . n_map_channels : The number of output channels in the output maps. resolution_in_cm : The resolution of the output map, see map_size_in_cm . map_size_in_cm : The height & width of the map in centimeters. The size of the map tensor returned on calls to forward will be map_size_in_cm/resolution_in_cm . Note that map_size_in_cm must be an divisible by resolution_in_cm. vision_range_in_cm : Given an RGB image input, this module will transform this image into an \"egocentric map\" with height and width equaling vision_range_in_cm/resolution_in_cm . This egocentr map corresponds to the area of the world directly in front of the agent. This \"egocentric map\" will be rotated/translated into the allocentric reference frame and used to update the larger, allocentric, map whose height and width equal map_size_in_cm/resolution_in_cm . Thus this parameter controls how much of the map will be updated on every step. use_pose_estimation : Whether or not we should estimate the agent's change in position/rotation. If False , you'll need to provide the ground truth changes in position/rotation. pretrained_resnet : Whether or not to use ImageNet pre-trained model weights for the ResNet18 backbone. freeze_resnet_batchnorm : Whether or not the batch normalization layers in the ResNet18 backbone should be frozen and batchnorm updates disabled. You almost certainly want this to be True as using batch normalization during RL training results in all sorts of issues unless you're very careful. use_resnet_layernorm : If you've enabled freeze_resnet_batchnorm (recommended) you'll likely want to normalize the output from the ResNet18 model as we've found that these values can otherwise grow quite large harming learning.","title":"ActiveNeuralSLAM.__init__"},{"location":"api/allenact/embodiedai/mapping/mapping_models/active_neural_slam/#activeneuralslamforward","text":"| forward ( images : Optional [ torch . Tensor ], last_map_probs_allocentric : Optional [ torch . Tensor ], last_xzrs_allocentric : Optional [ torch . Tensor ], dx_dz_drs_egocentric : Optional [ torch . Tensor ], last_map_logits_egocentric : Optional [ torch . Tensor ], return_allocentric_maps = True , resnet_image_features : Optional [ torch . Tensor ] = None ) [view_source] Create allocentric/egocentric maps predictions given RGB image inputs. Here it is assumed that last_xzrs_allocentric has been re-centered so that (x, z) == (0,0) corresponds to the top left of the returned map (with increasing x/z moving to the bottom right of the map). Note that all maps are oriented so that: * Increasing x values correspond to increasing columns in the map(s). * Increasing z values correspond to increasing rows in the map(s). Note that this may seem a bit weird as: * \"north\" is pointing downwards in the map, * if you picture yourself as the agent facing north (i.e. down) in the map, then moving to the right from the agent's perspective will correspond to increasing which column the agent is at: agent facing downwards - - > (dir. to the right of the agent, i.e. moving right corresponds to +cols) | | v (dir. agent faces, i.e. moving ahead corresponds to +rows) This may be the opposite of what you expect. Parameters images : A (# batches) x 3 x height x width tensor of RGB images. These should be __normalized for use with a resnet model. See here for information (see also the use_resnet_normalization parameter of the allenact.base_abstractions.sensor.RGBSensor sensor). last_map_probs_allocentric : A (# batches) x (map channels) x (map height) x (map width) tensor representing the colllection of allocentric maps to be updated. last_xzrs_allocentric : A (# batches) x 3 tensor where last_xzrs_allocentric[:, 0] are the agent's (allocentric) x-coordinates on the previous step, __ last_xzrs_allocentric[__:, 1] are the agent's (allocentric) z-coordinates from the previous __step, and last_xzrs_allocentric[__:, 2] are the agent's rotations (allocentric, in degrees) from the prevoius step. dx_dz_drs_egocentric : A (# batches) x 3 tensor representing the agent's change in x (in meters), z (in meters), and rotation (in degrees) from the previous step. Note that these changes are \"egocentric\" so that if the agent moved 1 meter ahead from it's perspective this should correspond to a dz of +1.0 regardless of the agent's orientation (similarly moving right would result in a dx of +1.0). This is ignored (and thus can be None ) if you are using pose estimation (i.e. self.use_pose_estimation is True ) or if return_allocentric_maps is False . last_map_logits_egocentric : The \"egocentric_update\" output when calling this function on the last agent's step. I.e. this should be the egocentric map view of the agent from the last step. This is used to compute the change in the agent's position rotation. This is ignored (and thus can be None ) if you do not wish to estimate the agent's pose (i.e. self.use_pose_estimation is False ). return_allocentric_maps : Whether or not to generate new allocentric maps given last_map_probs_allocentric and the new map estimates. Creating these new allocentric maps is expensive so better avoided when not needed. resnet_image_features : Sometimes you may wish to compute the ResNet image features yourself for use in another part of your model. Rather than having to recompute them multiple times, you can instead compute them once and pass them into this forward call (in this case the input images parameter is ignored). Note that if you're using the self.resnet_l5 module to compute these features, be sure to also normalize them with self.resnet_normalizer if you have opted to use_resnet_layernorm when initializing this module). Returns A dictionary with keys/values : * \"egocentric_update\" - The egocentric map view for the given RGB image. This is what should be used for computing losses in general. * \"map_logits_probs_update_no_grad\" - The egocentric map view after it has been rotated, translated, and moved into a full-sized allocentric map. This map has been detached from the computation graph and so should not be used for gradient computations. This will be None if return_allocentric_maps was False . * \"map_logits_probs_no_grad\" - The newly updated allocentric map, this corresponds to performing a pointwise maximum between last_map_probs_allocentric and the above returned map_probs_allocentric_update_no_grad . This will be None if return_allocentric_maps was False . * \"dx_dz_dr_egocentric_preds\" - The predicted change in x, z, and rotation of the agent (from the egocentric perspective of the agent). * \"xzr_allocentric_preds\" - The (predicted if self.use_pose_estimation == True ) allocentric (x, z) position and rotation of the agent. This will equal None if self.use_pose_estimation == False and dx_dz_drs_egocentric is None .","title":"ActiveNeuralSLAM.forward"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/map_builders/","text":"allenact.embodiedai.mapping.mapping_utils.map_builders # [view_source] BinnedPointCloudMapBuilder # class BinnedPointCloudMapBuilder ( object ) [view_source] Class used to iteratively construct a map of \"free space\" based on input depth maps (i.e. pointclouds). Adapted from https://github.com/devendrachaplot/Neural-SLAM This class can be used to (iteratively) construct a metric map of free space in an environment as an agent moves around. After every step the agent takes, you should call the update function and pass the agent's egocentric depth image along with the agent's new position. This depth map will be converted into a pointcloud, binned along the up/down axis, and then projected onto a 3-dimensional tensor of shape (HxWxC) whose where HxW represent the ground plane and where C equals the number of bins the up-down coordinate was binned into. This 3d map counts the number of points in each bin. Thus a lack of points within a region can be used to infer that that region is free space. Attributes fov : FOV of the camera used to produce the depth images given when calling update . vision_range_in_map_units : The maximum distance (in number of rows/columns) that will be updated when calling update , points outside of this map vision range are ignored. map_size_in_cm : Total map size in cm. resolution_in_cm : Number of cm per row/column in the map. height_bins : The bins used to bin the up-down coordinate (for us the y-coordinate). For example, if height_bins = [0.1, 1] then all y-values < 0.1 will be mapped to 0, all y values in [0.1, 1) will be mapped to 1, and all y-values >= 1 will be mapped to 2. **Importantly :** these y-values will first be recentered by the min_xyz value passed when calling reset(...) . device : A torch.device on which to run computations. If this device is a GPU you can potentially obtain significant speed-ups. BinnedPointCloudMapBuilder.update # | update ( depth_frame : np . ndarray , camera_xyz : np . ndarray , camera_rotation : float , camera_horizon : float ) -> Dict [ str , np . ndarray ] [view_source] Updates the map with the input depth frame from the agent. See the allenact.embodiedai.mapping.mapping_utils.point_cloud_utils.project_point_cloud_to_map function for more information input parameter definitions. We assume that the input depth_frame has depths recorded in meters . Returns Let map_size = self.map_size_in_cm // self.resolution_in_cm . Returns a dictionary with keys-values : \"egocentric_update\" - A tensor of shape (vision_range_in_map_units)x(vision_range_in_map_units)x(len(self.height_bins) + 1) corresponding to the binned pointcloud after having been centered on the agent and rotated so that points ahead of the agent correspond to larger row indices and points further to the right of the agent correspond to larger column indices. Note that by \"centered\" we mean that one can picture the agent as being positioned at (0, vision_range_in_map_units/2) and facing downward. Each entry in this tensor is a count equaling the number of points in the pointcloud that, once binned, fell into this entry. This is likely the output you want to use if you want to build a model to predict free space from an image. \"allocentric_update\" - A (map_size)x(map_size)x(len(self.height_bins) + 1) corresponding to \"egocentric_update\" but rotated to the world-space coordinates. This allocentric_update is what is used to update the internally stored representation of the map. \"map\" - A (map_size)x(map_size)x(len(self.height_bins) + 1) tensor corresponding to the sum of all \"allocentric_update\" values since the last reset() . <a name= \"allenact.embodiedai.mapping.mapping_utils.map_builders.BinnedPointCloudMapBuilder.reset\" ></a> ### `BinnedPointCloudMapBuilder.reset` ```python | reset(min_xyz: np.ndarray) [view_source] Reset the map. Resets the internally stored map. Parameters min_xyz : An array of size (3,) corresponding to the minimum possible x, y, and z values that will be observed as a point in a pointcloud when calling .update(...) . The (world-space) maps returned by calls to update will have been normalized so the (0,0, :) entry corresponds to these minimum values. ObjectHull2d # class ObjectHull2d () [view_source] ObjectHull2d.__init__ # | __init__ ( object_id : str , object_type : str , hull_points : Union [ np . ndarray , Sequence [ Sequence [ float ]]]) [view_source] A class used to represent 2d convex hulls of objects when projected to the ground plane. Parameters object_id : A unique id for the object. object_type : The type of the object. hull_points : A Nx2 matrix with hull_points[:, 0] being the x coordinates and hull_points[:, 1] being the z coordinates (this is using the Unity game engine conventions where the y axis is up/down). SemanticMapBuilder # class SemanticMapBuilder ( object ) [view_source] Class used to iteratively construct a semantic map based on input depth maps (i.e. pointclouds). Adapted from https://github.com/devendrachaplot/Neural-SLAM This class can be used to (iteratively) construct a semantic map of objects in the environment. This map is similar to that generated by BinnedPointCloudMapBuilder (see its documentation for more information) but the various channels correspond to different object types. Thus if the (i,j,k) entry of a map generated by this function is True , this means that an object of type k is present in position i,j in the map. In particular, by \"present\" we mean that, after projecting the object to the ground plane and taking the convex hull of the resulting 2d object, a non-trivial portion of this convex hull overlaps the i,j position. For attribute information, see the documentation of the BinnedPointCloudMapBuilder class. The only attribute present in this class that is not present in BinnedPointCloudMapBuilder is ordered_object_types which corresponds to a list of unique object types where object type ordered_object_types[i] will correspond to the i th channel of the map generated by this class. SemanticMapBuilder.update # | update ( depth_frame : np . ndarray , camera_xyz : np . ndarray , camera_rotation : float , camera_horizon : float ) -> Dict [ str , np . ndarray ] [view_source] Updates the map with the input depth frame from the agent. See the documentation for BinnedPointCloudMapBuilder.update , the inputs and outputs are similar except that channels are used to represent the presence/absence of objects of given types. Unlike BinnedPointCloudMapBuilder.update , this function also returns two masks with keys \"egocentric_mask\" and \"mask\" that can be used to determine what portions of the map have been observed by the agent so far in the egocentric and world-space reference frames respectively. SemanticMapBuilder.reset # | reset ( min_xyz : np . ndarray , object_hulls : Sequence [ ObjectHull2d ]) [view_source] Reset the map. Resets the internally stored map. Parameters min_xyz : An array of size (3,) corresponding to the minimum possible x, y, and z values that will be observed as a point in a pointcloud when calling .update(...) . The (world-space) maps returned by calls to update will have been normalized so the (0,0, :) entry corresponds to these minimum values. object_hulls : The object hulls corresponding to objects in the scene. These will be used to construct the map.","title":"map_builders"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/map_builders/#allenactembodiedaimappingmapping_utilsmap_builders","text":"[view_source]","title":"allenact.embodiedai.mapping.mapping_utils.map_builders"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/map_builders/#binnedpointcloudmapbuilder","text":"class BinnedPointCloudMapBuilder ( object ) [view_source] Class used to iteratively construct a map of \"free space\" based on input depth maps (i.e. pointclouds). Adapted from https://github.com/devendrachaplot/Neural-SLAM This class can be used to (iteratively) construct a metric map of free space in an environment as an agent moves around. After every step the agent takes, you should call the update function and pass the agent's egocentric depth image along with the agent's new position. This depth map will be converted into a pointcloud, binned along the up/down axis, and then projected onto a 3-dimensional tensor of shape (HxWxC) whose where HxW represent the ground plane and where C equals the number of bins the up-down coordinate was binned into. This 3d map counts the number of points in each bin. Thus a lack of points within a region can be used to infer that that region is free space. Attributes fov : FOV of the camera used to produce the depth images given when calling update . vision_range_in_map_units : The maximum distance (in number of rows/columns) that will be updated when calling update , points outside of this map vision range are ignored. map_size_in_cm : Total map size in cm. resolution_in_cm : Number of cm per row/column in the map. height_bins : The bins used to bin the up-down coordinate (for us the y-coordinate). For example, if height_bins = [0.1, 1] then all y-values < 0.1 will be mapped to 0, all y values in [0.1, 1) will be mapped to 1, and all y-values >= 1 will be mapped to 2. **Importantly :** these y-values will first be recentered by the min_xyz value passed when calling reset(...) . device : A torch.device on which to run computations. If this device is a GPU you can potentially obtain significant speed-ups.","title":"BinnedPointCloudMapBuilder"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/map_builders/#binnedpointcloudmapbuilderupdate","text":"| update ( depth_frame : np . ndarray , camera_xyz : np . ndarray , camera_rotation : float , camera_horizon : float ) -> Dict [ str , np . ndarray ] [view_source] Updates the map with the input depth frame from the agent. See the allenact.embodiedai.mapping.mapping_utils.point_cloud_utils.project_point_cloud_to_map function for more information input parameter definitions. We assume that the input depth_frame has depths recorded in meters . Returns Let map_size = self.map_size_in_cm // self.resolution_in_cm . Returns a dictionary with keys-values : \"egocentric_update\" - A tensor of shape (vision_range_in_map_units)x(vision_range_in_map_units)x(len(self.height_bins) + 1) corresponding to the binned pointcloud after having been centered on the agent and rotated so that points ahead of the agent correspond to larger row indices and points further to the right of the agent correspond to larger column indices. Note that by \"centered\" we mean that one can picture the agent as being positioned at (0, vision_range_in_map_units/2) and facing downward. Each entry in this tensor is a count equaling the number of points in the pointcloud that, once binned, fell into this entry. This is likely the output you want to use if you want to build a model to predict free space from an image. \"allocentric_update\" - A (map_size)x(map_size)x(len(self.height_bins) + 1) corresponding to \"egocentric_update\" but rotated to the world-space coordinates. This allocentric_update is what is used to update the internally stored representation of the map. \"map\" - A (map_size)x(map_size)x(len(self.height_bins) + 1) tensor corresponding to the sum of all \"allocentric_update\" values since the last reset() . <a name= \"allenact.embodiedai.mapping.mapping_utils.map_builders.BinnedPointCloudMapBuilder.reset\" ></a> ### `BinnedPointCloudMapBuilder.reset` ```python | reset(min_xyz: np.ndarray) [view_source] Reset the map. Resets the internally stored map. Parameters min_xyz : An array of size (3,) corresponding to the minimum possible x, y, and z values that will be observed as a point in a pointcloud when calling .update(...) . The (world-space) maps returned by calls to update will have been normalized so the (0,0, :) entry corresponds to these minimum values.","title":"BinnedPointCloudMapBuilder.update"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/map_builders/#objecthull2d","text":"class ObjectHull2d () [view_source]","title":"ObjectHull2d"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/map_builders/#objecthull2d__init__","text":"| __init__ ( object_id : str , object_type : str , hull_points : Union [ np . ndarray , Sequence [ Sequence [ float ]]]) [view_source] A class used to represent 2d convex hulls of objects when projected to the ground plane. Parameters object_id : A unique id for the object. object_type : The type of the object. hull_points : A Nx2 matrix with hull_points[:, 0] being the x coordinates and hull_points[:, 1] being the z coordinates (this is using the Unity game engine conventions where the y axis is up/down).","title":"ObjectHull2d.__init__"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/map_builders/#semanticmapbuilder","text":"class SemanticMapBuilder ( object ) [view_source] Class used to iteratively construct a semantic map based on input depth maps (i.e. pointclouds). Adapted from https://github.com/devendrachaplot/Neural-SLAM This class can be used to (iteratively) construct a semantic map of objects in the environment. This map is similar to that generated by BinnedPointCloudMapBuilder (see its documentation for more information) but the various channels correspond to different object types. Thus if the (i,j,k) entry of a map generated by this function is True , this means that an object of type k is present in position i,j in the map. In particular, by \"present\" we mean that, after projecting the object to the ground plane and taking the convex hull of the resulting 2d object, a non-trivial portion of this convex hull overlaps the i,j position. For attribute information, see the documentation of the BinnedPointCloudMapBuilder class. The only attribute present in this class that is not present in BinnedPointCloudMapBuilder is ordered_object_types which corresponds to a list of unique object types where object type ordered_object_types[i] will correspond to the i th channel of the map generated by this class.","title":"SemanticMapBuilder"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/map_builders/#semanticmapbuilderupdate","text":"| update ( depth_frame : np . ndarray , camera_xyz : np . ndarray , camera_rotation : float , camera_horizon : float ) -> Dict [ str , np . ndarray ] [view_source] Updates the map with the input depth frame from the agent. See the documentation for BinnedPointCloudMapBuilder.update , the inputs and outputs are similar except that channels are used to represent the presence/absence of objects of given types. Unlike BinnedPointCloudMapBuilder.update , this function also returns two masks with keys \"egocentric_mask\" and \"mask\" that can be used to determine what portions of the map have been observed by the agent so far in the egocentric and world-space reference frames respectively.","title":"SemanticMapBuilder.update"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/map_builders/#semanticmapbuilderreset","text":"| reset ( min_xyz : np . ndarray , object_hulls : Sequence [ ObjectHull2d ]) [view_source] Reset the map. Resets the internally stored map. Parameters min_xyz : An array of size (3,) corresponding to the minimum possible x, y, and z values that will be observed as a point in a pointcloud when calling .update(...) . The (world-space) maps returned by calls to update will have been normalized so the (0,0, :) entry corresponds to these minimum values. object_hulls : The object hulls corresponding to objects in the scene. These will be used to construct the map.","title":"SemanticMapBuilder.reset"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/point_cloud_utils/","text":"allenact.embodiedai.mapping.mapping_utils.point_cloud_utils # [view_source] camera_space_xyz_to_world_xyz # camera_space_xyz_to_world_xyz ( camera_space_xyzs : torch . Tensor , camera_world_xyz : torch . Tensor , rotation : float , horizon : float ) -> torch . Tensor [view_source] Transforms xyz coordinates in the camera's coordinate frame to world- space (global) xyz frame. This code has been adapted from https://github.com/devendrachaplot/Neural-SLAM. IMPORTANT: We use the conventions from the Unity game engine. In particular: A rotation of 0 corresponds to facing north. Positive rotations correspond to CLOCKWISE rotations. That is a rotation of 90 degrees corresponds to facing east. THIS IS THE OPPOSITE CONVENTION OF THE ONE GENERALLY USED IN MATHEMATICS. When facing NORTH (rotation==0) moving ahead by 1 meter results in the the z coordinate increasing by 1. Moving to the right by 1 meter corresponds to increasing the x coordinate by 1. Finally moving upwards by 1 meter corresponds to increasing the y coordinate by 1. Having x,z as the ground plane in this way is common in computer graphics but is different than the usual mathematical convention of having z be \"up\". The horizon corresponds to how far below the horizontal the camera is facing. I.e. a horizon of 30 corresponds to the camera being angled downwards at an angle of 30 degrees. Parameters camera_space_xyzs : A 3xN matrix of xyz coordinates in the camera's reference frame. __Here x, y, z = camera_space_xyzs[__:, i] should equal the xyz coordinates for the ith point. camera_world_xyz : The camera's xyz position in the world reference frame. rotation : The world-space rotation (in degrees) of the camera. horizon : The horizon (in degrees) of the camera. Returns 3xN tensor with entry [ :, i] is the xyz world-space coordinate corresponding to the camera-space coordinate camera_space_xyzs[ :, i] depth_frame_to_camera_space_xyz # depth_frame_to_camera_space_xyz ( depth_frame : torch . Tensor , mask : Optional [ torch . Tensor ], fov : float = 90 ) -> torch . Tensor [view_source] Transforms a input depth map into a collection of xyz points (i.e. a point cloud) in the camera's coordinate frame. Parameters depth_frame : A square depth map, i.e. an MxM matrix with entry depth_frame[i, j] equaling the distance from the camera to nearest surface at pixel (i,j). mask : An optional boolean mask of the same size (MxM) as the input depth. Only values where this mask are true will be included in the returned matrix of xyz coordinates. If None then no pixels will be masked out (so the returned matrix of xyz points will have dimension 3x(M*M) fov : The field of view of the camera. Returns A 3xN matrix with entry [ :, i] equalling a the xyz coordinates (in the camera's coordinate frame) of a point in the point cloud corresponding to the input depth frame. depth_frame_to_world_space_xyz # depth_frame_to_world_space_xyz ( depth_frame : torch . Tensor , camera_world_xyz : torch . Tensor , rotation : float , horizon : float , fov : float ) [view_source] Transforms a input depth map into a collection of xyz points (i.e. a point cloud) in the world-space coordinate frame. IMPORTANT: We use the conventions from the Unity game engine. In particular: A rotation of 0 corresponds to facing north. Positive rotations correspond to CLOCKWISE rotations. That is a rotation of 90 degrees corresponds to facing east. THIS IS THE OPPOSITE CONVENTION OF THE ONE GENERALLY USED IN MATHEMATICS. When facing NORTH (rotation==0) moving ahead by 1 meter results in the the z coordinate increasing by 1. Moving to the right by 1 meter corresponds to increasing the x coordinate by 1. Finally moving upwards by 1 meter corresponds to increasing the y coordinate by 1. Having x,z as the ground plane in this way is common in computer graphics but is different than the usual mathematical convention of having z be \"up\". The horizon corresponds to how far below the horizontal the camera is facing. I.e. a horizon of 30 corresponds to the camera being angled downwards at an angle of 30 degrees. Parameters depth_frame : A square depth map, i.e. an MxM matrix with entry depth_frame[i, j] equaling the distance from the camera to nearest surface at pixel (i,j). mask : An optional boolean mask of the same size (MxM) as the input depth. Only values where this mask are true will be included in the returned matrix of xyz coordinates. If None then no pixels will be masked out (so the returned matrix of xyz points will have dimension 3x(M*M) camera_space_xyzs : A 3xN matrix of xyz coordinates in the camera's reference frame. __Here x, y, z = camera_space_xyzs[__:, i] should equal the xyz coordinates for the ith point. camera_world_xyz : The camera's xyz position in the world reference frame. rotation : The world-space rotation (in degrees) of the camera. horizon : The horizon (in degrees) of the camera. fov : The field of view of the camera. Returns A 3xN matrix with entry [ :, i] equalling a the xyz coordinates (in the world coordinate frame) of a point in the point cloud corresponding to the input depth frame. project_point_cloud_to_map # project_point_cloud_to_map ( xyz_points : torch . Tensor , bin_axis : str , bins : Sequence [ float ], map_size : int , resolution_in_cm : int , flip_row_col : bool ) [view_source] Bins an input point cloud into a map tensor with the bins equaling the channels. This code has been adapted from https://github.com/devendrachaplot/Neural-SLAM. Parameters xyz_points : (x,y,z) pointcloud(s) as a torch.Tensor of shape (... x height x width x 3). All operations are vectorized across the ... dimensions. bin_axis : Either \"x\", \"y\", or \"z\", the axis which should be binned by the values in bins . If you have generated your point clouds with any of the other functions in the point_cloud_utils module you almost certainly want this to be \"y\" as this is the default upwards dimension. bins : The values by which to bin along bin_axis , see the bins parameter of np.digitize for more info. map_size : The axes not specified by bin_axis will be be divided by resolution_in_cm / 100 and then rounded to the nearest integer. They are then expected to have their values within the interval [0, ..., map_size - 1]. resolution_in_cm : The resolution_in_cm, in cm, of the map output from this function. Every grid square of the map corresponds to a ( resolution_in_cm x resolution_in_cm ) square in space. flip_row_col : Should the rows/cols of the map be flipped? See the 'Returns' section below for more info. Returns A collection of maps of shape (... x map_size x map_size x (len(bins)+1)), note that bin_axis has been moved to the last index of this returned map, the other two axes stay in their original order unless flip_row_col has been called in which case they are reversed (useful as often rows should correspond to y or z instead of x).","title":"point_cloud_utils"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/point_cloud_utils/#allenactembodiedaimappingmapping_utilspoint_cloud_utils","text":"[view_source]","title":"allenact.embodiedai.mapping.mapping_utils.point_cloud_utils"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/point_cloud_utils/#camera_space_xyz_to_world_xyz","text":"camera_space_xyz_to_world_xyz ( camera_space_xyzs : torch . Tensor , camera_world_xyz : torch . Tensor , rotation : float , horizon : float ) -> torch . Tensor [view_source] Transforms xyz coordinates in the camera's coordinate frame to world- space (global) xyz frame. This code has been adapted from https://github.com/devendrachaplot/Neural-SLAM. IMPORTANT: We use the conventions from the Unity game engine. In particular: A rotation of 0 corresponds to facing north. Positive rotations correspond to CLOCKWISE rotations. That is a rotation of 90 degrees corresponds to facing east. THIS IS THE OPPOSITE CONVENTION OF THE ONE GENERALLY USED IN MATHEMATICS. When facing NORTH (rotation==0) moving ahead by 1 meter results in the the z coordinate increasing by 1. Moving to the right by 1 meter corresponds to increasing the x coordinate by 1. Finally moving upwards by 1 meter corresponds to increasing the y coordinate by 1. Having x,z as the ground plane in this way is common in computer graphics but is different than the usual mathematical convention of having z be \"up\". The horizon corresponds to how far below the horizontal the camera is facing. I.e. a horizon of 30 corresponds to the camera being angled downwards at an angle of 30 degrees. Parameters camera_space_xyzs : A 3xN matrix of xyz coordinates in the camera's reference frame. __Here x, y, z = camera_space_xyzs[__:, i] should equal the xyz coordinates for the ith point. camera_world_xyz : The camera's xyz position in the world reference frame. rotation : The world-space rotation (in degrees) of the camera. horizon : The horizon (in degrees) of the camera. Returns 3xN tensor with entry [ :, i] is the xyz world-space coordinate corresponding to the camera-space coordinate camera_space_xyzs[ :, i]","title":"camera_space_xyz_to_world_xyz"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/point_cloud_utils/#depth_frame_to_camera_space_xyz","text":"depth_frame_to_camera_space_xyz ( depth_frame : torch . Tensor , mask : Optional [ torch . Tensor ], fov : float = 90 ) -> torch . Tensor [view_source] Transforms a input depth map into a collection of xyz points (i.e. a point cloud) in the camera's coordinate frame. Parameters depth_frame : A square depth map, i.e. an MxM matrix with entry depth_frame[i, j] equaling the distance from the camera to nearest surface at pixel (i,j). mask : An optional boolean mask of the same size (MxM) as the input depth. Only values where this mask are true will be included in the returned matrix of xyz coordinates. If None then no pixels will be masked out (so the returned matrix of xyz points will have dimension 3x(M*M) fov : The field of view of the camera. Returns A 3xN matrix with entry [ :, i] equalling a the xyz coordinates (in the camera's coordinate frame) of a point in the point cloud corresponding to the input depth frame.","title":"depth_frame_to_camera_space_xyz"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/point_cloud_utils/#depth_frame_to_world_space_xyz","text":"depth_frame_to_world_space_xyz ( depth_frame : torch . Tensor , camera_world_xyz : torch . Tensor , rotation : float , horizon : float , fov : float ) [view_source] Transforms a input depth map into a collection of xyz points (i.e. a point cloud) in the world-space coordinate frame. IMPORTANT: We use the conventions from the Unity game engine. In particular: A rotation of 0 corresponds to facing north. Positive rotations correspond to CLOCKWISE rotations. That is a rotation of 90 degrees corresponds to facing east. THIS IS THE OPPOSITE CONVENTION OF THE ONE GENERALLY USED IN MATHEMATICS. When facing NORTH (rotation==0) moving ahead by 1 meter results in the the z coordinate increasing by 1. Moving to the right by 1 meter corresponds to increasing the x coordinate by 1. Finally moving upwards by 1 meter corresponds to increasing the y coordinate by 1. Having x,z as the ground plane in this way is common in computer graphics but is different than the usual mathematical convention of having z be \"up\". The horizon corresponds to how far below the horizontal the camera is facing. I.e. a horizon of 30 corresponds to the camera being angled downwards at an angle of 30 degrees. Parameters depth_frame : A square depth map, i.e. an MxM matrix with entry depth_frame[i, j] equaling the distance from the camera to nearest surface at pixel (i,j). mask : An optional boolean mask of the same size (MxM) as the input depth. Only values where this mask are true will be included in the returned matrix of xyz coordinates. If None then no pixels will be masked out (so the returned matrix of xyz points will have dimension 3x(M*M) camera_space_xyzs : A 3xN matrix of xyz coordinates in the camera's reference frame. __Here x, y, z = camera_space_xyzs[__:, i] should equal the xyz coordinates for the ith point. camera_world_xyz : The camera's xyz position in the world reference frame. rotation : The world-space rotation (in degrees) of the camera. horizon : The horizon (in degrees) of the camera. fov : The field of view of the camera. Returns A 3xN matrix with entry [ :, i] equalling a the xyz coordinates (in the world coordinate frame) of a point in the point cloud corresponding to the input depth frame.","title":"depth_frame_to_world_space_xyz"},{"location":"api/allenact/embodiedai/mapping/mapping_utils/point_cloud_utils/#project_point_cloud_to_map","text":"project_point_cloud_to_map ( xyz_points : torch . Tensor , bin_axis : str , bins : Sequence [ float ], map_size : int , resolution_in_cm : int , flip_row_col : bool ) [view_source] Bins an input point cloud into a map tensor with the bins equaling the channels. This code has been adapted from https://github.com/devendrachaplot/Neural-SLAM. Parameters xyz_points : (x,y,z) pointcloud(s) as a torch.Tensor of shape (... x height x width x 3). All operations are vectorized across the ... dimensions. bin_axis : Either \"x\", \"y\", or \"z\", the axis which should be binned by the values in bins . If you have generated your point clouds with any of the other functions in the point_cloud_utils module you almost certainly want this to be \"y\" as this is the default upwards dimension. bins : The values by which to bin along bin_axis , see the bins parameter of np.digitize for more info. map_size : The axes not specified by bin_axis will be be divided by resolution_in_cm / 100 and then rounded to the nearest integer. They are then expected to have their values within the interval [0, ..., map_size - 1]. resolution_in_cm : The resolution_in_cm, in cm, of the map output from this function. Every grid square of the map corresponds to a ( resolution_in_cm x resolution_in_cm ) square in space. flip_row_col : Should the rows/cols of the map be flipped? See the 'Returns' section below for more info. Returns A collection of maps of shape (... x map_size x map_size x (len(bins)+1)), note that bin_axis has been moved to the last index of this returned map, the other two axes stay in their original order unless flip_row_col has been called in which case they are reversed (useful as often rows should correspond to y or z instead of x).","title":"project_point_cloud_to_map"},{"location":"api/allenact/embodiedai/models/basic_models/","text":"allenact.embodiedai.models.basic_models # [view_source] Basic building block torch networks that can be used across a variety of tasks. SimpleCNN # class SimpleCNN ( nn . Module ) [view_source] A Simple N-Conv CNN followed by a fully connected layer. Takes in observations (of type gym.spaces.dict) and produces an embedding of the rgb_uuid and/or depth_uuid components. Attributes observation_space : The observation_space of the agent, should have rgb_uuid or depth_uuid as a component (otherwise it is a blind model). output_size : The size of the embedding vector to produce. SimpleCNN.__init__ # | __init__ ( observation_space : SpaceDict , output_size : int , rgb_uuid : Optional [ str ], depth_uuid : Optional [ str ], layer_channels : Sequence [ int ] = ( 32 , 64 , 32 ), kernel_sizes : Sequence [ Tuple [ int , int ]] = (( 8 , 8 ), ( 4 , 4 ), ( 3 , 3 )), layers_stride : Sequence [ Tuple [ int , int ]] = (( 4 , 4 ), ( 2 , 2 ), ( 1 , 1 )), paddings : Sequence [ Tuple [ int , int ]] = (( 0 , 0 ), ( 0 , 0 ), ( 0 , 0 )), dilations : Sequence [ Tuple [ int , int ]] = (( 1 , 1 ), ( 1 , 1 ), ( 1 , 1 )), flatten : bool = True , output_relu : bool = True ) [view_source] Initializer. Parameters observation_space : See class attributes documentation. output_size : See class attributes documentation. SimpleCNN.layer_init # | @staticmethod | layer_init ( cnn ) -> None [view_source] Initialize layer parameters using Kaiming normal. SimpleCNN.is_blind # | @property | is_blind () [view_source] True if the observation space doesn't include self.rgb_uuid or self.depth_uuid . RNNStateEncoder # class RNNStateEncoder ( nn . Module ) [view_source] A simple RNN-based model playing a role in many baseline embodied- navigation agents. See seq_forward for more details of how this model is used. RNNStateEncoder.__init__ # | __init__ ( input_size : int , hidden_size : int , num_layers : int = 1 , rnn_type : str = \"GRU\" , trainable_masked_hidden_state : bool = False ) [view_source] An RNN for encoding the state in RL. Supports masking the hidden state during various timesteps in the forward lass. Parameters input_size : The input size of the RNN. hidden_size : The hidden size. num_layers : The number of recurrent layers. rnn_type : The RNN cell type. Must be GRU or LSTM. trainable_masked_hidden_state : If True the initial hidden state (used at the start of a Task) is trainable (as opposed to being a vector of zeros). RNNStateEncoder.layer_init # | layer_init () [view_source] Initialize the RNN parameters in the model. RNNStateEncoder.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] The number of recurrent layers in the network. RNNStateEncoder.single_forward # | single_forward ( x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor ) -> Tuple [ | torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]] | ] [view_source] Forward for a single-step input. RNNStateEncoder.seq_forward # | seq_forward ( x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor ) -> Tuple [ | torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]] | ] [view_source] Forward for a sequence of length T. Parameters x : (Steps, Samplers, Agents, -1) tensor. hidden_states : The starting hidden states. masks : A (Steps, Samplers, Agents) tensor. The masks to be applied to hidden state at every timestep, equal to 0 whenever the previous step finalized the task, 1 elsewhere.","title":"basic_models"},{"location":"api/allenact/embodiedai/models/basic_models/#allenactembodiedaimodelsbasic_models","text":"[view_source] Basic building block torch networks that can be used across a variety of tasks.","title":"allenact.embodiedai.models.basic_models"},{"location":"api/allenact/embodiedai/models/basic_models/#simplecnn","text":"class SimpleCNN ( nn . Module ) [view_source] A Simple N-Conv CNN followed by a fully connected layer. Takes in observations (of type gym.spaces.dict) and produces an embedding of the rgb_uuid and/or depth_uuid components. Attributes observation_space : The observation_space of the agent, should have rgb_uuid or depth_uuid as a component (otherwise it is a blind model). output_size : The size of the embedding vector to produce.","title":"SimpleCNN"},{"location":"api/allenact/embodiedai/models/basic_models/#simplecnn__init__","text":"| __init__ ( observation_space : SpaceDict , output_size : int , rgb_uuid : Optional [ str ], depth_uuid : Optional [ str ], layer_channels : Sequence [ int ] = ( 32 , 64 , 32 ), kernel_sizes : Sequence [ Tuple [ int , int ]] = (( 8 , 8 ), ( 4 , 4 ), ( 3 , 3 )), layers_stride : Sequence [ Tuple [ int , int ]] = (( 4 , 4 ), ( 2 , 2 ), ( 1 , 1 )), paddings : Sequence [ Tuple [ int , int ]] = (( 0 , 0 ), ( 0 , 0 ), ( 0 , 0 )), dilations : Sequence [ Tuple [ int , int ]] = (( 1 , 1 ), ( 1 , 1 ), ( 1 , 1 )), flatten : bool = True , output_relu : bool = True ) [view_source] Initializer. Parameters observation_space : See class attributes documentation. output_size : See class attributes documentation.","title":"SimpleCNN.__init__"},{"location":"api/allenact/embodiedai/models/basic_models/#simplecnnlayer_init","text":"| @staticmethod | layer_init ( cnn ) -> None [view_source] Initialize layer parameters using Kaiming normal.","title":"SimpleCNN.layer_init"},{"location":"api/allenact/embodiedai/models/basic_models/#simplecnnis_blind","text":"| @property | is_blind () [view_source] True if the observation space doesn't include self.rgb_uuid or self.depth_uuid .","title":"SimpleCNN.is_blind"},{"location":"api/allenact/embodiedai/models/basic_models/#rnnstateencoder","text":"class RNNStateEncoder ( nn . Module ) [view_source] A simple RNN-based model playing a role in many baseline embodied- navigation agents. See seq_forward for more details of how this model is used.","title":"RNNStateEncoder"},{"location":"api/allenact/embodiedai/models/basic_models/#rnnstateencoder__init__","text":"| __init__ ( input_size : int , hidden_size : int , num_layers : int = 1 , rnn_type : str = \"GRU\" , trainable_masked_hidden_state : bool = False ) [view_source] An RNN for encoding the state in RL. Supports masking the hidden state during various timesteps in the forward lass. Parameters input_size : The input size of the RNN. hidden_size : The hidden size. num_layers : The number of recurrent layers. rnn_type : The RNN cell type. Must be GRU or LSTM. trainable_masked_hidden_state : If True the initial hidden state (used at the start of a Task) is trainable (as opposed to being a vector of zeros).","title":"RNNStateEncoder.__init__"},{"location":"api/allenact/embodiedai/models/basic_models/#rnnstateencoderlayer_init","text":"| layer_init () [view_source] Initialize the RNN parameters in the model.","title":"RNNStateEncoder.layer_init"},{"location":"api/allenact/embodiedai/models/basic_models/#rnnstateencodernum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] The number of recurrent layers in the network.","title":"RNNStateEncoder.num_recurrent_layers"},{"location":"api/allenact/embodiedai/models/basic_models/#rnnstateencodersingle_forward","text":"| single_forward ( x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor ) -> Tuple [ | torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]] | ] [view_source] Forward for a single-step input.","title":"RNNStateEncoder.single_forward"},{"location":"api/allenact/embodiedai/models/basic_models/#rnnstateencoderseq_forward","text":"| seq_forward ( x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor ) -> Tuple [ | torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]] | ] [view_source] Forward for a sequence of length T. Parameters x : (Steps, Samplers, Agents, -1) tensor. hidden_states : The starting hidden states. masks : A (Steps, Samplers, Agents) tensor. The masks to be applied to hidden state at every timestep, equal to 0 whenever the previous step finalized the task, 1 elsewhere.","title":"RNNStateEncoder.seq_forward"},{"location":"api/allenact/embodiedai/preprocessors/resnet/","text":"allenact.embodiedai.preprocessors.resnet # [view_source] ResNetPreprocessor # class ResNetPreprocessor ( Preprocessor ) [view_source] Preprocess RGB or depth image using a ResNet model.","title":"resnet"},{"location":"api/allenact/embodiedai/preprocessors/resnet/#allenactembodiedaipreprocessorsresnet","text":"[view_source]","title":"allenact.embodiedai.preprocessors.resnet"},{"location":"api/allenact/embodiedai/preprocessors/resnet/#resnetpreprocessor","text":"class ResNetPreprocessor ( Preprocessor ) [view_source] Preprocess RGB or depth image using a ResNet model.","title":"ResNetPreprocessor"},{"location":"api/allenact/embodiedai/sensors/vision_sensors/","text":"allenact.embodiedai.sensors.vision_sensors # [view_source] VisionSensor # class VisionSensor ( Sensor [ EnvType , SubTaskType ]) [view_source] VisionSensor.__init__ # | __init__ ( mean : Optional [ np . ndarray ] = None , stdev : Optional [ np . ndarray ] = None , height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"vision\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : Optional [ int ] = None , unnormalized_infimum : float = - np . inf , unnormalized_supremum : float = np . inf , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters mean : The images will be normalized with the given mean stdev : The images will be normalized with the given standard deviations. height : If it's a non-negative integer and width is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. width : If it's a non-negative integer and height is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. uuid : The universally unique identifier for the sensor. output_shape : Optional observation space shape (alternative to output_channels ). output_channels : Optional observation space number of channels (alternative to output_shape ). unnormalized_infimum : Lower limit(s) for the observation space range. unnormalized_supremum : Upper limit(s) for the observation space range. scale_first : Whether to scale image before normalization (if needed). kwargs : Extra kwargs. Currently unused. VisionSensor.height # | @property | height () -> Optional [ int ] [view_source] Height that input image will be rescale to have. Returns The height as a non-negative integer or None if no rescaling is done. VisionSensor.width # | @property | width () -> Optional [ int ] [view_source] Width that input image will be rescale to have. Returns The width as a non-negative integer or None if no rescaling is done. RGBSensor # class RGBSensor ( VisionSensor [ EnvType , SubTaskType ], ABC ) [view_source] RGBSensor.__init__ # | __init__ ( use_resnet_normalization : bool = False , mean : Optional [ Union [ np . ndarray , Sequence [ float ]]] = ( 0.485 , 0.456 , 0.406 ), stdev : Optional [ Union [ np . ndarray , Sequence [ float ]]] = ( 0.229 , 0.224 , 0.225 ), height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"rgb\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 3 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 1.0 , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters use_resnet_normalization : Whether to apply image normalization with the given mean and stdev . mean : The images will be normalized with the given mean if use_resnet_normalization is True (default [0.485, 0.456, 0.406] , i.e. the standard resnet normalization mean). stdev : The images will be normalized with the given standard deviation if use_resnet_normalization is True (default [0.229, 0.224, 0.225] , i.e. the standard resnet normalization standard deviation). height : If it's a non-negative integer and width is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. width : If it's a non-negative integer and height is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. uuid : The universally unique identifier for the sensor. output_shape : Optional observation space shape (alternative to output_channels ). output_channels : Optional observation space number of channels (alternative to output_shape ). unnormalized_infimum : Lower limit(s) for the observation space range. unnormalized_supremum : Upper limit(s) for the observation space range. scale_first : Whether to scale image before normalization (if needed). kwargs : Extra kwargs. Currently unused. DepthSensor # class DepthSensor ( VisionSensor [ EnvType , SubTaskType ], ABC ) [view_source] DepthSensor.__init__ # | __init__ ( use_normalization : bool = False , mean : Optional [ Union [ np . ndarray , float ]] = 0.5 , stdev : Optional [ Union [ np . ndarray , float ]] = 0.25 , height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"depth\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 1 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 5.0 , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : If config[\"use_normalization\"] is True then the depth images will be normalized with mean 0.5 and standard deviation 0.25. If both config[\"height\"] and config[\"width\"] are non-negative integers then the depth image returned from the environment will be rescaled to have shape (config[\"height\"], config[\"width\"]) using bilinear sampling. use_normalization : Whether to apply image normalization with the given mean and stdev . mean : The images will be normalized with the given mean if use_normalization is True (default 0.5). stdev : The images will be normalized with the given standard deviation if use_normalization is True (default 0.25). height : If it's a non-negative integer and width is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. width : If it's a non-negative integer and height is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. uuid : The universally unique identifier for the sensor. output_shape : Optional observation space shape (alternative to output_channels ). output_channels : Optional observation space number of channels (alternative to output_shape ). unnormalized_infimum : Lower limit(s) for the observation space range. unnormalized_supremum : Upper limit(s) for the observation space range. scale_first : Whether to scale image before normalization (if needed). kwargs : Extra kwargs. Currently unused.","title":"vision_sensors"},{"location":"api/allenact/embodiedai/sensors/vision_sensors/#allenactembodiedaisensorsvision_sensors","text":"[view_source]","title":"allenact.embodiedai.sensors.vision_sensors"},{"location":"api/allenact/embodiedai/sensors/vision_sensors/#visionsensor","text":"class VisionSensor ( Sensor [ EnvType , SubTaskType ]) [view_source]","title":"VisionSensor"},{"location":"api/allenact/embodiedai/sensors/vision_sensors/#visionsensor__init__","text":"| __init__ ( mean : Optional [ np . ndarray ] = None , stdev : Optional [ np . ndarray ] = None , height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"vision\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : Optional [ int ] = None , unnormalized_infimum : float = - np . inf , unnormalized_supremum : float = np . inf , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters mean : The images will be normalized with the given mean stdev : The images will be normalized with the given standard deviations. height : If it's a non-negative integer and width is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. width : If it's a non-negative integer and height is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. uuid : The universally unique identifier for the sensor. output_shape : Optional observation space shape (alternative to output_channels ). output_channels : Optional observation space number of channels (alternative to output_shape ). unnormalized_infimum : Lower limit(s) for the observation space range. unnormalized_supremum : Upper limit(s) for the observation space range. scale_first : Whether to scale image before normalization (if needed). kwargs : Extra kwargs. Currently unused.","title":"VisionSensor.__init__"},{"location":"api/allenact/embodiedai/sensors/vision_sensors/#visionsensorheight","text":"| @property | height () -> Optional [ int ] [view_source] Height that input image will be rescale to have. Returns The height as a non-negative integer or None if no rescaling is done.","title":"VisionSensor.height"},{"location":"api/allenact/embodiedai/sensors/vision_sensors/#visionsensorwidth","text":"| @property | width () -> Optional [ int ] [view_source] Width that input image will be rescale to have. Returns The width as a non-negative integer or None if no rescaling is done.","title":"VisionSensor.width"},{"location":"api/allenact/embodiedai/sensors/vision_sensors/#rgbsensor","text":"class RGBSensor ( VisionSensor [ EnvType , SubTaskType ], ABC ) [view_source]","title":"RGBSensor"},{"location":"api/allenact/embodiedai/sensors/vision_sensors/#rgbsensor__init__","text":"| __init__ ( use_resnet_normalization : bool = False , mean : Optional [ Union [ np . ndarray , Sequence [ float ]]] = ( 0.485 , 0.456 , 0.406 ), stdev : Optional [ Union [ np . ndarray , Sequence [ float ]]] = ( 0.229 , 0.224 , 0.225 ), height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"rgb\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 3 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 1.0 , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters use_resnet_normalization : Whether to apply image normalization with the given mean and stdev . mean : The images will be normalized with the given mean if use_resnet_normalization is True (default [0.485, 0.456, 0.406] , i.e. the standard resnet normalization mean). stdev : The images will be normalized with the given standard deviation if use_resnet_normalization is True (default [0.229, 0.224, 0.225] , i.e. the standard resnet normalization standard deviation). height : If it's a non-negative integer and width is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. width : If it's a non-negative integer and height is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. uuid : The universally unique identifier for the sensor. output_shape : Optional observation space shape (alternative to output_channels ). output_channels : Optional observation space number of channels (alternative to output_shape ). unnormalized_infimum : Lower limit(s) for the observation space range. unnormalized_supremum : Upper limit(s) for the observation space range. scale_first : Whether to scale image before normalization (if needed). kwargs : Extra kwargs. Currently unused.","title":"RGBSensor.__init__"},{"location":"api/allenact/embodiedai/sensors/vision_sensors/#depthsensor","text":"class DepthSensor ( VisionSensor [ EnvType , SubTaskType ], ABC ) [view_source]","title":"DepthSensor"},{"location":"api/allenact/embodiedai/sensors/vision_sensors/#depthsensor__init__","text":"| __init__ ( use_normalization : bool = False , mean : Optional [ Union [ np . ndarray , float ]] = 0.5 , stdev : Optional [ Union [ np . ndarray , float ]] = 0.25 , height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"depth\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 1 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 5.0 , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : If config[\"use_normalization\"] is True then the depth images will be normalized with mean 0.5 and standard deviation 0.25. If both config[\"height\"] and config[\"width\"] are non-negative integers then the depth image returned from the environment will be rescaled to have shape (config[\"height\"], config[\"width\"]) using bilinear sampling. use_normalization : Whether to apply image normalization with the given mean and stdev . mean : The images will be normalized with the given mean if use_normalization is True (default 0.5). stdev : The images will be normalized with the given standard deviation if use_normalization is True (default 0.25). height : If it's a non-negative integer and width is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. width : If it's a non-negative integer and height is also non-negative integer, the image returned from the environment will be rescaled to have height rows and width columns using bilinear sampling. uuid : The universally unique identifier for the sensor. output_shape : Optional observation space shape (alternative to output_channels ). output_channels : Optional observation space number of channels (alternative to output_shape ). unnormalized_infimum : Lower limit(s) for the observation space range. unnormalized_supremum : Upper limit(s) for the observation space range. scale_first : Whether to scale image before normalization (if needed). kwargs : Extra kwargs. Currently unused.","title":"DepthSensor.__init__"},{"location":"api/allenact/utils/cache_utils/","text":"allenact.utils.cache_utils # [view_source]","title":"cache_utils"},{"location":"api/allenact/utils/cache_utils/#allenactutilscache_utils","text":"[view_source]","title":"allenact.utils.cache_utils"},{"location":"api/allenact/utils/cacheless_frcnn/","text":"allenact.utils.cacheless_frcnn # [view_source]","title":"cacheless_frcnn"},{"location":"api/allenact/utils/cacheless_frcnn/#allenactutilscacheless_frcnn","text":"[view_source]","title":"allenact.utils.cacheless_frcnn"},{"location":"api/allenact/utils/experiment_utils/","text":"allenact.utils.experiment_utils # [view_source] Utility classes and functions for running and designing experiments. evenly_distribute_count_into_bins # evenly_distribute_count_into_bins ( count : int , nbins : int ) -> List [ int ] [view_source] Distribute a count into a number of bins. Parameters count : A positive integer to be distributed, should be >= nbins . nbins : The number of bins. Returns A list of positive integers which sum to count . These values will be as close to equal as possible (may differ by at most 1). recursive_update # recursive_update ( original : Union [ Dict , collections . abc . MutableMapping ], update : Union [ Dict , collections . abc . MutableMapping ]) [view_source] Recursively updates original dictionary with entries form update dict. Parameters original : Original dictionary to be updated. update : Dictionary with additional or replacement entries. Returns Updated original dictionary. Builder # class Builder ( tuple , Generic [ ToBuildType ]) [view_source] Used to instantiate a given class with (default) parameters. Helper class that stores a class, default parameters for that class, and key word arguments that (possibly) overwrite the defaults. When calling this an object of the Builder class it generates a class of type class_type with parameters specified by the attributes default and kwargs (and possibly additional, overwriting, keyword arguments). Attributes class_type : The class to be instantiated when calling the object. kwargs : Keyword arguments used to instantiate an object of type class_type . default : Default parameters used when instantiating the class. Builder.__new__ # | __new__ ( cls , class_type : ToBuildType , kwargs : Optional [ Dict [ str , Any ]] = None , default : Optional [ Dict [ str , Any ]] = None ) [view_source] Create a new Builder. For parameter descriptions see the class documentation. Note that kwargs and default can be None in which case they are set to be empty dictionaries. Builder.__call__ # | __call__ ( ** kwargs ) -> ToBuildType [view_source] Build and return a new class. Parameters kwargs : additional keyword arguments to use when instantiating the object. These overwrite all arguments already in the self.kwargs and self.default attributes. Returns Class of type self.class_type with parameters taken from self.default , self.kwargs , and any keyword arguments additionally passed to __call__ . ScalarMeanTracker # class ScalarMeanTracker ( object ) [view_source] Track a collection scalar key -> mean pairs. ScalarMeanTracker.add_scalars # | add_scalars ( scalars : Dict [ str , Union [ float , int ]], n : Union [ int , Dict [ str , int ]] = 1 ) -> None [view_source] Add additional scalars to track. Parameters scalars : A dictionary of scalar key -> value pairs. ScalarMeanTracker.pop_and_reset # | pop_and_reset () -> Dict [ str , float ] [view_source] Return tracked means and reset. On resetting all previously tracked values are discarded. Returns A dictionary of scalar key -> current mean pairs corresponding to those values added with add_scalars . LoggingPackage # class LoggingPackage ( object ) [view_source] Data package used for logging. LinearDecay # class LinearDecay ( object ) [view_source] Linearly decay between two values over some number of steps. Obtain the value corresponding to the i th step by calling an instantiation of this object with the value i . Parameters steps : The number of steps over which to decay. startp : The starting value. endp : The ending value. LinearDecay.__init__ # | __init__ ( steps : int , startp : float = 1.0 , endp : float = 0.0 ) -> None [view_source] Initializer. See class documentation for parameter definitions. LinearDecay.__call__ # | __call__ ( epoch : int ) -> float [view_source] Get the decayed value for epoch number of steps. Parameters epoch : The number of steps. Returns Decayed value for epoch number of steps. set_deterministic_cudnn # set_deterministic_cudnn () -> None [view_source] Makes cudnn deterministic. This may slow down computations. set_seed # set_seed ( seed : Optional [ int ] = None ) -> None [view_source] Set seeds for multiple (cpu) sources of randomness. Sets seeds for (cpu) pytorch , base random , and numpy . Parameters seed : The seed to set. If set to None, keep using the current seed. EarlyStoppingCriterion # class EarlyStoppingCriterion ( abc . ABC ) [view_source] Abstract class for class who determines if training should stop early in a particular pipeline stage. EarlyStoppingCriterion.__call__ # | @abc . abstractmethod | __call__ ( stage_steps : int , total_steps : int , training_metrics : ScalarMeanTracker ) -> bool [view_source] Returns True if training should be stopped early. Parameters stage_steps : Total number of steps taken in the current pipeline stage. total_steps : Total number of steps taken during training so far (includes steps taken in prior pipeline stages). training_metrics : Metrics recovered over some fixed number of steps (see the metric_accumulate_interval attribute in the TrainingPipeline class) training. NeverEarlyStoppingCriterion # class NeverEarlyStoppingCriterion ( EarlyStoppingCriterion ) [view_source] Implementation of EarlyStoppingCriterion which never stops early. OffPolicyPipelineComponent # class OffPolicyPipelineComponent ( NamedTuple ) [view_source] An off-policy component for a PipeLineStage. Attributes data_iterator_builder : A function to instantiate a Data Iterator (with a next (self) method) loss_names : list of unique names assigned to off-policy losses updates : number of off-policy updates between on-policy rollout collections loss_weights : A list of floating point numbers describing the relative weights applied to the losses referenced by loss_names . Should be the same length as loss_names . If this is None , all weights will be assumed to be one. data_iterator_kwargs_generator : Optional generator of keyword arguments for data_iterator_builder (useful for distributed training. It takes a cur_worker int value, a rollouts_per_worker list of number of samplers per training worker, and an optional random seed shared by all workers, which can be None. PipelineStage # class PipelineStage ( object ) [view_source] A single stage in a training pipeline. Attributes loss_name : A collection of unique names assigned to losses. These will reference the Loss objects in a TrainingPipeline instance. max_stage_steps : Either the total number of steps agents should take in this stage or a Callable object (e.g. a function) loss_weights : A list of floating point numbers describing the relative weights applied to the losses referenced by loss_name . Should be the same length as loss_name . If this is None , all weights will be assumed to be one. teacher_forcing : If applicable, defines the probability an agent will take the expert action (as opposed to its own sampled action) at a given time point. early_stopping_criterion : An EarlyStoppingCriterion object which determines if training in this stage should be stopped early. If None then no early stopping occurs. If early_stopping_criterion is not None then we do not guarantee reproducibility when restarting a model from a checkpoint (as the EarlyStoppingCriterion object may store internal state which is not saved in the checkpoint). Currently AllenAct only supports using early stopping criterion when not using distributed training. TrainingPipeline # class TrainingPipeline ( object ) [view_source] Class defining the stages (and global parameters) in a training pipeline. The training pipeline can be used as an iterator to go through the pipeline stages in, for instance, a loop. Attributes named_losses : Dictionary mapping a the name of a loss to either an instantiation of that loss or a Builder that, when called, will return that loss. pipeline_stages : A list of PipelineStages. Each of these define how the agent will be trained and are executed sequentially. optimizer_builder : Builder object to instantiate the optimizer to use during training. num_mini_batch : The number of mini-batches to break a rollout into. update_repeats : The number of times we will cycle through the mini-batches corresponding to a single rollout doing gradient updates. max_grad_norm : The maximum \"inf\" norm of any gradient step (gradients are clipped to not exceed this). num_steps : Total number of steps a single agent takes in a rollout. gamma : Discount factor applied to rewards (should be in [0, 1]). use_gae : Whether or not to use generalized advantage estimation (GAE). gae_lambda : The additional parameter used in GAE. save_interval : The frequency with which to save (in total agent steps taken). If None then no checkpoints will be saved. Otherwise, in addition to the checkpoints being saved every save_interval steps, a checkpoint will always be saved at the end of each pipeline stage. If save_interval <= 0 then checkpoints will only be saved at the end of each pipeline stage. metric_accumulate_interval : The frequency with which training/validation metrics are accumulated (in total agent steps). Metrics accumulated in an interval are logged (if should_log is True ) and used by the stage's early stopping criterion (if any). should_log : True if metrics accumulated during training should be logged to the console as well as to a tensorboard file. lr_scheduler_builder : Optional builder object to instantiate the learning rate scheduler used through the pipeline. TrainingPipeline.__init__ # | __init__ ( named_losses : Dict [ str , Union [ Loss , Builder [ Loss ]]], pipeline_stages : List [ PipelineStage ], optimizer_builder : Builder [ optim . Optimizer ], num_mini_batch : int , update_repeats : int , max_grad_norm : float , num_steps : int , gamma : float , use_gae : bool , gae_lambda : float , advance_scene_rollout_period : Optional [ int ], save_interval : Optional [ int ], metric_accumulate_interval : int , should_log : bool = True , lr_scheduler_builder : Optional [ Builder [ optim . lr_scheduler . _LRScheduler ]] = None ) [view_source] Initializer. See class docstring for parameter definitions.","title":"experiment_utils"},{"location":"api/allenact/utils/experiment_utils/#allenactutilsexperiment_utils","text":"[view_source] Utility classes and functions for running and designing experiments.","title":"allenact.utils.experiment_utils"},{"location":"api/allenact/utils/experiment_utils/#evenly_distribute_count_into_bins","text":"evenly_distribute_count_into_bins ( count : int , nbins : int ) -> List [ int ] [view_source] Distribute a count into a number of bins. Parameters count : A positive integer to be distributed, should be >= nbins . nbins : The number of bins. Returns A list of positive integers which sum to count . These values will be as close to equal as possible (may differ by at most 1).","title":"evenly_distribute_count_into_bins"},{"location":"api/allenact/utils/experiment_utils/#recursive_update","text":"recursive_update ( original : Union [ Dict , collections . abc . MutableMapping ], update : Union [ Dict , collections . abc . MutableMapping ]) [view_source] Recursively updates original dictionary with entries form update dict. Parameters original : Original dictionary to be updated. update : Dictionary with additional or replacement entries. Returns Updated original dictionary.","title":"recursive_update"},{"location":"api/allenact/utils/experiment_utils/#builder","text":"class Builder ( tuple , Generic [ ToBuildType ]) [view_source] Used to instantiate a given class with (default) parameters. Helper class that stores a class, default parameters for that class, and key word arguments that (possibly) overwrite the defaults. When calling this an object of the Builder class it generates a class of type class_type with parameters specified by the attributes default and kwargs (and possibly additional, overwriting, keyword arguments). Attributes class_type : The class to be instantiated when calling the object. kwargs : Keyword arguments used to instantiate an object of type class_type . default : Default parameters used when instantiating the class.","title":"Builder"},{"location":"api/allenact/utils/experiment_utils/#builder__new__","text":"| __new__ ( cls , class_type : ToBuildType , kwargs : Optional [ Dict [ str , Any ]] = None , default : Optional [ Dict [ str , Any ]] = None ) [view_source] Create a new Builder. For parameter descriptions see the class documentation. Note that kwargs and default can be None in which case they are set to be empty dictionaries.","title":"Builder.__new__"},{"location":"api/allenact/utils/experiment_utils/#builder__call__","text":"| __call__ ( ** kwargs ) -> ToBuildType [view_source] Build and return a new class. Parameters kwargs : additional keyword arguments to use when instantiating the object. These overwrite all arguments already in the self.kwargs and self.default attributes. Returns Class of type self.class_type with parameters taken from self.default , self.kwargs , and any keyword arguments additionally passed to __call__ .","title":"Builder.__call__"},{"location":"api/allenact/utils/experiment_utils/#scalarmeantracker","text":"class ScalarMeanTracker ( object ) [view_source] Track a collection scalar key -> mean pairs.","title":"ScalarMeanTracker"},{"location":"api/allenact/utils/experiment_utils/#scalarmeantrackeradd_scalars","text":"| add_scalars ( scalars : Dict [ str , Union [ float , int ]], n : Union [ int , Dict [ str , int ]] = 1 ) -> None [view_source] Add additional scalars to track. Parameters scalars : A dictionary of scalar key -> value pairs.","title":"ScalarMeanTracker.add_scalars"},{"location":"api/allenact/utils/experiment_utils/#scalarmeantrackerpop_and_reset","text":"| pop_and_reset () -> Dict [ str , float ] [view_source] Return tracked means and reset. On resetting all previously tracked values are discarded. Returns A dictionary of scalar key -> current mean pairs corresponding to those values added with add_scalars .","title":"ScalarMeanTracker.pop_and_reset"},{"location":"api/allenact/utils/experiment_utils/#loggingpackage","text":"class LoggingPackage ( object ) [view_source] Data package used for logging.","title":"LoggingPackage"},{"location":"api/allenact/utils/experiment_utils/#lineardecay","text":"class LinearDecay ( object ) [view_source] Linearly decay between two values over some number of steps. Obtain the value corresponding to the i th step by calling an instantiation of this object with the value i . Parameters steps : The number of steps over which to decay. startp : The starting value. endp : The ending value.","title":"LinearDecay"},{"location":"api/allenact/utils/experiment_utils/#lineardecay__init__","text":"| __init__ ( steps : int , startp : float = 1.0 , endp : float = 0.0 ) -> None [view_source] Initializer. See class documentation for parameter definitions.","title":"LinearDecay.__init__"},{"location":"api/allenact/utils/experiment_utils/#lineardecay__call__","text":"| __call__ ( epoch : int ) -> float [view_source] Get the decayed value for epoch number of steps. Parameters epoch : The number of steps. Returns Decayed value for epoch number of steps.","title":"LinearDecay.__call__"},{"location":"api/allenact/utils/experiment_utils/#set_deterministic_cudnn","text":"set_deterministic_cudnn () -> None [view_source] Makes cudnn deterministic. This may slow down computations.","title":"set_deterministic_cudnn"},{"location":"api/allenact/utils/experiment_utils/#set_seed","text":"set_seed ( seed : Optional [ int ] = None ) -> None [view_source] Set seeds for multiple (cpu) sources of randomness. Sets seeds for (cpu) pytorch , base random , and numpy . Parameters seed : The seed to set. If set to None, keep using the current seed.","title":"set_seed"},{"location":"api/allenact/utils/experiment_utils/#earlystoppingcriterion","text":"class EarlyStoppingCriterion ( abc . ABC ) [view_source] Abstract class for class who determines if training should stop early in a particular pipeline stage.","title":"EarlyStoppingCriterion"},{"location":"api/allenact/utils/experiment_utils/#earlystoppingcriterion__call__","text":"| @abc . abstractmethod | __call__ ( stage_steps : int , total_steps : int , training_metrics : ScalarMeanTracker ) -> bool [view_source] Returns True if training should be stopped early. Parameters stage_steps : Total number of steps taken in the current pipeline stage. total_steps : Total number of steps taken during training so far (includes steps taken in prior pipeline stages). training_metrics : Metrics recovered over some fixed number of steps (see the metric_accumulate_interval attribute in the TrainingPipeline class) training.","title":"EarlyStoppingCriterion.__call__"},{"location":"api/allenact/utils/experiment_utils/#neverearlystoppingcriterion","text":"class NeverEarlyStoppingCriterion ( EarlyStoppingCriterion ) [view_source] Implementation of EarlyStoppingCriterion which never stops early.","title":"NeverEarlyStoppingCriterion"},{"location":"api/allenact/utils/experiment_utils/#offpolicypipelinecomponent","text":"class OffPolicyPipelineComponent ( NamedTuple ) [view_source] An off-policy component for a PipeLineStage. Attributes data_iterator_builder : A function to instantiate a Data Iterator (with a next (self) method) loss_names : list of unique names assigned to off-policy losses updates : number of off-policy updates between on-policy rollout collections loss_weights : A list of floating point numbers describing the relative weights applied to the losses referenced by loss_names . Should be the same length as loss_names . If this is None , all weights will be assumed to be one. data_iterator_kwargs_generator : Optional generator of keyword arguments for data_iterator_builder (useful for distributed training. It takes a cur_worker int value, a rollouts_per_worker list of number of samplers per training worker, and an optional random seed shared by all workers, which can be None.","title":"OffPolicyPipelineComponent"},{"location":"api/allenact/utils/experiment_utils/#pipelinestage","text":"class PipelineStage ( object ) [view_source] A single stage in a training pipeline. Attributes loss_name : A collection of unique names assigned to losses. These will reference the Loss objects in a TrainingPipeline instance. max_stage_steps : Either the total number of steps agents should take in this stage or a Callable object (e.g. a function) loss_weights : A list of floating point numbers describing the relative weights applied to the losses referenced by loss_name . Should be the same length as loss_name . If this is None , all weights will be assumed to be one. teacher_forcing : If applicable, defines the probability an agent will take the expert action (as opposed to its own sampled action) at a given time point. early_stopping_criterion : An EarlyStoppingCriterion object which determines if training in this stage should be stopped early. If None then no early stopping occurs. If early_stopping_criterion is not None then we do not guarantee reproducibility when restarting a model from a checkpoint (as the EarlyStoppingCriterion object may store internal state which is not saved in the checkpoint). Currently AllenAct only supports using early stopping criterion when not using distributed training.","title":"PipelineStage"},{"location":"api/allenact/utils/experiment_utils/#trainingpipeline","text":"class TrainingPipeline ( object ) [view_source] Class defining the stages (and global parameters) in a training pipeline. The training pipeline can be used as an iterator to go through the pipeline stages in, for instance, a loop. Attributes named_losses : Dictionary mapping a the name of a loss to either an instantiation of that loss or a Builder that, when called, will return that loss. pipeline_stages : A list of PipelineStages. Each of these define how the agent will be trained and are executed sequentially. optimizer_builder : Builder object to instantiate the optimizer to use during training. num_mini_batch : The number of mini-batches to break a rollout into. update_repeats : The number of times we will cycle through the mini-batches corresponding to a single rollout doing gradient updates. max_grad_norm : The maximum \"inf\" norm of any gradient step (gradients are clipped to not exceed this). num_steps : Total number of steps a single agent takes in a rollout. gamma : Discount factor applied to rewards (should be in [0, 1]). use_gae : Whether or not to use generalized advantage estimation (GAE). gae_lambda : The additional parameter used in GAE. save_interval : The frequency with which to save (in total agent steps taken). If None then no checkpoints will be saved. Otherwise, in addition to the checkpoints being saved every save_interval steps, a checkpoint will always be saved at the end of each pipeline stage. If save_interval <= 0 then checkpoints will only be saved at the end of each pipeline stage. metric_accumulate_interval : The frequency with which training/validation metrics are accumulated (in total agent steps). Metrics accumulated in an interval are logged (if should_log is True ) and used by the stage's early stopping criterion (if any). should_log : True if metrics accumulated during training should be logged to the console as well as to a tensorboard file. lr_scheduler_builder : Optional builder object to instantiate the learning rate scheduler used through the pipeline.","title":"TrainingPipeline"},{"location":"api/allenact/utils/experiment_utils/#trainingpipeline__init__","text":"| __init__ ( named_losses : Dict [ str , Union [ Loss , Builder [ Loss ]]], pipeline_stages : List [ PipelineStage ], optimizer_builder : Builder [ optim . Optimizer ], num_mini_batch : int , update_repeats : int , max_grad_norm : float , num_steps : int , gamma : float , use_gae : bool , gae_lambda : float , advance_scene_rollout_period : Optional [ int ], save_interval : Optional [ int ], metric_accumulate_interval : int , should_log : bool = True , lr_scheduler_builder : Optional [ Builder [ optim . lr_scheduler . _LRScheduler ]] = None ) [view_source] Initializer. See class docstring for parameter definitions.","title":"TrainingPipeline.__init__"},{"location":"api/allenact/utils/misc_utils/","text":"allenact.utils.misc_utils # [view_source] experimental_api # experimental_api ( to_decorate ) [view_source] Decorate a function to note that it is part of the experimental API. deprecated # deprecated ( to_decorate ) [view_source] Decorate a function to note that it has been deprecated. NumpyJSONEncoder # class NumpyJSONEncoder ( json . JSONEncoder ) [view_source] JSON encoder for numpy objects. Based off the stackoverflow answer by Jie Yang here: https://stackoverflow.com/a/57915246. The license for this code is BY-SA 4.0 . HashableDict # class HashableDict ( dict ) [view_source] A dictionary which is hashable so long as all of its values are hashable. A HashableDict object will allow setting / deleting of items until the first time that __hash__() is called on it after which attempts to set or delete items will throw RuntimeError exceptions.","title":"misc_utils"},{"location":"api/allenact/utils/misc_utils/#allenactutilsmisc_utils","text":"[view_source]","title":"allenact.utils.misc_utils"},{"location":"api/allenact/utils/misc_utils/#experimental_api","text":"experimental_api ( to_decorate ) [view_source] Decorate a function to note that it is part of the experimental API.","title":"experimental_api"},{"location":"api/allenact/utils/misc_utils/#deprecated","text":"deprecated ( to_decorate ) [view_source] Decorate a function to note that it has been deprecated.","title":"deprecated"},{"location":"api/allenact/utils/misc_utils/#numpyjsonencoder","text":"class NumpyJSONEncoder ( json . JSONEncoder ) [view_source] JSON encoder for numpy objects. Based off the stackoverflow answer by Jie Yang here: https://stackoverflow.com/a/57915246. The license for this code is BY-SA 4.0 .","title":"NumpyJSONEncoder"},{"location":"api/allenact/utils/misc_utils/#hashabledict","text":"class HashableDict ( dict ) [view_source] A dictionary which is hashable so long as all of its values are hashable. A HashableDict object will allow setting / deleting of items until the first time that __hash__() is called on it after which attempts to set or delete items will throw RuntimeError exceptions.","title":"HashableDict"},{"location":"api/allenact/utils/model_utils/","text":"allenact.utils.model_utils # [view_source] Functions used to initialize and manipulate pytorch models. Flatten # class Flatten ( nn . Module ) [view_source] Flatten input tensor so that it is of shape (FLATTENED_BATCH x -1). Flatten.forward # | forward ( x ) [view_source] Flatten input tensor. Parameters x : Tensor of size (FLATTENED_BATCH x ...) to flatten to size (FLATTENED_BATCH x -1) Returns Flattened tensor. init_linear_layer # init_linear_layer ( module : nn . Linear , weight_init : Callable , bias_init : Callable , gain = 1 ) [view_source] Initialize a torch.nn.Linear layer. Parameters module : A torch linear layer. weight_init : Function used to initialize the weight parameters of the linear layer. Should take the weight data tensor and gain as input. bias_init : Function used to initialize the bias parameters of the linear layer. Should take the bias data tensor and gain as input. gain : The gain to apply. Returns The initialized linear layer. compute_cnn_output # compute_cnn_output ( cnn : nn . Module , cnn_input : torch . Tensor , permute_order : Optional [ Tuple [ int , ... ]] = ( 0 , # FLAT_BATCH (flattening steps, samplers and agents) 3 , # CHANNEL 1 , # ROW 2 , # COL )) [view_source] Computes CNN outputs for given inputs. Parameters cnn : A torch CNN. cnn_input : A torch Tensor with inputs. permute_order : A permutation Tuple to provide PyTorch dimension order, default (0, 3, 1, 2), where 0 corresponds to the flattened batch dimensions (combining step, sampler and agent) Returns CNN output with dimensions [STEP, SAMPLER, AGENT, CHANNEL, (HEIGHT, WIDTH)].","title":"model_utils"},{"location":"api/allenact/utils/model_utils/#allenactutilsmodel_utils","text":"[view_source] Functions used to initialize and manipulate pytorch models.","title":"allenact.utils.model_utils"},{"location":"api/allenact/utils/model_utils/#flatten","text":"class Flatten ( nn . Module ) [view_source] Flatten input tensor so that it is of shape (FLATTENED_BATCH x -1).","title":"Flatten"},{"location":"api/allenact/utils/model_utils/#flattenforward","text":"| forward ( x ) [view_source] Flatten input tensor. Parameters x : Tensor of size (FLATTENED_BATCH x ...) to flatten to size (FLATTENED_BATCH x -1) Returns Flattened tensor.","title":"Flatten.forward"},{"location":"api/allenact/utils/model_utils/#init_linear_layer","text":"init_linear_layer ( module : nn . Linear , weight_init : Callable , bias_init : Callable , gain = 1 ) [view_source] Initialize a torch.nn.Linear layer. Parameters module : A torch linear layer. weight_init : Function used to initialize the weight parameters of the linear layer. Should take the weight data tensor and gain as input. bias_init : Function used to initialize the bias parameters of the linear layer. Should take the bias data tensor and gain as input. gain : The gain to apply. Returns The initialized linear layer.","title":"init_linear_layer"},{"location":"api/allenact/utils/model_utils/#compute_cnn_output","text":"compute_cnn_output ( cnn : nn . Module , cnn_input : torch . Tensor , permute_order : Optional [ Tuple [ int , ... ]] = ( 0 , # FLAT_BATCH (flattening steps, samplers and agents) 3 , # CHANNEL 1 , # ROW 2 , # COL )) [view_source] Computes CNN outputs for given inputs. Parameters cnn : A torch CNN. cnn_input : A torch Tensor with inputs. permute_order : A permutation Tuple to provide PyTorch dimension order, default (0, 3, 1, 2), where 0 corresponds to the flattened batch dimensions (combining step, sampler and agent) Returns CNN output with dimensions [STEP, SAMPLER, AGENT, CHANNEL, (HEIGHT, WIDTH)].","title":"compute_cnn_output"},{"location":"api/allenact/utils/multi_agent_viz_utils/","text":"allenact.utils.multi_agent_viz_utils # [view_source]","title":"multi_agent_viz_utils"},{"location":"api/allenact/utils/multi_agent_viz_utils/#allenactutilsmulti_agent_viz_utils","text":"[view_source]","title":"allenact.utils.multi_agent_viz_utils"},{"location":"api/allenact/utils/spaces_utils/","text":"allenact.utils.spaces_utils # [view_source] flatdim # flatdim ( space ) [view_source] Return the number of dimensions a flattened equivalent of this space would have. Accepts a space and returns an integer. Raises NotImplementedError if the space is not defined in gym.spaces . flatten # flatten ( space , torch_x ) [view_source] Flatten data points from a space. unflatten # unflatten ( space , torch_x ) [view_source] Unflatten a concatenated data points tensor from a space. torch_point # torch_point ( space , np_x ) [view_source] Convert numpy space point into torch. numpy_point # numpy_point ( space : gym . Space , torch_x : Union [ int , torch . Tensor , OrderedDict , Tuple ]) [view_source] Convert torch space point into numpy. action_list # action_list ( action_space : gym . Space , flat_actions : torch . Tensor ) -> List [ ActionType ] [view_source] Convert flattened actions to list. Assumes flat_actions are of shape [step, sampler, flatdim] .","title":"spaces_utils"},{"location":"api/allenact/utils/spaces_utils/#allenactutilsspaces_utils","text":"[view_source]","title":"allenact.utils.spaces_utils"},{"location":"api/allenact/utils/spaces_utils/#flatdim","text":"flatdim ( space ) [view_source] Return the number of dimensions a flattened equivalent of this space would have. Accepts a space and returns an integer. Raises NotImplementedError if the space is not defined in gym.spaces .","title":"flatdim"},{"location":"api/allenact/utils/spaces_utils/#flatten","text":"flatten ( space , torch_x ) [view_source] Flatten data points from a space.","title":"flatten"},{"location":"api/allenact/utils/spaces_utils/#unflatten","text":"unflatten ( space , torch_x ) [view_source] Unflatten a concatenated data points tensor from a space.","title":"unflatten"},{"location":"api/allenact/utils/spaces_utils/#torch_point","text":"torch_point ( space , np_x ) [view_source] Convert numpy space point into torch.","title":"torch_point"},{"location":"api/allenact/utils/spaces_utils/#numpy_point","text":"numpy_point ( space : gym . Space , torch_x : Union [ int , torch . Tensor , OrderedDict , Tuple ]) [view_source] Convert torch space point into numpy.","title":"numpy_point"},{"location":"api/allenact/utils/spaces_utils/#action_list","text":"action_list ( action_space : gym . Space , flat_actions : torch . Tensor ) -> List [ ActionType ] [view_source] Convert flattened actions to list. Assumes flat_actions are of shape [step, sampler, flatdim] .","title":"action_list"},{"location":"api/allenact/utils/system/","text":"allenact.utils.system # [view_source] HUMAN_LOG_LEVELS # [view_source] Available log levels: \"debug\", \"info\", \"warning\", \"error\", \"none\" ColoredFormatter # class ColoredFormatter ( logging . Formatter ) [view_source] Format a log string with colors. This implementation taken (with modifications) from https://stackoverflow.com/a/384125. get_logger # get_logger () -> logging . Logger [view_source] Get a logging.Logger to stderr. It can be called whenever we wish to log some message. Messages can get mixed-up (https://docs.python.org/3.6/library/multiprocessing.html#logging), but it works well in most cases. Returns logger : the logging.Logger object init_logging # init_logging ( human_log_level : str = \"info\" ) -> None [view_source] Init the logging.Logger . It should be called only once in the app (e.g. in main ). It sets the log_level to one of HUMAN_LOG_LEVELS . And sets up a handler for stderr. The logging level is propagated to all subprocesses. find_free_port # find_free_port ( address : str = \"127.0.0.1\" ) -> int [view_source] Finds a free port for distributed training. Returns port : port number that can be used to listen","title":"system"},{"location":"api/allenact/utils/system/#allenactutilssystem","text":"[view_source]","title":"allenact.utils.system"},{"location":"api/allenact/utils/system/#human_log_levels","text":"[view_source] Available log levels: \"debug\", \"info\", \"warning\", \"error\", \"none\"","title":"HUMAN_LOG_LEVELS"},{"location":"api/allenact/utils/system/#coloredformatter","text":"class ColoredFormatter ( logging . Formatter ) [view_source] Format a log string with colors. This implementation taken (with modifications) from https://stackoverflow.com/a/384125.","title":"ColoredFormatter"},{"location":"api/allenact/utils/system/#get_logger","text":"get_logger () -> logging . Logger [view_source] Get a logging.Logger to stderr. It can be called whenever we wish to log some message. Messages can get mixed-up (https://docs.python.org/3.6/library/multiprocessing.html#logging), but it works well in most cases. Returns logger : the logging.Logger object","title":"get_logger"},{"location":"api/allenact/utils/system/#init_logging","text":"init_logging ( human_log_level : str = \"info\" ) -> None [view_source] Init the logging.Logger . It should be called only once in the app (e.g. in main ). It sets the log_level to one of HUMAN_LOG_LEVELS . And sets up a handler for stderr. The logging level is propagated to all subprocesses.","title":"init_logging"},{"location":"api/allenact/utils/system/#find_free_port","text":"find_free_port ( address : str = \"127.0.0.1\" ) -> int [view_source] Finds a free port for distributed training. Returns port : port number that can be used to listen","title":"find_free_port"},{"location":"api/allenact/utils/tensor_utils/","text":"allenact.utils.tensor_utils # [view_source] Functions used to manipulate pytorch tensors and numpy arrays. to_device_recursively # to_device_recursively ( input : Any , device : Union [ str , torch . device , int ], inplace : bool = True ) [view_source] Recursively places tensors on the appropriate device. detach_recursively # detach_recursively ( input : Any , inplace = True ) [view_source] Recursively detaches tensors in some data structure from their computation graph. batch_observations # batch_observations ( observations : List [ Dict ], device : Optional [ torch . device ] = None ) -> Dict [ str , Union [ Dict , torch . Tensor ]] [view_source] Transpose a batch of observation dicts to a dict of batched observations. Arguments observations : List of dicts of observations. device : The torch.device to put the resulting tensors on. Will not move the tensors if None. Returns Transposed dict of lists of observations. to_tensor # to_tensor ( v ) -> torch . Tensor [view_source] Return a torch.Tensor version of the input. Parameters v : Input values that can be coerced into being a tensor. Returns A tensor version of the input. tile_images # tile_images ( images : List [ np . ndarray ]) -> np . ndarray [view_source] Tile multiple images into single image. Parameters images : list of images where each image has dimension (height x width x channels) Returns Tiled image (new_height x width x channels). image # image ( tag , tensor , rescale = 1 , dataformats = \"CHW\" ) [view_source] Outputs a Summary protocol buffer with images. The summary has up to max_images summary values containing images. The images are built from tensor which must be 3-D with shape [height, width, channels] and where channels can be: 1: tensor is interpreted as Grayscale. 3: tensor is interpreted as RGB. 4: tensor is interpreted as RGBA. Parameters tag : A name for the generated node. Will also serve as a series name in TensorBoard. tensor : A 3-D uint8 or float32 Tensor of shape [height, width, channels] where channels is 1, 3, or 4. 'tensor' can either have values in [0, 1] (float32) or [0, 255] (uint8). The image() function will scale the image values to [0, 255] by applying a scale factor of either 1 (uint8) or 255 (float32). rescale : The scale. dataformats : Input image shape format. Returns A scalar Tensor of type string . The serialized Summary protocol buffer. ScaleBothSides # class ScaleBothSides ( object ) [view_source] Rescales the input PIL.Image to the given 'width' and height . Attributes width: new width height: new height interpolation: Default: PIL.Image.BILINEAR","title":"tensor_utils"},{"location":"api/allenact/utils/tensor_utils/#allenactutilstensor_utils","text":"[view_source] Functions used to manipulate pytorch tensors and numpy arrays.","title":"allenact.utils.tensor_utils"},{"location":"api/allenact/utils/tensor_utils/#to_device_recursively","text":"to_device_recursively ( input : Any , device : Union [ str , torch . device , int ], inplace : bool = True ) [view_source] Recursively places tensors on the appropriate device.","title":"to_device_recursively"},{"location":"api/allenact/utils/tensor_utils/#detach_recursively","text":"detach_recursively ( input : Any , inplace = True ) [view_source] Recursively detaches tensors in some data structure from their computation graph.","title":"detach_recursively"},{"location":"api/allenact/utils/tensor_utils/#batch_observations","text":"batch_observations ( observations : List [ Dict ], device : Optional [ torch . device ] = None ) -> Dict [ str , Union [ Dict , torch . Tensor ]] [view_source] Transpose a batch of observation dicts to a dict of batched observations. Arguments observations : List of dicts of observations. device : The torch.device to put the resulting tensors on. Will not move the tensors if None. Returns Transposed dict of lists of observations.","title":"batch_observations"},{"location":"api/allenact/utils/tensor_utils/#to_tensor","text":"to_tensor ( v ) -> torch . Tensor [view_source] Return a torch.Tensor version of the input. Parameters v : Input values that can be coerced into being a tensor. Returns A tensor version of the input.","title":"to_tensor"},{"location":"api/allenact/utils/tensor_utils/#tile_images","text":"tile_images ( images : List [ np . ndarray ]) -> np . ndarray [view_source] Tile multiple images into single image. Parameters images : list of images where each image has dimension (height x width x channels) Returns Tiled image (new_height x width x channels).","title":"tile_images"},{"location":"api/allenact/utils/tensor_utils/#image","text":"image ( tag , tensor , rescale = 1 , dataformats = \"CHW\" ) [view_source] Outputs a Summary protocol buffer with images. The summary has up to max_images summary values containing images. The images are built from tensor which must be 3-D with shape [height, width, channels] and where channels can be: 1: tensor is interpreted as Grayscale. 3: tensor is interpreted as RGB. 4: tensor is interpreted as RGBA. Parameters tag : A name for the generated node. Will also serve as a series name in TensorBoard. tensor : A 3-D uint8 or float32 Tensor of shape [height, width, channels] where channels is 1, 3, or 4. 'tensor' can either have values in [0, 1] (float32) or [0, 255] (uint8). The image() function will scale the image values to [0, 255] by applying a scale factor of either 1 (uint8) or 255 (float32). rescale : The scale. dataformats : Input image shape format. Returns A scalar Tensor of type string . The serialized Summary protocol buffer.","title":"image"},{"location":"api/allenact/utils/tensor_utils/#scalebothsides","text":"class ScaleBothSides ( object ) [view_source] Rescales the input PIL.Image to the given 'width' and height . Attributes width: new width height: new height interpolation: Default: PIL.Image.BILINEAR","title":"ScaleBothSides"},{"location":"api/allenact/utils/viz_utils/","text":"allenact.utils.viz_utils # [view_source]","title":"viz_utils"},{"location":"api/allenact/utils/viz_utils/#allenactutilsviz_utils","text":"[view_source]","title":"allenact.utils.viz_utils"},{"location":"api/allenact_plugins/babyai_plugin/babyai_constants/","text":"allenact_plugins.babyai_plugin.babyai_constants # [view_source]","title":"babyai_constants"},{"location":"api/allenact_plugins/babyai_plugin/babyai_constants/#allenact_pluginsbabyai_pluginbabyai_constants","text":"[view_source]","title":"allenact_plugins.babyai_plugin.babyai_constants"},{"location":"api/allenact_plugins/babyai_plugin/babyai_models/","text":"allenact_plugins.babyai_plugin.babyai_models # [view_source] BabyAIACModelWrapped # class BabyAIACModelWrapped ( babyai . model . ACModel ) [view_source] BabyAIACModelWrapped.forward_once # | forward_once ( obs , memory , instr_embedding = None ) [view_source] Copied (with minor modifications) from babyai.model.ACModel.forward(...) .","title":"babyai_models"},{"location":"api/allenact_plugins/babyai_plugin/babyai_models/#allenact_pluginsbabyai_pluginbabyai_models","text":"[view_source]","title":"allenact_plugins.babyai_plugin.babyai_models"},{"location":"api/allenact_plugins/babyai_plugin/babyai_models/#babyaiacmodelwrapped","text":"class BabyAIACModelWrapped ( babyai . model . ACModel ) [view_source]","title":"BabyAIACModelWrapped"},{"location":"api/allenact_plugins/babyai_plugin/babyai_models/#babyaiacmodelwrappedforward_once","text":"| forward_once ( obs , memory , instr_embedding = None ) [view_source] Copied (with minor modifications) from babyai.model.ACModel.forward(...) .","title":"BabyAIACModelWrapped.forward_once"},{"location":"api/allenact_plugins/babyai_plugin/babyai_tasks/","text":"allenact_plugins.babyai_plugin.babyai_tasks # [view_source]","title":"babyai_tasks"},{"location":"api/allenact_plugins/babyai_plugin/babyai_tasks/#allenact_pluginsbabyai_pluginbabyai_tasks","text":"[view_source]","title":"allenact_plugins.babyai_plugin.babyai_tasks"},{"location":"api/allenact_plugins/babyai_plugin/scripts/download_babyai_expert_demos/","text":"allenact_plugins.babyai_plugin.scripts.download_babyai_expert_demos # [view_source] get_args # get_args () [view_source] Creates the argument parser and parses input arguments.","title":"download_babyai_expert_demos"},{"location":"api/allenact_plugins/babyai_plugin/scripts/download_babyai_expert_demos/#allenact_pluginsbabyai_pluginscriptsdownload_babyai_expert_demos","text":"[view_source]","title":"allenact_plugins.babyai_plugin.scripts.download_babyai_expert_demos"},{"location":"api/allenact_plugins/babyai_plugin/scripts/download_babyai_expert_demos/#get_args","text":"get_args () [view_source] Creates the argument parser and parses input arguments.","title":"get_args"},{"location":"api/allenact_plugins/babyai_plugin/scripts/get_instr_length_percentiles/","text":"allenact_plugins.babyai_plugin.scripts.get_instr_length_percentiles # [view_source]","title":"get_instr_length_percentiles"},{"location":"api/allenact_plugins/babyai_plugin/scripts/get_instr_length_percentiles/#allenact_pluginsbabyai_pluginscriptsget_instr_length_percentiles","text":"[view_source]","title":"allenact_plugins.babyai_plugin.scripts.get_instr_length_percentiles"},{"location":"api/allenact_plugins/babyai_plugin/scripts/truncate_expert_demos/","text":"allenact_plugins.babyai_plugin.scripts.truncate_expert_demos # [view_source]","title":"truncate_expert_demos"},{"location":"api/allenact_plugins/babyai_plugin/scripts/truncate_expert_demos/#allenact_pluginsbabyai_pluginscriptstruncate_expert_demos","text":"[view_source]","title":"allenact_plugins.babyai_plugin.scripts.truncate_expert_demos"},{"location":"api/allenact_plugins/gym_plugin/gym_distributions/","text":"allenact_plugins.gym_plugin.gym_distributions # [view_source] GaussianDistr # class GaussianDistr ( torch . distributions . Normal , Distr ) [view_source] PyTorch's Normal distribution with a mode method.","title":"gym_distributions"},{"location":"api/allenact_plugins/gym_plugin/gym_distributions/#allenact_pluginsgym_plugingym_distributions","text":"[view_source]","title":"allenact_plugins.gym_plugin.gym_distributions"},{"location":"api/allenact_plugins/gym_plugin/gym_distributions/#gaussiandistr","text":"class GaussianDistr ( torch . distributions . Normal , Distr ) [view_source] PyTorch's Normal distribution with a mode method.","title":"GaussianDistr"},{"location":"api/allenact_plugins/gym_plugin/gym_environment/","text":"allenact_plugins.gym_plugin.gym_environment # [view_source] GymEnvironment # class GymEnvironment ( gym . Wrapper ) [view_source] gym.Wrapper with minimal bookkeeping (initial observation).","title":"gym_environment"},{"location":"api/allenact_plugins/gym_plugin/gym_environment/#allenact_pluginsgym_plugingym_environment","text":"[view_source]","title":"allenact_plugins.gym_plugin.gym_environment"},{"location":"api/allenact_plugins/gym_plugin/gym_environment/#gymenvironment","text":"class GymEnvironment ( gym . Wrapper ) [view_source] gym.Wrapper with minimal bookkeeping (initial observation).","title":"GymEnvironment"},{"location":"api/allenact_plugins/gym_plugin/gym_models/","text":"allenact_plugins.gym_plugin.gym_models # [view_source] MemorylessActorCritic # class MemorylessActorCritic ( ActorCriticModel [ GaussianDistr ]) [view_source] ActorCriticModel for gym tasks with continuous control in the range [-1, 1].","title":"gym_models"},{"location":"api/allenact_plugins/gym_plugin/gym_models/#allenact_pluginsgym_plugingym_models","text":"[view_source]","title":"allenact_plugins.gym_plugin.gym_models"},{"location":"api/allenact_plugins/gym_plugin/gym_models/#memorylessactorcritic","text":"class MemorylessActorCritic ( ActorCriticModel [ GaussianDistr ]) [view_source] ActorCriticModel for gym tasks with continuous control in the range [-1, 1].","title":"MemorylessActorCritic"},{"location":"api/allenact_plugins/gym_plugin/gym_sensors/","text":"allenact_plugins.gym_plugin.gym_sensors # [view_source] GymBox2DSensor # class GymBox2DSensor ( Sensor [ gym . Env , Task [ gym . Env ]]) [view_source] Wrapper for gym Box2D tasks' observations.","title":"gym_sensors"},{"location":"api/allenact_plugins/gym_plugin/gym_sensors/#allenact_pluginsgym_plugingym_sensors","text":"[view_source]","title":"allenact_plugins.gym_plugin.gym_sensors"},{"location":"api/allenact_plugins/gym_plugin/gym_sensors/#gymbox2dsensor","text":"class GymBox2DSensor ( Sensor [ gym . Env , Task [ gym . Env ]]) [view_source] Wrapper for gym Box2D tasks' observations.","title":"GymBox2DSensor"},{"location":"api/allenact_plugins/gym_plugin/gym_tasks/","text":"allenact_plugins.gym_plugin.gym_tasks # [view_source] GymTask # class GymTask ( Task [ gym . Env ]) [view_source] Abstract gym task. Subclasses need to implement class_action_names and _step . GymContinuousBox2DTask # class GymContinuousBox2DTask ( GymTask ) [view_source] Task for a continuous-control gym Box2D Env; it allows interfacing allenact with gym tasks. task_selector # task_selector ( env_name : str ) -> type [view_source] Helper function for GymTaskSampler . sensor_selector # sensor_selector ( env_name : str ) -> Sensor [view_source] Helper function for GymTaskSampler . GymTaskSampler # class GymTaskSampler ( TaskSampler ) [view_source] TaskSampler for gym environments.","title":"gym_tasks"},{"location":"api/allenact_plugins/gym_plugin/gym_tasks/#allenact_pluginsgym_plugingym_tasks","text":"[view_source]","title":"allenact_plugins.gym_plugin.gym_tasks"},{"location":"api/allenact_plugins/gym_plugin/gym_tasks/#gymtask","text":"class GymTask ( Task [ gym . Env ]) [view_source] Abstract gym task. Subclasses need to implement class_action_names and _step .","title":"GymTask"},{"location":"api/allenact_plugins/gym_plugin/gym_tasks/#gymcontinuousbox2dtask","text":"class GymContinuousBox2DTask ( GymTask ) [view_source] Task for a continuous-control gym Box2D Env; it allows interfacing allenact with gym tasks.","title":"GymContinuousBox2DTask"},{"location":"api/allenact_plugins/gym_plugin/gym_tasks/#task_selector","text":"task_selector ( env_name : str ) -> type [view_source] Helper function for GymTaskSampler .","title":"task_selector"},{"location":"api/allenact_plugins/gym_plugin/gym_tasks/#sensor_selector","text":"sensor_selector ( env_name : str ) -> Sensor [view_source] Helper function for GymTaskSampler .","title":"sensor_selector"},{"location":"api/allenact_plugins/gym_plugin/gym_tasks/#gymtasksampler","text":"class GymTaskSampler ( TaskSampler ) [view_source] TaskSampler for gym environments.","title":"GymTaskSampler"},{"location":"api/allenact_plugins/habitat_plugin/habitat_constants/","text":"allenact_plugins.habitat_plugin.habitat_constants # [view_source]","title":"habitat_constants"},{"location":"api/allenact_plugins/habitat_plugin/habitat_constants/#allenact_pluginshabitat_pluginhabitat_constants","text":"[view_source]","title":"allenact_plugins.habitat_plugin.habitat_constants"},{"location":"api/allenact_plugins/habitat_plugin/habitat_environment/","text":"allenact_plugins.habitat_plugin.habitat_environment # [view_source] A wrapper for interacting with the Habitat environment.","title":"habitat_environment"},{"location":"api/allenact_plugins/habitat_plugin/habitat_environment/#allenact_pluginshabitat_pluginhabitat_environment","text":"[view_source] A wrapper for interacting with the Habitat environment.","title":"allenact_plugins.habitat_plugin.habitat_environment"},{"location":"api/allenact_plugins/habitat_plugin/habitat_preprocessors/","text":"allenact_plugins.habitat_plugin.habitat_preprocessors # [view_source] ResnetPreProcessorHabitat # class ResnetPreProcessorHabitat ( ResNetPreprocessor ) [view_source] Preprocess RGB or depth image using a ResNet model.","title":"habitat_preprocessors"},{"location":"api/allenact_plugins/habitat_plugin/habitat_preprocessors/#allenact_pluginshabitat_pluginhabitat_preprocessors","text":"[view_source]","title":"allenact_plugins.habitat_plugin.habitat_preprocessors"},{"location":"api/allenact_plugins/habitat_plugin/habitat_preprocessors/#resnetpreprocessorhabitat","text":"class ResnetPreProcessorHabitat ( ResNetPreprocessor ) [view_source] Preprocess RGB or depth image using a ResNet model.","title":"ResnetPreProcessorHabitat"},{"location":"api/allenact_plugins/habitat_plugin/habitat_sensors/","text":"allenact_plugins.habitat_plugin.habitat_sensors # [view_source]","title":"habitat_sensors"},{"location":"api/allenact_plugins/habitat_plugin/habitat_sensors/#allenact_pluginshabitat_pluginhabitat_sensors","text":"[view_source]","title":"allenact_plugins.habitat_plugin.habitat_sensors"},{"location":"api/allenact_plugins/habitat_plugin/habitat_task_samplers/","text":"allenact_plugins.habitat_plugin.habitat_task_samplers # [view_source] PointNavTaskSampler # class PointNavTaskSampler ( TaskSampler ) [view_source] PointNavTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] @return: Number of total tasks remaining that can be sampled. Can be float('inf'). PointNavTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] @return: True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. ObjectNavTaskSampler # class ObjectNavTaskSampler ( TaskSampler ) [view_source] ObjectNavTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] @return: Number of total tasks remaining that can be sampled. Can be float('inf'). ObjectNavTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] @return: True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"habitat_task_samplers"},{"location":"api/allenact_plugins/habitat_plugin/habitat_task_samplers/#allenact_pluginshabitat_pluginhabitat_task_samplers","text":"[view_source]","title":"allenact_plugins.habitat_plugin.habitat_task_samplers"},{"location":"api/allenact_plugins/habitat_plugin/habitat_task_samplers/#pointnavtasksampler","text":"class PointNavTaskSampler ( TaskSampler ) [view_source]","title":"PointNavTaskSampler"},{"location":"api/allenact_plugins/habitat_plugin/habitat_task_samplers/#pointnavtasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] @return: Number of total tasks remaining that can be sampled. Can be float('inf').","title":"PointNavTaskSampler.length"},{"location":"api/allenact_plugins/habitat_plugin/habitat_task_samplers/#pointnavtasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] @return: True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"PointNavTaskSampler.all_observation_spaces_equal"},{"location":"api/allenact_plugins/habitat_plugin/habitat_task_samplers/#objectnavtasksampler","text":"class ObjectNavTaskSampler ( TaskSampler ) [view_source]","title":"ObjectNavTaskSampler"},{"location":"api/allenact_plugins/habitat_plugin/habitat_task_samplers/#objectnavtasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] @return: Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ObjectNavTaskSampler.length"},{"location":"api/allenact_plugins/habitat_plugin/habitat_task_samplers/#objectnavtasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] @return: True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"ObjectNavTaskSampler.all_observation_spaces_equal"},{"location":"api/allenact_plugins/habitat_plugin/habitat_tasks/","text":"allenact_plugins.habitat_plugin.habitat_tasks # [view_source]","title":"habitat_tasks"},{"location":"api/allenact_plugins/habitat_plugin/habitat_tasks/#allenact_pluginshabitat_pluginhabitat_tasks","text":"[view_source]","title":"allenact_plugins.habitat_plugin.habitat_tasks"},{"location":"api/allenact_plugins/habitat_plugin/habitat_utils/","text":"allenact_plugins.habitat_plugin.habitat_utils # [view_source] construct_env_configs # construct_env_configs ( config : Config , allow_scene_repeat : bool = False ) -> List [ Config ] [view_source] Create list of Habitat Configs for training on multiple processes To allow better performance, dataset are split into small ones for each individual env, grouped by scenes. Parameters config : configs that contain num_processes as well as information necessary to create individual environments. allow_scene_repeat : if True and the number of distinct scenes in the dataset is less than the total number of processes this will result in scenes being repeated across processes. If False , then if the total number of processes is greater than the number of scenes, this will result in a RuntimeError exception being raised. Returns List of Configs, one for each process.","title":"habitat_utils"},{"location":"api/allenact_plugins/habitat_plugin/habitat_utils/#allenact_pluginshabitat_pluginhabitat_utils","text":"[view_source]","title":"allenact_plugins.habitat_plugin.habitat_utils"},{"location":"api/allenact_plugins/habitat_plugin/habitat_utils/#construct_env_configs","text":"construct_env_configs ( config : Config , allow_scene_repeat : bool = False ) -> List [ Config ] [view_source] Create list of Habitat Configs for training on multiple processes To allow better performance, dataset are split into small ones for each individual env, grouped by scenes. Parameters config : configs that contain num_processes as well as information necessary to create individual environments. allow_scene_repeat : if True and the number of distinct scenes in the dataset is less than the total number of processes this will result in scenes being repeated across processes. If False , then if the total number of processes is greater than the number of scenes, this will result in a RuntimeError exception being raised. Returns List of Configs, one for each process.","title":"construct_env_configs"},{"location":"api/allenact_plugins/habitat_plugin/scripts/agent_demo/","text":"allenact_plugins.habitat_plugin.scripts.agent_demo # [view_source]","title":"agent_demo"},{"location":"api/allenact_plugins/habitat_plugin/scripts/agent_demo/#allenact_pluginshabitat_pluginscriptsagent_demo","text":"[view_source]","title":"allenact_plugins.habitat_plugin.scripts.agent_demo"},{"location":"api/allenact_plugins/habitat_plugin/scripts/make_map/","text":"allenact_plugins.habitat_plugin.scripts.make_map # [view_source]","title":"make_map"},{"location":"api/allenact_plugins/habitat_plugin/scripts/make_map/#allenact_pluginshabitat_pluginscriptsmake_map","text":"[view_source]","title":"allenact_plugins.habitat_plugin.scripts.make_map"},{"location":"api/allenact_plugins/ithor_plugin/ithor_constants/","text":"allenact_plugins.ithor_plugin.ithor_constants # [view_source] Common constants used when training agents to complete tasks in iTHOR, the interactive version of AI2-THOR.","title":"ithor_constants"},{"location":"api/allenact_plugins/ithor_plugin/ithor_constants/#allenact_pluginsithor_pluginithor_constants","text":"[view_source] Common constants used when training agents to complete tasks in iTHOR, the interactive version of AI2-THOR.","title":"allenact_plugins.ithor_plugin.ithor_constants"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/","text":"allenact_plugins.ithor_plugin.ithor_environment # [view_source] A wrapper for engaging with the THOR environment. IThorEnvironment # class IThorEnvironment ( object ) [view_source] Wrapper for the ai2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on AI2-THOR. Attributes controller : The ai2thor controller. IThorEnvironment.__init__ # | __init__ ( x_display : Optional [ str ] = None , docker_enabled : bool = False , local_thor_build : Optional [ str ] = None , visibility_distance : float = VISIBILITY_DISTANCE , fov : float = FOV , player_screen_width : int = 300 , player_screen_height : int = 300 , quality : str = \"Very Low\" , restrict_to_initially_reachable_points : bool = False , make_agents_visible : bool = True , object_open_speed : float = 1.0 , simplify_physics : bool = False ) -> None [view_source] Initializer. Parameters x_display : The x display into which to launch ai2thor (possibly necessarily if you are running on a server without an attached display). docker_enabled : Whether or not to run thor in a docker container (useful on a server without an attached display so that you don't have to start an x display). local_thor_build : The path to a local build of ai2thor. This is probably not necessary for your use case and can be safely ignored. visibility_distance : The distance (in meters) at which objects, in the viewport of the agent, are considered visible by ai2thor and will have their \"visible\" flag be set to True in the metadata. fov : The agent's camera's field of view. player_screen_width : The width resolution (in pixels) of the images returned by ai2thor. player_screen_height : The height resolution (in pixels) of the images returned by ai2thor. quality : The quality at which to render. Possible quality settings can be found in ai2thor._quality_settings.QUALITY_SETTINGS . restrict_to_initially_reachable_points : Whether or not to restrict the agent to locations in ai2thor that were found to be (initially) reachable by the agent (i.e. reachable by the agent after resetting the scene). This can be useful if you want to ensure there are only a fixed set of locations where the agent can go. make_agents_visible : Whether or not the agent should be visible. Most noticable when there are multiple agents or when quality settings are high so that the agent casts a shadow. object_open_speed : How quickly objects should be opened. High speeds mean faster simulation but also mean that opening objects have a lot of kinetic energy and can, possibly, knock other objects away. simplify_physics : Whether or not to simplify physics when applicable. Currently this only simplies object interactions when opening drawers (when simplified, objects within a drawer do not slide around on their own when the drawer is opened or closed, instead they are effectively glued down). IThorEnvironment.scene_name # | @property | scene_name () -> str [view_source] Current ai2thor scene. IThorEnvironment.current_frame # | @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view. IThorEnvironment.last_event # | @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller. IThorEnvironment.started # | @property | started () -> bool [view_source] Has the ai2thor controller been started. IThorEnvironment.last_action # | @property | last_action () -> str [view_source] Last action, as a string, taken by the agent. IThorEnvironment.last_action # | @last_action . setter | last_action ( value : str ) -> None [view_source] Set the last action taken by the agent. Doing this is rewriting history, be careful. IThorEnvironment.last_action_success # | @property | last_action_success () -> bool [view_source] Was the last action taken by the agent a success? IThorEnvironment.last_action_success # | @last_action_success . setter | last_action_success ( value : bool ) -> None [view_source] Set whether or not the last action taken by the agent was a success. Doing this is rewriting history, be careful. IThorEnvironment.last_action_return # | @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" . IThorEnvironment.last_action_return # | @last_action_return . setter | last_action_return ( value : Any ) -> None [view_source] Set the value returned by the last action. Doing this is rewriting history, be careful. IThorEnvironment.start # | start ( scene_name : Optional [ str ], move_mag : float = 0.25 , ** kwargs , ,) -> None [view_source] Starts the ai2thor controller if it was previously stopped. After starting, reset will be called with the scene name and move magnitude. Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to reset. IThorEnvironment.stop # | stop () -> None [view_source] Stops the ai2thor controller. IThorEnvironment.reset # | reset ( scene_name : Optional [ str ], move_mag : float = 0.25 , ** kwargs , ,) [view_source] Resets the ai2thor in a new scene. Resets ai2thor into a new scene and initializes the scene/agents with prespecified settings (e.g. move magnitude). Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to the controller \"Initialize\" action. IThorEnvironment.teleport_agent_to # | teleport_agent_to ( x : float , y : float , z : float , rotation : float , horizon : float , standing : Optional [ bool ] = None , force_action : bool = False , only_initially_reachable : Optional [ bool ] = None , verbose = True , ignore_y_diffs = False ) -> None [view_source] Helper function teleporting the agent to a given location. IThorEnvironment.random_reachable_state # | random_reachable_state ( seed : int = None ) -> Dict [view_source] Returns a random reachable location in the scene. IThorEnvironment.randomize_agent_location # | randomize_agent_location ( seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None ) -> Dict [view_source] Teleports the agent to a random reachable location in the scene. IThorEnvironment.object_pixels_in_frame # | object_pixels_in_frame ( object_id : str , hide_all : bool = True , hide_transparent : bool = False ) -> np . ndarray [view_source] Return an mask for a given object in the agent's current view. Parameters object_id : The id of the object. hide_all : Whether or not to hide all other objects in the scene before getting the mask. hide_transparent : Whether or not partially transparent objects are considered to occlude the object. Returns A numpy array of the mask. IThorEnvironment.object_pixels_on_grid # | object_pixels_on_grid ( object_id : str , grid_shape : Tuple [ int , int ], hide_all : bool = True , hide_transparent : bool = False ) -> np . ndarray [view_source] Like object_pixels_in_frame but counts object pixels in a partitioning of the image. IThorEnvironment.object_in_hand # | object_in_hand () [view_source] Object metadata for the object in the agent's hand. IThorEnvironment.initially_reachable_points # | @property | initially_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that were reachable after initially resetting. IThorEnvironment.initially_reachable_points_set # | @property | initially_reachable_points_set () -> Set [ Tuple [ float , float ]] [view_source] Set of (x,z) locations in the scene that were reachable after initially resetting. IThorEnvironment.currently_reachable_points # | @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable. IThorEnvironment.get_agent_location # | get_agent_location () -> Dict [ str , Union [ float , bool ]] [view_source] Gets agent's location. IThorEnvironment.step # | step ( action_dict : Optional [ Dict [ str , Union [ str , int , float , Dict ]]] = None , ** kwargs : Union [ str , int , float , Dict ], ,) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment. IThorEnvironment.position_dist # | @staticmethod | position_dist ( p0 : Mapping [ str , Any ], p1 : Mapping [ str , Any ], ignore_y : bool = False , l1_dist : bool = False ) -> float [view_source] Distance between two points of the form {\"x\": x, \"y\":y, \"z\":z\"}. IThorEnvironment.rotation_dist # | @staticmethod | rotation_dist ( a : Dict [ str , float ], b : Dict [ str , float ]) [view_source] Distance between rotations. IThorEnvironment.closest_object_with_properties # | closest_object_with_properties ( properties : Dict [ str , Any ]) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that has the given properties. IThorEnvironment.closest_visible_object_of_type # | closest_visible_object_of_type ( object_type : str ) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that is visible and has the given type. IThorEnvironment.closest_object_of_type # | closest_object_of_type ( object_type : str ) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that has the given type. IThorEnvironment.closest_reachable_point_to_position # | closest_reachable_point_to_position ( position : Dict [ str , float ]) -> Tuple [ Dict [ str , float ], float ] [view_source] Of all reachable positions, find the one that is closest to the given location. IThorEnvironment.all_objects # | all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata. IThorEnvironment.all_objects_with_properties # | all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties. IThorEnvironment.visible_objects # | visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects.","title":"ithor_environment"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#allenact_pluginsithor_pluginithor_environment","text":"[view_source] A wrapper for engaging with the THOR environment.","title":"allenact_plugins.ithor_plugin.ithor_environment"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironment","text":"class IThorEnvironment ( object ) [view_source] Wrapper for the ai2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on AI2-THOR. Attributes controller : The ai2thor controller.","title":"IThorEnvironment"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironment__init__","text":"| __init__ ( x_display : Optional [ str ] = None , docker_enabled : bool = False , local_thor_build : Optional [ str ] = None , visibility_distance : float = VISIBILITY_DISTANCE , fov : float = FOV , player_screen_width : int = 300 , player_screen_height : int = 300 , quality : str = \"Very Low\" , restrict_to_initially_reachable_points : bool = False , make_agents_visible : bool = True , object_open_speed : float = 1.0 , simplify_physics : bool = False ) -> None [view_source] Initializer. Parameters x_display : The x display into which to launch ai2thor (possibly necessarily if you are running on a server without an attached display). docker_enabled : Whether or not to run thor in a docker container (useful on a server without an attached display so that you don't have to start an x display). local_thor_build : The path to a local build of ai2thor. This is probably not necessary for your use case and can be safely ignored. visibility_distance : The distance (in meters) at which objects, in the viewport of the agent, are considered visible by ai2thor and will have their \"visible\" flag be set to True in the metadata. fov : The agent's camera's field of view. player_screen_width : The width resolution (in pixels) of the images returned by ai2thor. player_screen_height : The height resolution (in pixels) of the images returned by ai2thor. quality : The quality at which to render. Possible quality settings can be found in ai2thor._quality_settings.QUALITY_SETTINGS . restrict_to_initially_reachable_points : Whether or not to restrict the agent to locations in ai2thor that were found to be (initially) reachable by the agent (i.e. reachable by the agent after resetting the scene). This can be useful if you want to ensure there are only a fixed set of locations where the agent can go. make_agents_visible : Whether or not the agent should be visible. Most noticable when there are multiple agents or when quality settings are high so that the agent casts a shadow. object_open_speed : How quickly objects should be opened. High speeds mean faster simulation but also mean that opening objects have a lot of kinetic energy and can, possibly, knock other objects away. simplify_physics : Whether or not to simplify physics when applicable. Currently this only simplies object interactions when opening drawers (when simplified, objects within a drawer do not slide around on their own when the drawer is opened or closed, instead they are effectively glued down).","title":"IThorEnvironment.__init__"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentscene_name","text":"| @property | scene_name () -> str [view_source] Current ai2thor scene.","title":"IThorEnvironment.scene_name"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentcurrent_frame","text":"| @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view.","title":"IThorEnvironment.current_frame"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_event","text":"| @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller.","title":"IThorEnvironment.last_event"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentstarted","text":"| @property | started () -> bool [view_source] Has the ai2thor controller been started.","title":"IThorEnvironment.started"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action","text":"| @property | last_action () -> str [view_source] Last action, as a string, taken by the agent.","title":"IThorEnvironment.last_action"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action_1","text":"| @last_action . setter | last_action ( value : str ) -> None [view_source] Set the last action taken by the agent. Doing this is rewriting history, be careful.","title":"IThorEnvironment.last_action"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action_success","text":"| @property | last_action_success () -> bool [view_source] Was the last action taken by the agent a success?","title":"IThorEnvironment.last_action_success"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action_success_1","text":"| @last_action_success . setter | last_action_success ( value : bool ) -> None [view_source] Set whether or not the last action taken by the agent was a success. Doing this is rewriting history, be careful.","title":"IThorEnvironment.last_action_success"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action_return","text":"| @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" .","title":"IThorEnvironment.last_action_return"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action_return_1","text":"| @last_action_return . setter | last_action_return ( value : Any ) -> None [view_source] Set the value returned by the last action. Doing this is rewriting history, be careful.","title":"IThorEnvironment.last_action_return"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentstart","text":"| start ( scene_name : Optional [ str ], move_mag : float = 0.25 , ** kwargs , ,) -> None [view_source] Starts the ai2thor controller if it was previously stopped. After starting, reset will be called with the scene name and move magnitude. Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to reset.","title":"IThorEnvironment.start"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentstop","text":"| stop () -> None [view_source] Stops the ai2thor controller.","title":"IThorEnvironment.stop"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentreset","text":"| reset ( scene_name : Optional [ str ], move_mag : float = 0.25 , ** kwargs , ,) [view_source] Resets the ai2thor in a new scene. Resets ai2thor into a new scene and initializes the scene/agents with prespecified settings (e.g. move magnitude). Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to the controller \"Initialize\" action.","title":"IThorEnvironment.reset"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentteleport_agent_to","text":"| teleport_agent_to ( x : float , y : float , z : float , rotation : float , horizon : float , standing : Optional [ bool ] = None , force_action : bool = False , only_initially_reachable : Optional [ bool ] = None , verbose = True , ignore_y_diffs = False ) -> None [view_source] Helper function teleporting the agent to a given location.","title":"IThorEnvironment.teleport_agent_to"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentrandom_reachable_state","text":"| random_reachable_state ( seed : int = None ) -> Dict [view_source] Returns a random reachable location in the scene.","title":"IThorEnvironment.random_reachable_state"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentrandomize_agent_location","text":"| randomize_agent_location ( seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None ) -> Dict [view_source] Teleports the agent to a random reachable location in the scene.","title":"IThorEnvironment.randomize_agent_location"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentobject_pixels_in_frame","text":"| object_pixels_in_frame ( object_id : str , hide_all : bool = True , hide_transparent : bool = False ) -> np . ndarray [view_source] Return an mask for a given object in the agent's current view. Parameters object_id : The id of the object. hide_all : Whether or not to hide all other objects in the scene before getting the mask. hide_transparent : Whether or not partially transparent objects are considered to occlude the object. Returns A numpy array of the mask.","title":"IThorEnvironment.object_pixels_in_frame"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentobject_pixels_on_grid","text":"| object_pixels_on_grid ( object_id : str , grid_shape : Tuple [ int , int ], hide_all : bool = True , hide_transparent : bool = False ) -> np . ndarray [view_source] Like object_pixels_in_frame but counts object pixels in a partitioning of the image.","title":"IThorEnvironment.object_pixels_on_grid"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentobject_in_hand","text":"| object_in_hand () [view_source] Object metadata for the object in the agent's hand.","title":"IThorEnvironment.object_in_hand"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentinitially_reachable_points","text":"| @property | initially_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that were reachable after initially resetting.","title":"IThorEnvironment.initially_reachable_points"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentinitially_reachable_points_set","text":"| @property | initially_reachable_points_set () -> Set [ Tuple [ float , float ]] [view_source] Set of (x,z) locations in the scene that were reachable after initially resetting.","title":"IThorEnvironment.initially_reachable_points_set"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentcurrently_reachable_points","text":"| @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable.","title":"IThorEnvironment.currently_reachable_points"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentget_agent_location","text":"| get_agent_location () -> Dict [ str , Union [ float , bool ]] [view_source] Gets agent's location.","title":"IThorEnvironment.get_agent_location"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentstep","text":"| step ( action_dict : Optional [ Dict [ str , Union [ str , int , float , Dict ]]] = None , ** kwargs : Union [ str , int , float , Dict ], ,) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment.","title":"IThorEnvironment.step"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentposition_dist","text":"| @staticmethod | position_dist ( p0 : Mapping [ str , Any ], p1 : Mapping [ str , Any ], ignore_y : bool = False , l1_dist : bool = False ) -> float [view_source] Distance between two points of the form {\"x\": x, \"y\":y, \"z\":z\"}.","title":"IThorEnvironment.position_dist"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentrotation_dist","text":"| @staticmethod | rotation_dist ( a : Dict [ str , float ], b : Dict [ str , float ]) [view_source] Distance between rotations.","title":"IThorEnvironment.rotation_dist"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentclosest_object_with_properties","text":"| closest_object_with_properties ( properties : Dict [ str , Any ]) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that has the given properties.","title":"IThorEnvironment.closest_object_with_properties"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentclosest_visible_object_of_type","text":"| closest_visible_object_of_type ( object_type : str ) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that is visible and has the given type.","title":"IThorEnvironment.closest_visible_object_of_type"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentclosest_object_of_type","text":"| closest_object_of_type ( object_type : str ) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that has the given type.","title":"IThorEnvironment.closest_object_of_type"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentclosest_reachable_point_to_position","text":"| closest_reachable_point_to_position ( position : Dict [ str , float ]) -> Tuple [ Dict [ str , float ], float ] [view_source] Of all reachable positions, find the one that is closest to the given location.","title":"IThorEnvironment.closest_reachable_point_to_position"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentall_objects","text":"| all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata.","title":"IThorEnvironment.all_objects"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentall_objects_with_properties","text":"| all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties.","title":"IThorEnvironment.all_objects_with_properties"},{"location":"api/allenact_plugins/ithor_plugin/ithor_environment/#ithorenvironmentvisible_objects","text":"| visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects.","title":"IThorEnvironment.visible_objects"},{"location":"api/allenact_plugins/ithor_plugin/ithor_sensors/","text":"allenact_plugins.ithor_plugin.ithor_sensors # [view_source] RGBSensorThor # class RGBSensorThor ( RGBSensor [ Union [ IThorEnvironment , RoboThorEnvironment ], Union [ Task [ IThorEnvironment ], Task [ RoboThorEnvironment ]], ]) [view_source] Sensor for RGB images in THOR. Returns from a running IThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"ithor_sensors"},{"location":"api/allenact_plugins/ithor_plugin/ithor_sensors/#allenact_pluginsithor_pluginithor_sensors","text":"[view_source]","title":"allenact_plugins.ithor_plugin.ithor_sensors"},{"location":"api/allenact_plugins/ithor_plugin/ithor_sensors/#rgbsensorthor","text":"class RGBSensorThor ( RGBSensor [ Union [ IThorEnvironment , RoboThorEnvironment ], Union [ Task [ IThorEnvironment ], Task [ RoboThorEnvironment ]], ]) [view_source] Sensor for RGB images in THOR. Returns from a running IThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"RGBSensorThor"},{"location":"api/allenact_plugins/ithor_plugin/ithor_task_samplers/","text":"allenact_plugins.ithor_plugin.ithor_task_samplers # [view_source] ObjectNavTaskSampler # class ObjectNavTaskSampler ( TaskSampler ) [view_source] ObjectNavTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). ObjectNavTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"ithor_task_samplers"},{"location":"api/allenact_plugins/ithor_plugin/ithor_task_samplers/#allenact_pluginsithor_pluginithor_task_samplers","text":"[view_source]","title":"allenact_plugins.ithor_plugin.ithor_task_samplers"},{"location":"api/allenact_plugins/ithor_plugin/ithor_task_samplers/#objectnavtasksampler","text":"class ObjectNavTaskSampler ( TaskSampler ) [view_source]","title":"ObjectNavTaskSampler"},{"location":"api/allenact_plugins/ithor_plugin/ithor_task_samplers/#objectnavtasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ObjectNavTaskSampler.length"},{"location":"api/allenact_plugins/ithor_plugin/ithor_task_samplers/#objectnavtasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"ObjectNavTaskSampler.all_observation_spaces_equal"},{"location":"api/allenact_plugins/ithor_plugin/ithor_tasks/","text":"allenact_plugins.ithor_plugin.ithor_tasks # [view_source] ObjectNaviThorGridTask # class ObjectNaviThorGridTask ( Task [ IThorEnvironment ]) [view_source] Defines the object navigation task in AI2-THOR. In object navigation an agent is randomly initialized into an AI2-THOR scene and must find an object of a given type (e.g. tomato, television, etc). An object is considered found if the agent takes an End action and the object is visible to the agent (see here for a definition of visibiliy in AI2-THOR). The actions available to an agent in this task are: Move ahead Moves agent ahead by 0.25 meters. Rotate left / rotate right Rotates the agent by 90 degrees counter-clockwise / clockwise. Look down / look up Changes agent view angle by 30 degrees up or down. An agent cannot look more than 30 degrees above horizontal or less than 60 degrees below horizontal. End Ends the task and the agent receives a positive reward if the object type is visible to the agent, otherwise it receives a negative reward. Attributes env : The ai2thor environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : The task info. Must contain a field \"object_type\" that specifies, as a string, the goal object type. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors. ObjectNaviThorGridTask.__init__ # | __init__ ( env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs , ,) -> None [view_source] Initializer. See class documentation for parameter definitions. ObjectNaviThorGridTask.is_goal_object_visible # | is_goal_object_visible () -> bool [view_source] Is the goal object currently visible? ObjectNaviThorGridTask.judge # | judge () -> float [view_source] Compute the reward after having taken a step.","title":"ithor_tasks"},{"location":"api/allenact_plugins/ithor_plugin/ithor_tasks/#allenact_pluginsithor_pluginithor_tasks","text":"[view_source]","title":"allenact_plugins.ithor_plugin.ithor_tasks"},{"location":"api/allenact_plugins/ithor_plugin/ithor_tasks/#objectnavithorgridtask","text":"class ObjectNaviThorGridTask ( Task [ IThorEnvironment ]) [view_source] Defines the object navigation task in AI2-THOR. In object navigation an agent is randomly initialized into an AI2-THOR scene and must find an object of a given type (e.g. tomato, television, etc). An object is considered found if the agent takes an End action and the object is visible to the agent (see here for a definition of visibiliy in AI2-THOR). The actions available to an agent in this task are: Move ahead Moves agent ahead by 0.25 meters. Rotate left / rotate right Rotates the agent by 90 degrees counter-clockwise / clockwise. Look down / look up Changes agent view angle by 30 degrees up or down. An agent cannot look more than 30 degrees above horizontal or less than 60 degrees below horizontal. End Ends the task and the agent receives a positive reward if the object type is visible to the agent, otherwise it receives a negative reward. Attributes env : The ai2thor environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : The task info. Must contain a field \"object_type\" that specifies, as a string, the goal object type. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors.","title":"ObjectNaviThorGridTask"},{"location":"api/allenact_plugins/ithor_plugin/ithor_tasks/#objectnavithorgridtask__init__","text":"| __init__ ( env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs , ,) -> None [view_source] Initializer. See class documentation for parameter definitions.","title":"ObjectNaviThorGridTask.__init__"},{"location":"api/allenact_plugins/ithor_plugin/ithor_tasks/#objectnavithorgridtaskis_goal_object_visible","text":"| is_goal_object_visible () -> bool [view_source] Is the goal object currently visible?","title":"ObjectNaviThorGridTask.is_goal_object_visible"},{"location":"api/allenact_plugins/ithor_plugin/ithor_tasks/#objectnavithorgridtaskjudge","text":"| judge () -> float [view_source] Compute the reward after having taken a step.","title":"ObjectNaviThorGridTask.judge"},{"location":"api/allenact_plugins/ithor_plugin/ithor_util/","text":"allenact_plugins.ithor_plugin.ithor_util # [view_source] round_to_factor # round_to_factor ( num : float , base : int ) -> int [view_source] Rounds floating point number to the nearest integer multiple of the given base. E.g., for floating number 90.1 and integer base 45, the result is 90. Attributes num : floating point number to be rounded. base : integer base","title":"ithor_util"},{"location":"api/allenact_plugins/ithor_plugin/ithor_util/#allenact_pluginsithor_pluginithor_util","text":"[view_source]","title":"allenact_plugins.ithor_plugin.ithor_util"},{"location":"api/allenact_plugins/ithor_plugin/ithor_util/#round_to_factor","text":"round_to_factor ( num : float , base : int ) -> int [view_source] Rounds floating point number to the nearest integer multiple of the given base. E.g., for floating number 90.1 and integer base 45, the result is 90. Attributes num : floating point number to be rounded. base : integer base","title":"round_to_factor"},{"location":"api/allenact_plugins/ithor_plugin/ithor_viz/","text":"allenact_plugins.ithor_plugin.ithor_viz # [view_source]","title":"ithor_viz"},{"location":"api/allenact_plugins/ithor_plugin/ithor_viz/#allenact_pluginsithor_pluginithor_viz","text":"[view_source]","title":"allenact_plugins.ithor_plugin.ithor_viz"},{"location":"api/allenact_plugins/ithor_plugin/scripts/make_objectnav_debug_dataset/","text":"allenact_plugins.ithor_plugin.scripts.make_objectnav_debug_dataset # [view_source]","title":"make_objectnav_debug_dataset"},{"location":"api/allenact_plugins/ithor_plugin/scripts/make_objectnav_debug_dataset/#allenact_pluginsithor_pluginscriptsmake_objectnav_debug_dataset","text":"[view_source]","title":"allenact_plugins.ithor_plugin.scripts.make_objectnav_debug_dataset"},{"location":"api/allenact_plugins/ithor_plugin/scripts/make_pointnav_debug_dataset/","text":"allenact_plugins.ithor_plugin.scripts.make_pointnav_debug_dataset # [view_source]","title":"make_pointnav_debug_dataset"},{"location":"api/allenact_plugins/ithor_plugin/scripts/make_pointnav_debug_dataset/#allenact_pluginsithor_pluginscriptsmake_pointnav_debug_dataset","text":"[view_source]","title":"allenact_plugins.ithor_plugin.scripts.make_pointnav_debug_dataset"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_environment/","text":"allenact_plugins.lighthouse_plugin.lighthouse_environment # [view_source]","title":"lighthouse_environment"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_environment/#allenact_pluginslighthouse_pluginlighthouse_environment","text":"[view_source]","title":"allenact_plugins.lighthouse_plugin.lighthouse_environment"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_models/","text":"allenact_plugins.lighthouse_plugin.lighthouse_models # [view_source]","title":"lighthouse_models"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_models/#allenact_pluginslighthouse_pluginlighthouse_models","text":"[view_source]","title":"allenact_plugins.lighthouse_plugin.lighthouse_models"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_sensors/","text":"allenact_plugins.lighthouse_plugin.lighthouse_sensors # [view_source]","title":"lighthouse_sensors"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_sensors/#allenact_pluginslighthouse_pluginlighthouse_sensors","text":"[view_source]","title":"allenact_plugins.lighthouse_plugin.lighthouse_sensors"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_tasks/","text":"allenact_plugins.lighthouse_plugin.lighthouse_tasks # [view_source] LightHouseTask # class LightHouseTask ( Task [ LightHouseEnvironment ], abc . ABC ) [view_source] Defines an abstract embodied task in the light house gridworld. Attributes env : The light house environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : Dictionary of (k, v) pairs defining task goals and other task information. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors. LightHouseTask.__init__ # | __init__ ( env : LightHouseEnvironment , sensors : Union [ SensorSuite , List [ Sensor ]], task_info : Dict [ str , Any ], max_steps : int , ** kwargs , ,) -> None [view_source] Initializer. See class documentation for parameter definitions.","title":"lighthouse_tasks"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_tasks/#allenact_pluginslighthouse_pluginlighthouse_tasks","text":"[view_source]","title":"allenact_plugins.lighthouse_plugin.lighthouse_tasks"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_tasks/#lighthousetask","text":"class LightHouseTask ( Task [ LightHouseEnvironment ], abc . ABC ) [view_source] Defines an abstract embodied task in the light house gridworld. Attributes env : The light house environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : Dictionary of (k, v) pairs defining task goals and other task information. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors.","title":"LightHouseTask"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_tasks/#lighthousetask__init__","text":"| __init__ ( env : LightHouseEnvironment , sensors : Union [ SensorSuite , List [ Sensor ]], task_info : Dict [ str , Any ], max_steps : int , ** kwargs , ,) -> None [view_source] Initializer. See class documentation for parameter definitions.","title":"LightHouseTask.__init__"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_util/","text":"allenact_plugins.lighthouse_plugin.lighthouse_util # [view_source]","title":"lighthouse_util"},{"location":"api/allenact_plugins/lighthouse_plugin/lighthouse_util/#allenact_pluginslighthouse_pluginlighthouse_util","text":"[view_source]","title":"allenact_plugins.lighthouse_plugin.lighthouse_util"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_environments/","text":"allenact_plugins.minigrid_plugin.minigrid_environments # [view_source] FastCrossing # class FastCrossing ( CrossingEnv ) [view_source] Similar to CrossingEnv , but to support faster task sampling as per repeat_failed_task_for_min_steps flag in MiniGridTaskSampler. AskForHelpSimpleCrossing # class AskForHelpSimpleCrossing ( CrossingEnv ) [view_source] Corresponds to WC FAULTY SWITCH environment. AskForHelpSimpleCrossing.step # | step ( action : int ) [view_source] Reveal the observation only if the toggle action is executed.","title":"minigrid_environments"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_environments/#allenact_pluginsminigrid_pluginminigrid_environments","text":"[view_source]","title":"allenact_plugins.minigrid_plugin.minigrid_environments"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_environments/#fastcrossing","text":"class FastCrossing ( CrossingEnv ) [view_source] Similar to CrossingEnv , but to support faster task sampling as per repeat_failed_task_for_min_steps flag in MiniGridTaskSampler.","title":"FastCrossing"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_environments/#askforhelpsimplecrossing","text":"class AskForHelpSimpleCrossing ( CrossingEnv ) [view_source] Corresponds to WC FAULTY SWITCH environment.","title":"AskForHelpSimpleCrossing"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_environments/#askforhelpsimplecrossingstep","text":"| step ( action : int ) [view_source] Reveal the observation only if the toggle action is executed.","title":"AskForHelpSimpleCrossing.step"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_models/","text":"allenact_plugins.minigrid_plugin.minigrid_models # [view_source]","title":"minigrid_models"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_models/#allenact_pluginsminigrid_pluginminigrid_models","text":"[view_source]","title":"allenact_plugins.minigrid_plugin.minigrid_models"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_offpolicy/","text":"allenact_plugins.minigrid_plugin.minigrid_offpolicy # [view_source]","title":"minigrid_offpolicy"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_offpolicy/#allenact_pluginsminigrid_pluginminigrid_offpolicy","text":"[view_source]","title":"allenact_plugins.minigrid_plugin.minigrid_offpolicy"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_sensors/","text":"allenact_plugins.minigrid_plugin.minigrid_sensors # [view_source]","title":"minigrid_sensors"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_sensors/#allenact_pluginsminigrid_pluginminigrid_sensors","text":"[view_source]","title":"allenact_plugins.minigrid_plugin.minigrid_sensors"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_tasks/","text":"allenact_plugins.minigrid_plugin.minigrid_tasks # [view_source] MiniGridTask # class MiniGridTask ( Task [ CrossingEnv ]) [view_source] MiniGridTask.generate_graph # | generate_graph () -> nx . DiGraph [view_source] The generated graph is based on the fully observable grid (as the expert sees it all). env: environment to generate the graph over","title":"minigrid_tasks"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_tasks/#allenact_pluginsminigrid_pluginminigrid_tasks","text":"[view_source]","title":"allenact_plugins.minigrid_plugin.minigrid_tasks"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_tasks/#minigridtask","text":"class MiniGridTask ( Task [ CrossingEnv ]) [view_source]","title":"MiniGridTask"},{"location":"api/allenact_plugins/minigrid_plugin/minigrid_tasks/#minigridtaskgenerate_graph","text":"| generate_graph () -> nx . DiGraph [view_source] The generated graph is based on the fully observable grid (as the expert sees it all). env: environment to generate the graph over","title":"MiniGridTask.generate_graph"},{"location":"api/allenact_plugins/minigrid_plugin/configs/minigrid_nomemory/","text":"allenact_plugins.minigrid_plugin.configs.minigrid_nomemory # [view_source] Experiment Config for MiniGrid tutorial.","title":"minigrid_nomemory"},{"location":"api/allenact_plugins/minigrid_plugin/configs/minigrid_nomemory/#allenact_pluginsminigrid_pluginconfigsminigrid_nomemory","text":"[view_source] Experiment Config for MiniGrid tutorial.","title":"allenact_plugins.minigrid_plugin.configs.minigrid_nomemory"},{"location":"api/allenact_plugins/robothor_plugin/robothor_constants/","text":"allenact_plugins.robothor_plugin.robothor_constants # [view_source]","title":"robothor_constants"},{"location":"api/allenact_plugins/robothor_plugin/robothor_constants/#allenact_pluginsrobothor_pluginrobothor_constants","text":"[view_source]","title":"allenact_plugins.robothor_plugin.robothor_constants"},{"location":"api/allenact_plugins/robothor_plugin/robothor_distributions/","text":"allenact_plugins.robothor_plugin.robothor_distributions # [view_source]","title":"robothor_distributions"},{"location":"api/allenact_plugins/robothor_plugin/robothor_distributions/#allenact_pluginsrobothor_pluginrobothor_distributions","text":"[view_source]","title":"allenact_plugins.robothor_plugin.robothor_distributions"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/","text":"allenact_plugins.robothor_plugin.robothor_environment # [view_source] RoboThorEnvironment # class RoboThorEnvironment () [view_source] Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration RoboThorEnvironment.initialize_grid_dimensions # | initialize_grid_dimensions ( reachable_points : Collection [ Dict [ str , float ]]) -> Tuple [ int , int , int , int ] [view_source] Computes bounding box for reachable points quantized with the current gridSize. RoboThorEnvironment.distance_from_point_to_object_type # | distance_from_point_to_object_type ( point : Dict [ str , float ], object_type : str , allowed_error : float ) -> float [view_source] Minimal geodesic distance from a point to an object of the given type. It might return -1.0 for unreachable targets. RoboThorEnvironment.distance_to_object_type # | distance_to_object_type ( object_type : str , agent_id : int = 0 ) -> float [view_source] Minimal geodesic distance to object of given type from agent's current location. It might return -1.0 for unreachable targets. RoboThorEnvironment.distance_to_point # | distance_to_point ( target : Dict [ str , float ], agent_id : int = 0 ) -> float [view_source] Minimal geodesic distance to end point from agent's current location. It might return -1.0 for unreachable targets. RoboThorEnvironment.agent_state # | agent_state ( agent_id : int = 0 ) -> Dict [view_source] Return agent position, rotation and horizon. RoboThorEnvironment.reset # | reset ( scene_name : str = None , filtered_objects : Optional [ List [ str ]] = None ) -> None [view_source] Resets scene to a known initial state. RoboThorEnvironment.random_reachable_state # | random_reachable_state ( seed : Optional [ int ] = None ) -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Returns a random reachable location in the scene. RoboThorEnvironment.randomize_agent_location # | randomize_agent_location ( seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None , agent_id : int = 0 ) -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Teleports the agent to a random reachable location in the scene. RoboThorEnvironment.currently_reachable_points # | @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable. RoboThorEnvironment.scene_name # | @property | scene_name () -> str [view_source] Current ai2thor scene. RoboThorEnvironment.current_frame # | @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view. RoboThorEnvironment.current_depth # | @property | current_depth () -> np . ndarray [view_source] Returns depth image corresponding to the agent's egocentric view. RoboThorEnvironment.current_frames # | @property | current_frames () -> List [ np . ndarray ] [view_source] Returns rgb images corresponding to the agents' egocentric views. RoboThorEnvironment.current_depths # | @property | current_depths () -> List [ np . ndarray ] [view_source] Returns depth images corresponding to the agents' egocentric views. RoboThorEnvironment.last_event # | @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller. RoboThorEnvironment.last_action # | @property | last_action () -> str [view_source] Last action, as a string, taken by the agent. RoboThorEnvironment.last_action_success # | @property | last_action_success () -> bool [view_source] Was the last action taken by the agent a success? RoboThorEnvironment.last_action_return # | @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" . RoboThorEnvironment.step # | step ( action_dict : Optional [ Dict [ str , Union [ str , int , float , Dict ]]] = None , ** kwargs : Union [ str , int , float , Dict ], ,) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment. RoboThorEnvironment.stop # | stop () [view_source] Stops the ai2thor controller. RoboThorEnvironment.all_objects # | all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata. RoboThorEnvironment.all_objects_with_properties # | all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties. RoboThorEnvironment.visible_objects # | visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects. RoboThorCachedEnvironment # class RoboThorCachedEnvironment () [view_source] Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration RoboThorCachedEnvironment.agent_state # | agent_state () -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Return agent position, rotation and horizon. RoboThorCachedEnvironment.reset # | reset ( scene_name : str = None ) -> None [view_source] Resets scene to a known initial state. RoboThorCachedEnvironment.currently_reachable_points # | @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable. RoboThorCachedEnvironment.scene_name # | @property | scene_name () -> str [view_source] Current ai2thor scene. RoboThorCachedEnvironment.current_frame # | @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view. RoboThorCachedEnvironment.current_depth # | @property | current_depth () -> np . ndarray [view_source] Returns depth image corresponding to the agent's egocentric view. RoboThorCachedEnvironment.last_event # | @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller. RoboThorCachedEnvironment.last_action # | @property | last_action () -> str [view_source] Last action, as a string, taken by the agent. RoboThorCachedEnvironment.last_action_success # | @property | last_action_success () -> bool [view_source] In the cached environment, all actions succeed. RoboThorCachedEnvironment.last_action_return # | @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" . RoboThorCachedEnvironment.step # | step ( action_dict : Dict [ str , Union [ str , int , float ]]) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment. RoboThorCachedEnvironment.stop # | stop () [view_source] Stops the ai2thor controller. RoboThorCachedEnvironment.all_objects # | all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata. RoboThorCachedEnvironment.all_objects_with_properties # | all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties. RoboThorCachedEnvironment.visible_objects # | visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects.","title":"robothor_environment"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#allenact_pluginsrobothor_pluginrobothor_environment","text":"[view_source]","title":"allenact_plugins.robothor_plugin.robothor_environment"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironment","text":"class RoboThorEnvironment () [view_source] Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration","title":"RoboThorEnvironment"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentinitialize_grid_dimensions","text":"| initialize_grid_dimensions ( reachable_points : Collection [ Dict [ str , float ]]) -> Tuple [ int , int , int , int ] [view_source] Computes bounding box for reachable points quantized with the current gridSize.","title":"RoboThorEnvironment.initialize_grid_dimensions"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentdistance_from_point_to_object_type","text":"| distance_from_point_to_object_type ( point : Dict [ str , float ], object_type : str , allowed_error : float ) -> float [view_source] Minimal geodesic distance from a point to an object of the given type. It might return -1.0 for unreachable targets.","title":"RoboThorEnvironment.distance_from_point_to_object_type"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentdistance_to_object_type","text":"| distance_to_object_type ( object_type : str , agent_id : int = 0 ) -> float [view_source] Minimal geodesic distance to object of given type from agent's current location. It might return -1.0 for unreachable targets.","title":"RoboThorEnvironment.distance_to_object_type"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentdistance_to_point","text":"| distance_to_point ( target : Dict [ str , float ], agent_id : int = 0 ) -> float [view_source] Minimal geodesic distance to end point from agent's current location. It might return -1.0 for unreachable targets.","title":"RoboThorEnvironment.distance_to_point"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentagent_state","text":"| agent_state ( agent_id : int = 0 ) -> Dict [view_source] Return agent position, rotation and horizon.","title":"RoboThorEnvironment.agent_state"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentreset","text":"| reset ( scene_name : str = None , filtered_objects : Optional [ List [ str ]] = None ) -> None [view_source] Resets scene to a known initial state.","title":"RoboThorEnvironment.reset"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentrandom_reachable_state","text":"| random_reachable_state ( seed : Optional [ int ] = None ) -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Returns a random reachable location in the scene.","title":"RoboThorEnvironment.random_reachable_state"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentrandomize_agent_location","text":"| randomize_agent_location ( seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None , agent_id : int = 0 ) -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Teleports the agent to a random reachable location in the scene.","title":"RoboThorEnvironment.randomize_agent_location"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentcurrently_reachable_points","text":"| @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable.","title":"RoboThorEnvironment.currently_reachable_points"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentscene_name","text":"| @property | scene_name () -> str [view_source] Current ai2thor scene.","title":"RoboThorEnvironment.scene_name"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentcurrent_frame","text":"| @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view.","title":"RoboThorEnvironment.current_frame"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentcurrent_depth","text":"| @property | current_depth () -> np . ndarray [view_source] Returns depth image corresponding to the agent's egocentric view.","title":"RoboThorEnvironment.current_depth"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentcurrent_frames","text":"| @property | current_frames () -> List [ np . ndarray ] [view_source] Returns rgb images corresponding to the agents' egocentric views.","title":"RoboThorEnvironment.current_frames"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentcurrent_depths","text":"| @property | current_depths () -> List [ np . ndarray ] [view_source] Returns depth images corresponding to the agents' egocentric views.","title":"RoboThorEnvironment.current_depths"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentlast_event","text":"| @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller.","title":"RoboThorEnvironment.last_event"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentlast_action","text":"| @property | last_action () -> str [view_source] Last action, as a string, taken by the agent.","title":"RoboThorEnvironment.last_action"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentlast_action_success","text":"| @property | last_action_success () -> bool [view_source] Was the last action taken by the agent a success?","title":"RoboThorEnvironment.last_action_success"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentlast_action_return","text":"| @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" .","title":"RoboThorEnvironment.last_action_return"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentstep","text":"| step ( action_dict : Optional [ Dict [ str , Union [ str , int , float , Dict ]]] = None , ** kwargs : Union [ str , int , float , Dict ], ,) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment.","title":"RoboThorEnvironment.step"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentstop","text":"| stop () [view_source] Stops the ai2thor controller.","title":"RoboThorEnvironment.stop"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentall_objects","text":"| all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata.","title":"RoboThorEnvironment.all_objects"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentall_objects_with_properties","text":"| all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties.","title":"RoboThorEnvironment.all_objects_with_properties"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorenvironmentvisible_objects","text":"| visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects.","title":"RoboThorEnvironment.visible_objects"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironment","text":"class RoboThorCachedEnvironment () [view_source] Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration","title":"RoboThorCachedEnvironment"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentagent_state","text":"| agent_state () -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Return agent position, rotation and horizon.","title":"RoboThorCachedEnvironment.agent_state"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentreset","text":"| reset ( scene_name : str = None ) -> None [view_source] Resets scene to a known initial state.","title":"RoboThorCachedEnvironment.reset"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentcurrently_reachable_points","text":"| @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable.","title":"RoboThorCachedEnvironment.currently_reachable_points"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentscene_name","text":"| @property | scene_name () -> str [view_source] Current ai2thor scene.","title":"RoboThorCachedEnvironment.scene_name"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentcurrent_frame","text":"| @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view.","title":"RoboThorCachedEnvironment.current_frame"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentcurrent_depth","text":"| @property | current_depth () -> np . ndarray [view_source] Returns depth image corresponding to the agent's egocentric view.","title":"RoboThorCachedEnvironment.current_depth"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentlast_event","text":"| @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller.","title":"RoboThorCachedEnvironment.last_event"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentlast_action","text":"| @property | last_action () -> str [view_source] Last action, as a string, taken by the agent.","title":"RoboThorCachedEnvironment.last_action"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentlast_action_success","text":"| @property | last_action_success () -> bool [view_source] In the cached environment, all actions succeed.","title":"RoboThorCachedEnvironment.last_action_success"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentlast_action_return","text":"| @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" .","title":"RoboThorCachedEnvironment.last_action_return"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentstep","text":"| step ( action_dict : Dict [ str , Union [ str , int , float ]]) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment.","title":"RoboThorCachedEnvironment.step"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentstop","text":"| stop () [view_source] Stops the ai2thor controller.","title":"RoboThorCachedEnvironment.stop"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentall_objects","text":"| all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata.","title":"RoboThorCachedEnvironment.all_objects"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentall_objects_with_properties","text":"| all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties.","title":"RoboThorCachedEnvironment.all_objects_with_properties"},{"location":"api/allenact_plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentvisible_objects","text":"| visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects.","title":"RoboThorCachedEnvironment.visible_objects"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/","text":"allenact_plugins.robothor_plugin.robothor_models # [view_source] ResnetTensorGoalEncoder # class ResnetTensorGoalEncoder ( nn . Module ) [view_source] ResnetTensorGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetTensorObjectNavActorCritic # class ResnetTensorObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] ResnetTensorObjectNavActorCritic.recurrent_hidden_state_size # | @property | recurrent_hidden_state_size () -> Union [ int , Dict [ str , Tuple [ Sequence [ Tuple [ str , Optional [ int ]]], torch . dtype ]]] [view_source] The recurrent hidden state size of the model. ResnetTensorObjectNavActorCritic.is_blind # | @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). ResnetTensorObjectNavActorCritic.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers. ResnetTensorObjectNavActorCritic.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetFasterRCNNTensorsGoalEncoder # class ResnetFasterRCNNTensorsGoalEncoder ( nn . Module ) [view_source] ResnetFasterRCNNTensorsGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetFasterRCNNTensorsObjectNavActorCritic # class ResnetFasterRCNNTensorsObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] ResnetFasterRCNNTensorsObjectNavActorCritic.recurrent_hidden_state_size # | @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model. ResnetFasterRCNNTensorsObjectNavActorCritic.is_blind # | @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). ResnetFasterRCNNTensorsObjectNavActorCritic.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers. ResnetFasterRCNNTensorsObjectNavActorCritic.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"robothor_models"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#allenact_pluginsrobothor_pluginrobothor_models","text":"[view_source]","title":"allenact_plugins.robothor_plugin.robothor_models"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnettensorgoalencoder","text":"class ResnetTensorGoalEncoder ( nn . Module ) [view_source]","title":"ResnetTensorGoalEncoder"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnettensorgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetTensorGoalEncoder.get_object_type_encoding"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcritic","text":"class ResnetTensorObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source]","title":"ResnetTensorObjectNavActorCritic"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcriticrecurrent_hidden_state_size","text":"| @property | recurrent_hidden_state_size () -> Union [ int , Dict [ str , Tuple [ Sequence [ Tuple [ str , Optional [ int ]]], torch . dtype ]]] [view_source] The recurrent hidden state size of the model.","title":"ResnetTensorObjectNavActorCritic.recurrent_hidden_state_size"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcriticis_blind","text":"| @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"ResnetTensorObjectNavActorCritic.is_blind"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcriticnum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers.","title":"ResnetTensorObjectNavActorCritic.num_recurrent_layers"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcriticget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetTensorObjectNavActorCritic.get_object_type_encoding"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsgoalencoder","text":"class ResnetFasterRCNNTensorsGoalEncoder ( nn . Module ) [view_source]","title":"ResnetFasterRCNNTensorsGoalEncoder"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetFasterRCNNTensorsGoalEncoder.get_object_type_encoding"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcritic","text":"class ResnetFasterRCNNTensorsObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source]","title":"ResnetFasterRCNNTensorsObjectNavActorCritic"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcriticrecurrent_hidden_state_size","text":"| @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model.","title":"ResnetFasterRCNNTensorsObjectNavActorCritic.recurrent_hidden_state_size"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcriticis_blind","text":"| @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"ResnetFasterRCNNTensorsObjectNavActorCritic.is_blind"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcriticnum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers.","title":"ResnetFasterRCNNTensorsObjectNavActorCritic.num_recurrent_layers"},{"location":"api/allenact_plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcriticget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetFasterRCNNTensorsObjectNavActorCritic.get_object_type_encoding"},{"location":"api/allenact_plugins/robothor_plugin/robothor_preprocessors/","text":"allenact_plugins.robothor_plugin.robothor_preprocessors # [view_source] FasterRCNNPreProcessorRoboThor # class FasterRCNNPreProcessorRoboThor ( Preprocessor ) [view_source] Preprocess RGB image using a ResNet model.","title":"robothor_preprocessors"},{"location":"api/allenact_plugins/robothor_plugin/robothor_preprocessors/#allenact_pluginsrobothor_pluginrobothor_preprocessors","text":"[view_source]","title":"allenact_plugins.robothor_plugin.robothor_preprocessors"},{"location":"api/allenact_plugins/robothor_plugin/robothor_preprocessors/#fasterrcnnpreprocessorrobothor","text":"class FasterRCNNPreProcessorRoboThor ( Preprocessor ) [view_source] Preprocess RGB image using a ResNet model.","title":"FasterRCNNPreProcessorRoboThor"},{"location":"api/allenact_plugins/robothor_plugin/robothor_sensors/","text":"allenact_plugins.robothor_plugin.robothor_sensors # [view_source] RGBSensorRoboThor # class RGBSensorRoboThor ( RGBSensorThor ) [view_source] Sensor for RGB images in RoboTHOR. Returns from a running RoboThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view. RGBSensorMultiRoboThor # class RGBSensorMultiRoboThor ( RGBSensor [ RoboThorEnvironment , Task [ RoboThorEnvironment ]]) [view_source] Sensor for RGB images in RoboTHOR. Returns from a running RoboThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"robothor_sensors"},{"location":"api/allenact_plugins/robothor_plugin/robothor_sensors/#allenact_pluginsrobothor_pluginrobothor_sensors","text":"[view_source]","title":"allenact_plugins.robothor_plugin.robothor_sensors"},{"location":"api/allenact_plugins/robothor_plugin/robothor_sensors/#rgbsensorrobothor","text":"class RGBSensorRoboThor ( RGBSensorThor ) [view_source] Sensor for RGB images in RoboTHOR. Returns from a running RoboThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"RGBSensorRoboThor"},{"location":"api/allenact_plugins/robothor_plugin/robothor_sensors/#rgbsensormultirobothor","text":"class RGBSensorMultiRoboThor ( RGBSensor [ RoboThorEnvironment , Task [ RoboThorEnvironment ]]) [view_source] Sensor for RGB images in RoboTHOR. Returns from a running RoboThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"RGBSensorMultiRoboThor"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/","text":"allenact_plugins.robothor_plugin.robothor_task_samplers # [view_source] ObjectNavTaskSampler # class ObjectNavTaskSampler ( TaskSampler ) [view_source] ObjectNavTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). ObjectNavTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. ObjectNavDatasetTaskSampler # class ObjectNavDatasetTaskSampler ( TaskSampler ) [view_source] ObjectNavDatasetTaskSampler.__len__ # | @property | __len__ () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). ObjectNavDatasetTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. ObjectNavDatasetTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). PointNavTaskSampler # class PointNavTaskSampler ( TaskSampler ) [view_source] PointNavTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). PointNavTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. PointNavDatasetTaskSampler # class PointNavDatasetTaskSampler ( TaskSampler ) [view_source] PointNavDatasetTaskSampler.__len__ # | @property | __len__ () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). PointNavDatasetTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. PointNavDatasetTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). NavToPartnerTaskSampler # class NavToPartnerTaskSampler ( TaskSampler ) [view_source] NavToPartnerTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). NavToPartnerTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"robothor_task_samplers"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#allenact_pluginsrobothor_pluginrobothor_task_samplers","text":"[view_source]","title":"allenact_plugins.robothor_plugin.robothor_task_samplers"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#objectnavtasksampler","text":"class ObjectNavTaskSampler ( TaskSampler ) [view_source]","title":"ObjectNavTaskSampler"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#objectnavtasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ObjectNavTaskSampler.length"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#objectnavtasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"ObjectNavTaskSampler.all_observation_spaces_equal"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#objectnavdatasettasksampler","text":"class ObjectNavDatasetTaskSampler ( TaskSampler ) [view_source]","title":"ObjectNavDatasetTaskSampler"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#objectnavdatasettasksampler__len__","text":"| @property | __len__ () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ObjectNavDatasetTaskSampler.__len__"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#objectnavdatasettasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"ObjectNavDatasetTaskSampler.all_observation_spaces_equal"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#objectnavdatasettasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ObjectNavDatasetTaskSampler.length"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#pointnavtasksampler","text":"class PointNavTaskSampler ( TaskSampler ) [view_source]","title":"PointNavTaskSampler"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#pointnavtasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"PointNavTaskSampler.length"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#pointnavtasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"PointNavTaskSampler.all_observation_spaces_equal"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#pointnavdatasettasksampler","text":"class PointNavDatasetTaskSampler ( TaskSampler ) [view_source]","title":"PointNavDatasetTaskSampler"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#pointnavdatasettasksampler__len__","text":"| @property | __len__ () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"PointNavDatasetTaskSampler.__len__"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#pointnavdatasettasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"PointNavDatasetTaskSampler.all_observation_spaces_equal"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#pointnavdatasettasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"PointNavDatasetTaskSampler.length"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#navtopartnertasksampler","text":"class NavToPartnerTaskSampler ( TaskSampler ) [view_source]","title":"NavToPartnerTaskSampler"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#navtopartnertasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"NavToPartnerTaskSampler.length"},{"location":"api/allenact_plugins/robothor_plugin/robothor_task_samplers/#navtopartnertasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"NavToPartnerTaskSampler.all_observation_spaces_equal"},{"location":"api/allenact_plugins/robothor_plugin/robothor_tasks/","text":"allenact_plugins.robothor_plugin.robothor_tasks # [view_source] PointNavTask # class PointNavTask ( Task [ RoboThorEnvironment ]) [view_source] PointNavTask.judge # | judge () -> float [view_source] Judge the last event. ObjectNavTask # class ObjectNavTask ( Task [ RoboThorEnvironment ]) [view_source] ObjectNavTask.judge # | judge () -> float [view_source] Judge the last event. NavToPartnerTask # class NavToPartnerTask ( Task [ RoboThorEnvironment ]) [view_source] NavToPartnerTask.judge # | judge () -> float [view_source] Judge the last event.","title":"robothor_tasks"},{"location":"api/allenact_plugins/robothor_plugin/robothor_tasks/#allenact_pluginsrobothor_pluginrobothor_tasks","text":"[view_source]","title":"allenact_plugins.robothor_plugin.robothor_tasks"},{"location":"api/allenact_plugins/robothor_plugin/robothor_tasks/#pointnavtask","text":"class PointNavTask ( Task [ RoboThorEnvironment ]) [view_source]","title":"PointNavTask"},{"location":"api/allenact_plugins/robothor_plugin/robothor_tasks/#pointnavtaskjudge","text":"| judge () -> float [view_source] Judge the last event.","title":"PointNavTask.judge"},{"location":"api/allenact_plugins/robothor_plugin/robothor_tasks/#objectnavtask","text":"class ObjectNavTask ( Task [ RoboThorEnvironment ]) [view_source]","title":"ObjectNavTask"},{"location":"api/allenact_plugins/robothor_plugin/robothor_tasks/#objectnavtaskjudge","text":"| judge () -> float [view_source] Judge the last event.","title":"ObjectNavTask.judge"},{"location":"api/allenact_plugins/robothor_plugin/robothor_tasks/#navtopartnertask","text":"class NavToPartnerTask ( Task [ RoboThorEnvironment ]) [view_source]","title":"NavToPartnerTask"},{"location":"api/allenact_plugins/robothor_plugin/robothor_tasks/#navtopartnertaskjudge","text":"| judge () -> float [view_source] Judge the last event.","title":"NavToPartnerTask.judge"},{"location":"api/allenact_plugins/robothor_plugin/robothor_viz/","text":"allenact_plugins.robothor_plugin.robothor_viz # [view_source]","title":"robothor_viz"},{"location":"api/allenact_plugins/robothor_plugin/robothor_viz/#allenact_pluginsrobothor_pluginrobothor_viz","text":"[view_source]","title":"allenact_plugins.robothor_plugin.robothor_viz"},{"location":"api/allenact_plugins/robothor_plugin/scripts/make_objectnav_debug_dataset/","text":"allenact_plugins.robothor_plugin.scripts.make_objectnav_debug_dataset # [view_source]","title":"make_objectnav_debug_dataset"},{"location":"api/allenact_plugins/robothor_plugin/scripts/make_objectnav_debug_dataset/#allenact_pluginsrobothor_pluginscriptsmake_objectnav_debug_dataset","text":"[view_source]","title":"allenact_plugins.robothor_plugin.scripts.make_objectnav_debug_dataset"},{"location":"api/allenact_plugins/robothor_plugin/scripts/make_pointnav_debug_dataset/","text":"allenact_plugins.robothor_plugin.scripts.make_pointnav_debug_dataset # [view_source]","title":"make_pointnav_debug_dataset"},{"location":"api/allenact_plugins/robothor_plugin/scripts/make_pointnav_debug_dataset/#allenact_pluginsrobothor_pluginscriptsmake_pointnav_debug_dataset","text":"[view_source]","title":"allenact_plugins.robothor_plugin.scripts.make_pointnav_debug_dataset"},{"location":"api/projects/babyai_baselines/experiments/base/","text":"projects.babyai_baselines.experiments.base # [view_source] BaseBabyAIExperimentConfig # class BaseBabyAIExperimentConfig ( ExperimentConfig , ABC ) [view_source] Base experimental config.","title":"base"},{"location":"api/projects/babyai_baselines/experiments/base/#projectsbabyai_baselinesexperimentsbase","text":"[view_source]","title":"projects.babyai_baselines.experiments.base"},{"location":"api/projects/babyai_baselines/experiments/base/#basebabyaiexperimentconfig","text":"class BaseBabyAIExperimentConfig ( ExperimentConfig , ABC ) [view_source] Base experimental config.","title":"BaseBabyAIExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/","text":"projects.babyai_baselines.experiments.go_to_local.a2c # [view_source] A2CBabyAIGoToLocalExperimentConfig # class A2CBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] A2C only.","title":"a2c"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/#projectsbabyai_baselinesexperimentsgo_to_locala2c","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.a2c"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/#a2cbabyaigotolocalexperimentconfig","text":"class A2CBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] A2C only.","title":"A2CBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/","text":"projects.babyai_baselines.experiments.go_to_local.base # [view_source] BaseBabyAIGoToLocalExperimentConfig # class BaseBabyAIGoToLocalExperimentConfig ( BaseBabyAIExperimentConfig , ABC ) [view_source] Base experimental config.","title":"base"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#projectsbabyai_baselinesexperimentsgo_to_localbase","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.base"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#basebabyaigotolocalexperimentconfig","text":"class BaseBabyAIGoToLocalExperimentConfig ( BaseBabyAIExperimentConfig , ABC ) [view_source] Base experimental config.","title":"BaseBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc/","text":"projects.babyai_baselines.experiments.go_to_local.bc # [view_source] PPOBabyAIGoToLocalExperimentConfig # class PPOBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Behavior clone then PPO.","title":"bc"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc/#projectsbabyai_baselinesexperimentsgo_to_localbc","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.bc"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc/#ppobabyaigotolocalexperimentconfig","text":"class PPOBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Behavior clone then PPO.","title":"PPOBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_teacher_forcing/","text":"projects.babyai_baselines.experiments.go_to_local.bc_teacher_forcing # [view_source] BCTeacherForcingBabyAIGoToLocalExperimentConfig # class BCTeacherForcingBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Behavior clone with teacher forcing.","title":"bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_teacher_forcing/#projectsbabyai_baselinesexperimentsgo_to_localbc_teacher_forcing","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_teacher_forcing/#bcteacherforcingbabyaigotolocalexperimentconfig","text":"class BCTeacherForcingBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Behavior clone with teacher forcing.","title":"BCTeacherForcingBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/dagger/","text":"projects.babyai_baselines.experiments.go_to_local.dagger # [view_source] DaggerBabyAIGoToLocalExperimentConfig # class DaggerBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Find goal in lighthouse env using imitation learning. Training with Dagger.","title":"dagger"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/dagger/#projectsbabyai_baselinesexperimentsgo_to_localdagger","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.dagger"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/dagger/#daggerbabyaigotolocalexperimentconfig","text":"class DaggerBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Find goal in lighthouse env using imitation learning. Training with Dagger.","title":"DaggerBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_offpolicy/","text":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_offpolicy # [view_source] DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig # class DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig ( BCOffPolicyBabyAIGoToLocalExperimentConfig ) [view_source] Distributed Off policy imitation.","title":"distributed_bc_offpolicy"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_offpolicy/#projectsbabyai_baselinesexperimentsgo_to_localdistributed_bc_offpolicy","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_offpolicy"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_offpolicy/#distributedbcoffpolicybabyaigotolocalexperimentconfig","text":"class DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig ( BCOffPolicyBabyAIGoToLocalExperimentConfig ) [view_source] Distributed Off policy imitation.","title":"DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_teacher_forcing/","text":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_teacher_forcing # [view_source] DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig # class DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig ( BCTeacherForcingBabyAIGoToLocalExperimentConfig ) [view_source] Distributed behavior clone with teacher forcing.","title":"distributed_bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_teacher_forcing/#projectsbabyai_baselinesexperimentsgo_to_localdistributed_bc_teacher_forcing","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_teacher_forcing/#distributedbcteacherforcingbabyaigotolocalexperimentconfig","text":"class DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig ( BCTeacherForcingBabyAIGoToLocalExperimentConfig ) [view_source] Distributed behavior clone with teacher forcing.","title":"DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/","text":"projects.babyai_baselines.experiments.go_to_local.ppo # [view_source] PPOBabyAIGoToLocalExperimentConfig # class PPOBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] PPO only.","title":"ppo"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/#projectsbabyai_baselinesexperimentsgo_to_localppo","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.ppo"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/#ppobabyaigotolocalexperimentconfig","text":"class PPOBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] PPO only.","title":"PPOBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/a2c/","text":"projects.babyai_baselines.experiments.go_to_obj.a2c # [view_source] A2CBabyAIGoToObjExperimentConfig # class A2CBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] A2C only.","title":"a2c"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/a2c/#projectsbabyai_baselinesexperimentsgo_to_obja2c","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.a2c"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/a2c/#a2cbabyaigotoobjexperimentconfig","text":"class A2CBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] A2C only.","title":"A2CBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/","text":"projects.babyai_baselines.experiments.go_to_obj.base # [view_source] BaseBabyAIGoToObjExperimentConfig # class BaseBabyAIGoToObjExperimentConfig ( BaseBabyAIExperimentConfig , ABC ) [view_source] Base experimental config.","title":"base"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#projectsbabyai_baselinesexperimentsgo_to_objbase","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.base"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#basebabyaigotoobjexperimentconfig","text":"class BaseBabyAIGoToObjExperimentConfig ( BaseBabyAIExperimentConfig , ABC ) [view_source] Base experimental config.","title":"BaseBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc/","text":"projects.babyai_baselines.experiments.go_to_obj.bc # [view_source] PPOBabyAIGoToObjExperimentConfig # class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Behavior clone then PPO.","title":"bc"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc/#projectsbabyai_baselinesexperimentsgo_to_objbc","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.bc"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc/#ppobabyaigotoobjexperimentconfig","text":"class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Behavior clone then PPO.","title":"PPOBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc_teacher_forcing/","text":"projects.babyai_baselines.experiments.go_to_obj.bc_teacher_forcing # [view_source] PPOBabyAIGoToObjExperimentConfig # class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Behavior clone (with teacher forcing) then PPO.","title":"bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc_teacher_forcing/#projectsbabyai_baselinesexperimentsgo_to_objbc_teacher_forcing","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc_teacher_forcing/#ppobabyaigotoobjexperimentconfig","text":"class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Behavior clone (with teacher forcing) then PPO.","title":"PPOBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/dagger/","text":"projects.babyai_baselines.experiments.go_to_obj.dagger # [view_source] DaggerBabyAIGoToObjExperimentConfig # class DaggerBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Find goal in lighthouse env using imitation learning. Training with Dagger.","title":"dagger"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/dagger/#projectsbabyai_baselinesexperimentsgo_to_objdagger","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.dagger"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/dagger/#daggerbabyaigotoobjexperimentconfig","text":"class DaggerBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Find goal in lighthouse env using imitation learning. Training with Dagger.","title":"DaggerBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/ppo/","text":"projects.babyai_baselines.experiments.go_to_obj.ppo # [view_source] PPOBabyAIGoToObjExperimentConfig # class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] PPO only.","title":"ppo"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/ppo/#projectsbabyai_baselinesexperimentsgo_to_objppo","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.ppo"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/ppo/#ppobabyaigotoobjexperimentconfig","text":"class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] PPO only.","title":"PPOBabyAIGoToObjExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_base/","text":"projects.objectnav_baselines.experiments.objectnav_base # [view_source] ObjectNavBaseConfig # class ObjectNavBaseConfig ( ExperimentConfig , ABC ) [view_source] The base object navigation configuration file.","title":"objectnav_base"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_base/#projectsobjectnav_baselinesexperimentsobjectnav_base","text":"[view_source]","title":"projects.objectnav_baselines.experiments.objectnav_base"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_base/#objectnavbaseconfig","text":"class ObjectNavBaseConfig ( ExperimentConfig , ABC ) [view_source] The base object navigation configuration file.","title":"ObjectNavBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_mixin_dagger/","text":"projects.objectnav_baselines.experiments.objectnav_mixin_dagger # [view_source] ObjectNavMixInDAggerConfig # class ObjectNavMixInDAggerConfig ( ObjectNavBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"objectnav_mixin_dagger"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_mixin_dagger/#projectsobjectnav_baselinesexperimentsobjectnav_mixin_dagger","text":"[view_source]","title":"projects.objectnav_baselines.experiments.objectnav_mixin_dagger"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_mixin_dagger/#objectnavmixindaggerconfig","text":"class ObjectNavMixInDAggerConfig ( ObjectNavBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"ObjectNavMixInDAggerConfig"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_mixin_ddppo/","text":"projects.objectnav_baselines.experiments.objectnav_mixin_ddppo # [view_source]","title":"objectnav_mixin_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_mixin_ddppo/#projectsobjectnav_baselinesexperimentsobjectnav_mixin_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.objectnav_mixin_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_mixin_resnetgru/","text":"projects.objectnav_baselines.experiments.objectnav_mixin_resnetgru # [view_source]","title":"objectnav_mixin_resnetgru"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_mixin_resnetgru/#projectsobjectnav_baselinesexperimentsobjectnav_mixin_resnetgru","text":"[view_source]","title":"projects.objectnav_baselines.experiments.objectnav_mixin_resnetgru"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_thor_base/","text":"projects.objectnav_baselines.experiments.objectnav_thor_base # [view_source] ObjectNavThorBaseConfig # class ObjectNavThorBaseConfig ( ObjectNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"objectnav_thor_base"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_thor_base/#projectsobjectnav_baselinesexperimentsobjectnav_thor_base","text":"[view_source]","title":"projects.objectnav_baselines.experiments.objectnav_thor_base"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_thor_base/#objectnavthorbaseconfig","text":"class ObjectNavThorBaseConfig ( ObjectNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"ObjectNavThorBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_thor_mixin_ddppo_and_gbc/","text":"projects.objectnav_baselines.experiments.objectnav_thor_mixin_ddppo_and_gbc # [view_source]","title":"objectnav_thor_mixin_ddppo_and_gbc"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_thor_mixin_ddppo_and_gbc/#projectsobjectnav_baselinesexperimentsobjectnav_thor_mixin_ddppo_and_gbc","text":"[view_source]","title":"projects.objectnav_baselines.experiments.objectnav_thor_mixin_ddppo_and_gbc"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_base/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_base # [view_source] ObjectNaviThorBaseConfig # class ObjectNaviThorBaseConfig ( ObjectNavThorBaseConfig , ABC ) [view_source] The base config for all iTHOR ObjectNav experiments.","title":"objectnav_ithor_base"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_base/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_base","text":"[view_source]","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_base"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_base/#objectnavithorbaseconfig","text":"class ObjectNaviThorBaseConfig ( ObjectNavThorBaseConfig , ABC ) [view_source] The base config for all iTHOR ObjectNav experiments.","title":"ObjectNaviThorBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_depth_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_depth_resnetgru_ddppo # [view_source] ObjectNaviThorRGBPPOExperimentConfig # class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in iThor with Depth input.","title":"objectnav_ithor_depth_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_depth_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_depth_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_depth_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_depth_resnetgru_ddppo/#objectnavithorrgbppoexperimentconfig","text":"class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in iThor with Depth input.","title":"ObjectNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgb_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgb_resnetgru_ddppo # [view_source] ObjectNaviThorRGBPPOExperimentConfig # class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in iThor with RGB input.","title":"objectnav_ithor_rgb_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgb_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_rgb_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgb_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgb_resnetgru_ddppo/#objectnavithorrgbppoexperimentconfig","text":"class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in iThor with RGB input.","title":"ObjectNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgbd_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgbd_resnetgru_ddppo # [view_source] ObjectNaviThorRGBPPOExperimentConfig # class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in iThor with RGBD input.","title":"objectnav_ithor_rgbd_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgbd_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_rgbd_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgbd_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgbd_resnetgru_ddppo/#objectnavithorrgbppoexperimentconfig","text":"class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in iThor with RGBD input.","title":"ObjectNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_base # [view_source] ObjectNavRoboThorBaseConfig # class ObjectNavRoboThorBaseConfig ( ObjectNavThorBaseConfig , ABC ) [view_source] The base config for all RoboTHOR ObjectNav experiments.","title":"objectnav_robothor_base"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_base","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_base"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base/#objectnavrobothorbaseconfig","text":"class ObjectNavRoboThorBaseConfig ( ObjectNavThorBaseConfig , ABC ) [view_source] The base config for all RoboTHOR ObjectNav experiments.","title":"ObjectNavRoboThorBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_depth_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_depth_resnetgru_ddppo # [view_source] ObjectNavRoboThorRGBPPOExperimentConfig # class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with Depth input.","title":"objectnav_robothor_depth_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_depth_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_depth_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_depth_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_depth_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with Depth input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_dagger/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_dagger # [view_source] ObjectNaviThorRGBDAggerExperimentConfig # class ObjectNaviThorRGBDAggerExperimentConfig ( ObjectNavRoboThorBaseConfig , ObjectNavMixInDAggerConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"objectnav_robothor_rgb_resnetgru_dagger"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_dagger/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_dagger","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_dagger"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_dagger/#objectnavithorrgbdaggerexperimentconfig","text":"class ObjectNaviThorRGBDAggerExperimentConfig ( ObjectNavRoboThorBaseConfig , ObjectNavMixInDAggerConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"ObjectNaviThorRGBDAggerExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo # [view_source] ObjectNavRoboThorRGBPPOExperimentConfig # class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"objectnav_robothor_rgb_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_gbc/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo_and_gbc # [view_source] ObjectNaviThorRGBPPOExperimentConfig # class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig , ObjectNavThorMixInPPOAndGBCConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"objectnav_robothor_rgb_resnetgru_ddppo_and_gbc"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_gbc/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_ddppo_and_gbc","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo_and_gbc"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_gbc/#objectnavithorrgbppoexperimentconfig","text":"class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig , ObjectNavThorMixInPPOAndGBCConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"ObjectNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgbd_resnetgru_ddppo # [view_source] ObjectNavRoboThorRGBPPOExperimentConfig # class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGBD input.","title":"objectnav_robothor_rgbd_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgbd_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgbd_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig , ObjectNavMixInPPOConfig , ObjectNavMixInResNetGRUConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGBD input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/","text":"projects.objectnav_baselines.models.object_nav_models # [view_source] Baseline models for use in the object navigation task. Object navigation is currently available as a Task in AI2-THOR and Facebook's Habitat. ObjectNavBaselineActorCritic # class ObjectNavBaselineActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] Baseline recurrent actor critic model for object-navigation. Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type gym.spaces.Discrete ). observation_space : The observation space expected by the agent. This observation space should include (optionally) 'rgb' images and 'depth' images and is required to have a component corresponding to the goal goal_sensor_uuid . goal_sensor_uuid : The uuid of the sensor of the goal object. See GoalObjectTypeThorSensor as an example of such a sensor. hidden_size : The hidden size of the GRU RNN. object_type_embedding_dim : The dimensionality of the embedding corresponding to the goal object type. ObjectNavBaselineActorCritic.__init__ # | __init__ ( action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , rgb_uuid : Optional [ str ], depth_uuid : Optional [ str ], hidden_size = 512 , object_type_embedding_dim = 8 , trainable_masked_hidden_state : bool = False , num_rnn_layers = 1 , rnn_type = \"GRU\" ) [view_source] Initializer. See class documentation for parameter definitions. ObjectNavBaselineActorCritic.recurrent_hidden_state_size # | @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model. ObjectNavBaselineActorCritic.is_blind # | @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). ObjectNavBaselineActorCritic.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers. ObjectNavBaselineActorCritic.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ObjectNavBaselineActorCritic.forward # | forward ( observations : ObservationType , memory : Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]] [view_source] Processes input batched observations to produce new actor and critic values. Processes input batched observations (along with prior hidden states, previous actions, and masks denoting which recurrent hidden states should be masked) and returns an ActorCriticOutput object containing the model's policy (distribution over actions) and evaluation of the current state (value). Parameters observations : Batched input observations. memory : Memory containing the hidden states from initial timepoints. prev_actions : Tensor of previous actions taken. masks : Masks applied to hidden states. See RNNStateEncoder . Returns Tuple of the ActorCriticOutput and recurrent hidden state. ResnetTensorObjectNavActorCritic # class ResnetTensorObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] ResnetTensorObjectNavActorCritic.recurrent_hidden_state_size # | @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model. ResnetTensorObjectNavActorCritic.is_blind # | @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). ResnetTensorObjectNavActorCritic.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers. ResnetTensorObjectNavActorCritic.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetTensorGoalEncoder # class ResnetTensorGoalEncoder ( nn . Module ) [view_source] ResnetTensorGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetDualTensorGoalEncoder # class ResnetDualTensorGoalEncoder ( nn . Module ) [view_source] ResnetDualTensorGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"object_nav_models"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#projectsobjectnav_baselinesmodelsobject_nav_models","text":"[view_source] Baseline models for use in the object navigation task. Object navigation is currently available as a Task in AI2-THOR and Facebook's Habitat.","title":"projects.objectnav_baselines.models.object_nav_models"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcritic","text":"class ObjectNavBaselineActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] Baseline recurrent actor critic model for object-navigation. Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type gym.spaces.Discrete ). observation_space : The observation space expected by the agent. This observation space should include (optionally) 'rgb' images and 'depth' images and is required to have a component corresponding to the goal goal_sensor_uuid . goal_sensor_uuid : The uuid of the sensor of the goal object. See GoalObjectTypeThorSensor as an example of such a sensor. hidden_size : The hidden size of the GRU RNN. object_type_embedding_dim : The dimensionality of the embedding corresponding to the goal object type.","title":"ObjectNavBaselineActorCritic"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcritic__init__","text":"| __init__ ( action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , rgb_uuid : Optional [ str ], depth_uuid : Optional [ str ], hidden_size = 512 , object_type_embedding_dim = 8 , trainable_masked_hidden_state : bool = False , num_rnn_layers = 1 , rnn_type = \"GRU\" ) [view_source] Initializer. See class documentation for parameter definitions.","title":"ObjectNavBaselineActorCritic.__init__"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcriticrecurrent_hidden_state_size","text":"| @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model.","title":"ObjectNavBaselineActorCritic.recurrent_hidden_state_size"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcriticis_blind","text":"| @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"ObjectNavBaselineActorCritic.is_blind"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcriticnum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers.","title":"ObjectNavBaselineActorCritic.num_recurrent_layers"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcriticget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ObjectNavBaselineActorCritic.get_object_type_encoding"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcriticforward","text":"| forward ( observations : ObservationType , memory : Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]] [view_source] Processes input batched observations to produce new actor and critic values. Processes input batched observations (along with prior hidden states, previous actions, and masks denoting which recurrent hidden states should be masked) and returns an ActorCriticOutput object containing the model's policy (distribution over actions) and evaluation of the current state (value). Parameters observations : Batched input observations. memory : Memory containing the hidden states from initial timepoints. prev_actions : Tensor of previous actions taken. masks : Masks applied to hidden states. See RNNStateEncoder . Returns Tuple of the ActorCriticOutput and recurrent hidden state.","title":"ObjectNavBaselineActorCritic.forward"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcritic","text":"class ResnetTensorObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source]","title":"ResnetTensorObjectNavActorCritic"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcriticrecurrent_hidden_state_size","text":"| @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model.","title":"ResnetTensorObjectNavActorCritic.recurrent_hidden_state_size"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcriticis_blind","text":"| @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"ResnetTensorObjectNavActorCritic.is_blind"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcriticnum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers.","title":"ResnetTensorObjectNavActorCritic.num_recurrent_layers"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcriticget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetTensorObjectNavActorCritic.get_object_type_encoding"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorgoalencoder","text":"class ResnetTensorGoalEncoder ( nn . Module ) [view_source]","title":"ResnetTensorGoalEncoder"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetTensorGoalEncoder.get_object_type_encoding"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnetdualtensorgoalencoder","text":"class ResnetDualTensorGoalEncoder ( nn . Module ) [view_source]","title":"ResnetDualTensorGoalEncoder"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnetdualtensorgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetDualTensorGoalEncoder.get_object_type_encoding"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_base/","text":"projects.pointnav_baselines.experiments.pointnav_base # [view_source] PointNavBaseConfig # class PointNavBaseConfig ( ExperimentConfig , ABC ) [view_source] An Object Navigation experiment configuration in iThor.","title":"pointnav_base"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_base/#projectspointnav_baselinesexperimentspointnav_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.pointnav_base"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_base/#pointnavbaseconfig","text":"class PointNavBaseConfig ( ExperimentConfig , ABC ) [view_source] An Object Navigation experiment configuration in iThor.","title":"PointNavBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_habitat_mixin_ddppo/","text":"projects.pointnav_baselines.experiments.pointnav_habitat_mixin_ddppo # [view_source] PointNavHabitatMixInPPOConfig # class PointNavHabitatMixInPPOConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PPO PointNav experiments.","title":"pointnav_habitat_mixin_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_habitat_mixin_ddppo/#projectspointnav_baselinesexperimentspointnav_habitat_mixin_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.pointnav_habitat_mixin_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_habitat_mixin_ddppo/#pointnavhabitatmixinppoconfig","text":"class PointNavHabitatMixInPPOConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PPO PointNav experiments.","title":"PointNavHabitatMixInPPOConfig"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_mixin_simpleconvgru/","text":"projects.pointnav_baselines.experiments.pointnav_mixin_simpleconvgru # [view_source] PointNavMixInSimpleConvGRUConfig # class PointNavMixInSimpleConvGRUConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PPO PointNav experiments.","title":"pointnav_mixin_simpleconvgru"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_mixin_simpleconvgru/#projectspointnav_baselinesexperimentspointnav_mixin_simpleconvgru","text":"[view_source]","title":"projects.pointnav_baselines.experiments.pointnav_mixin_simpleconvgru"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_mixin_simpleconvgru/#pointnavmixinsimpleconvgruconfig","text":"class PointNavMixInSimpleConvGRUConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PPO PointNav experiments.","title":"PointNavMixInSimpleConvGRUConfig"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_thor_base/","text":"projects.pointnav_baselines.experiments.pointnav_thor_base # [view_source] PointNavThorBaseConfig # class PointNavThorBaseConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"pointnav_thor_base"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_thor_base/#projectspointnav_baselinesexperimentspointnav_thor_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.pointnav_thor_base"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_thor_base/#pointnavthorbaseconfig","text":"class PointNavThorBaseConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"PointNavThorBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_thor_mixin_ddppo/","text":"projects.pointnav_baselines.experiments.pointnav_thor_mixin_ddppo # [view_source] PointNavThorMixInPPOConfig # class PointNavThorMixInPPOConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PPO PointNav experiments.","title":"pointnav_thor_mixin_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_thor_mixin_ddppo/#projectspointnav_baselinesexperimentspointnav_thor_mixin_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.pointnav_thor_mixin_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_thor_mixin_ddppo/#pointnavthormixinppoconfig","text":"class PointNavThorMixInPPOConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PPO PointNav experiments.","title":"PointNavThorMixInPPOConfig"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_thor_mixin_ddppo_and_gbc/","text":"projects.pointnav_baselines.experiments.pointnav_thor_mixin_ddppo_and_gbc # [view_source] PointNavThorMixInPPOAndGBCConfig # class PointNavThorMixInPPOAndGBCConfig ( PointNavThorBaseConfig , ABC ) [view_source] The base config for all iTHOR PPO PointNav experiments.","title":"pointnav_thor_mixin_ddppo_and_gbc"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_thor_mixin_ddppo_and_gbc/#projectspointnav_baselinesexperimentspointnav_thor_mixin_ddppo_and_gbc","text":"[view_source]","title":"projects.pointnav_baselines.experiments.pointnav_thor_mixin_ddppo_and_gbc"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_thor_mixin_ddppo_and_gbc/#pointnavthormixinppoandgbcconfig","text":"class PointNavThorMixInPPOAndGBCConfig ( PointNavThorBaseConfig , ABC ) [view_source] The base config for all iTHOR PPO PointNav experiments.","title":"PointNavThorMixInPPOAndGBCConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_base/","text":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_base # [view_source] DebugPointNavHabitatBaseConfig # class DebugPointNavHabitatBaseConfig ( PointNavHabitatBaseConfig , ABC ) [view_source] The base config for all Habitat PointNav experiments.","title":"debug_pointnav_habitat_base"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_base/#projectspointnav_baselinesexperimentshabitatdebug_pointnav_habitat_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_base"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_base/#debugpointnavhabitatbaseconfig","text":"class DebugPointNavHabitatBaseConfig ( PointNavHabitatBaseConfig , ABC ) [view_source] The base config for all Habitat PointNav experiments.","title":"DebugPointNavHabitatBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_bc/","text":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgb_simpleconvgru_bc # [view_source] PointNavHabitatRGBDeterministiSimpleConvGRUImitationExperimentConfig # class PointNavHabitatRGBDeterministiSimpleConvGRUImitationExperimentConfig ( DebugPointNavHabitatBaseConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"debug_pointnav_habitat_rgb_simpleconvgru_bc"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_bc/#projectspointnav_baselinesexperimentshabitatdebug_pointnav_habitat_rgb_simpleconvgru_bc","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgb_simpleconvgru_bc"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_bc/#pointnavhabitatrgbdeterministisimpleconvgruimitationexperimentconfig","text":"class PointNavHabitatRGBDeterministiSimpleConvGRUImitationExperimentConfig ( DebugPointNavHabitatBaseConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"PointNavHabitatRGBDeterministiSimpleConvGRUImitationExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgb_simpleconvgru_ddppo # [view_source] PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig # class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( DebugPointNavHabitatBaseConfig , PointNavHabitatMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"debug_pointnav_habitat_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentshabitatdebug_pointnav_habitat_rgb_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_ddppo/#pointnavhabitatdepthdeterministisimpleconvgruddppoexperimentconfig","text":"class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( DebugPointNavHabitatBaseConfig , PointNavHabitatMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgbd_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgbd_simpleconvgru_ddppo # [view_source] PointNavHabitatRGBDDeterministiSimpleConvGRUDDPPOExperimentConfig # class PointNavHabitatRGBDDeterministiSimpleConvGRUDDPPOExperimentConfig ( DebugPointNavHabitatBaseConfig , PointNavHabitatMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"debug_pointnav_habitat_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgbd_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentshabitatdebug_pointnav_habitat_rgbd_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgbd_simpleconvgru_ddppo/#pointnavhabitatrgbddeterministisimpleconvgruddppoexperimentconfig","text":"class PointNavHabitatRGBDDeterministiSimpleConvGRUDDPPOExperimentConfig ( DebugPointNavHabitatBaseConfig , PointNavHabitatMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"PointNavHabitatRGBDDeterministiSimpleConvGRUDDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_base/","text":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_base # [view_source] PointNavHabitatBaseConfig # class PointNavHabitatBaseConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all Habitat PointNav experiments.","title":"pointnav_habitat_base"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_base/#projectspointnav_baselinesexperimentshabitatpointnav_habitat_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_base"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_base/#pointnavhabitatbaseconfig","text":"class PointNavHabitatBaseConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all Habitat PointNav experiments.","title":"PointNavHabitatBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_depth_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_depth_simpleconvgru_ddppo # [view_source] PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig # class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig , PointNavHabitatMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"pointnav_habitat_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_depth_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentshabitatpointnav_habitat_depth_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_depth_simpleconvgru_ddppo/#pointnavhabitatdepthdeterministisimpleconvgruddppoexperimentconfig","text":"class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig , PointNavHabitatMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgb_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_rgb_simpleconvgru_ddppo # [view_source] PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig # class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig , PointNavHabitatMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"pointnav_habitat_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgb_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentshabitatpointnav_habitat_rgb_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgb_simpleconvgru_ddppo/#pointnavhabitatdepthdeterministisimpleconvgruddppoexperimentconfig","text":"class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig , PointNavHabitatMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgbd_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_rgbd_simpleconvgru_ddppo # [view_source] PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig # class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig , PointNavHabitatMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with RGBD input.","title":"pointnav_habitat_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgbd_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentshabitatpointnav_habitat_rgbd_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgbd_simpleconvgru_ddppo/#pointnavhabitatdepthdeterministisimpleconvgruddppoexperimentconfig","text":"class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig , PointNavHabitatMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in Habitat with RGBD input.","title":"PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_base/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_base # [view_source] PointNaviThorBaseConfig # class PointNaviThorBaseConfig ( PointNavThorBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"pointnav_ithor_base"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_base/#projectspointnav_baselinesexperimentsithorpointnav_ithor_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_base"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_base/#pointnavithorbaseconfig","text":"class PointNaviThorBaseConfig ( PointNavThorBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"PointNaviThorBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_depth_simpleconvgru_ddppo # [view_source] PointNaviThorDepthPPOExperimentConfig # class PointNaviThorDepthPPOExperimentConfig ( PointNaviThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in iThor with Depth input.","title":"pointnav_ithor_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsithorpointnav_ithor_depth_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo/#pointnavithordepthppoexperimentconfig","text":"class PointNaviThorDepthPPOExperimentConfig ( PointNaviThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in iThor with Depth input.","title":"PointNaviThorDepthPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo_and_gbc/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_depth_simpleconvgru_ddppo_and_gbc # [view_source] PointNaviThorDepthPPOExperimentConfig # class PointNaviThorDepthPPOExperimentConfig ( PointNaviThorBaseConfig , PointNavThorMixInPPOAndGBCConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in iThor with Depth input.","title":"pointnav_ithor_depth_simpleconvgru_ddppo_and_gbc"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo_and_gbc/#projectspointnav_baselinesexperimentsithorpointnav_ithor_depth_simpleconvgru_ddppo_and_gbc","text":"[view_source]","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_depth_simpleconvgru_ddppo_and_gbc"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo_and_gbc/#pointnavithordepthppoexperimentconfig","text":"class PointNaviThorDepthPPOExperimentConfig ( PointNaviThorBaseConfig , PointNavThorMixInPPOAndGBCConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in iThor with Depth input.","title":"PointNaviThorDepthPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgb_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgb_simpleconvgru_ddppo # [view_source] PointNaviThorRGBPPOExperimentConfig # class PointNaviThorRGBPPOExperimentConfig ( PointNaviThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in iThor with RGB input.","title":"pointnav_ithor_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgb_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsithorpointnav_ithor_rgb_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgb_simpleconvgru_ddppo/#pointnavithorrgbppoexperimentconfig","text":"class PointNaviThorRGBPPOExperimentConfig ( PointNaviThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in iThor with RGB input.","title":"PointNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgbd_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgbd_simpleconvgru_ddppo # [view_source] PointNaviThorRGBDPPOExperimentConfig # class PointNaviThorRGBDPPOExperimentConfig ( PointNaviThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in iThor with RGBD input.","title":"pointnav_ithor_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgbd_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsithorpointnav_ithor_rgbd_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgbd_simpleconvgru_ddppo/#pointnavithorrgbdppoexperimentconfig","text":"class PointNaviThorRGBDPPOExperimentConfig ( PointNaviThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in iThor with RGBD input.","title":"PointNaviThorRGBDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_base/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_base # [view_source] PointNavRoboThorBaseConfig # class PointNavRoboThorBaseConfig ( PointNavThorBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"pointnav_robothor_base"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_base/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_base"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_base/#pointnavrobothorbaseconfig","text":"class PointNavRoboThorBaseConfig ( PointNavThorBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"PointNavRoboThorBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_depth_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_depth_simpleconvgru_ddppo # [view_source] PointNavRoboThorRGBPPOExperimentConfig # class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in RoboTHOR with Depth input.","title":"pointnav_robothor_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_depth_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_depth_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_depth_simpleconvgru_ddppo/#pointnavrobothorrgbppoexperimentconfig","text":"class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in RoboTHOR with Depth input.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgb_simpleconvgru_ddppo # [view_source] PointNavRoboThorRGBPPOExperimentConfig # class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in RoboThor with RGB input.","title":"pointnav_robothor_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_rgb_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo/#pointnavrobothorrgbppoexperimentconfig","text":"class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in RoboThor with RGB input.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo_and_gbc/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgb_simpleconvgru_ddppo_and_gbc # [view_source] PointNavRoboThorRGBPPOExperimentConfig # class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig , PointNavThorMixInPPOAndGBCConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An PointNavigation experiment configuration in RoboThor with RGB input.","title":"pointnav_robothor_rgb_simpleconvgru_ddppo_and_gbc"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo_and_gbc/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_rgb_simpleconvgru_ddppo_and_gbc","text":"[view_source]","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgb_simpleconvgru_ddppo_and_gbc"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo_and_gbc/#pointnavrobothorrgbppoexperimentconfig","text":"class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig , PointNavThorMixInPPOAndGBCConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An PointNavigation experiment configuration in RoboThor with RGB input.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgbd_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgbd_simpleconvgru_ddppo # [view_source] PointNavRoboThorRGBPPOExperimentConfig # class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in RoboThor with RGBD input.","title":"pointnav_robothor_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgbd_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_rgbd_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgbd_simpleconvgru_ddppo/#pointnavrobothorrgbppoexperimentconfig","text":"class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig , PointNavThorMixInPPOConfig , PointNavMixInSimpleConvGRUConfig ) [view_source] An Point Navigation experiment configuration in RoboThor with RGBD input.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/","text":"projects.pointnav_baselines.models.point_nav_models # [view_source] ResnetTensorPointNavActorCritic # class ResnetTensorPointNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] ResnetTensorPointNavActorCritic.recurrent_hidden_state_size # | @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model. ResnetTensorPointNavActorCritic.is_blind # | @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). ResnetTensorPointNavActorCritic.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers. ResnetTensorGoalEncoder # class ResnetTensorGoalEncoder ( nn . Module ) [view_source] ResnetTensorGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetDualTensorGoalEncoder # class ResnetDualTensorGoalEncoder ( nn . Module ) [view_source] ResnetDualTensorGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"point_nav_models"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#projectspointnav_baselinesmodelspoint_nav_models","text":"[view_source]","title":"projects.pointnav_baselines.models.point_nav_models"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorpointnavactorcritic","text":"class ResnetTensorPointNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source]","title":"ResnetTensorPointNavActorCritic"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorpointnavactorcriticrecurrent_hidden_state_size","text":"| @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model.","title":"ResnetTensorPointNavActorCritic.recurrent_hidden_state_size"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorpointnavactorcriticis_blind","text":"| @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"ResnetTensorPointNavActorCritic.is_blind"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorpointnavactorcriticnum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers.","title":"ResnetTensorPointNavActorCritic.num_recurrent_layers"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorgoalencoder","text":"class ResnetTensorGoalEncoder ( nn . Module ) [view_source]","title":"ResnetTensorGoalEncoder"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetTensorGoalEncoder.get_object_type_encoding"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnetdualtensorgoalencoder","text":"class ResnetDualTensorGoalEncoder ( nn . Module ) [view_source]","title":"ResnetDualTensorGoalEncoder"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnetdualtensorgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetDualTensorGoalEncoder.get_object_type_encoding"},{"location":"api/projects/tutorials/minigrid_tutorial_conds/","text":"projects.tutorials.minigrid_tutorial_conds # [view_source]","title":"minigrid_tutorial_conds"},{"location":"api/projects/tutorials/minigrid_tutorial_conds/#projectstutorialsminigrid_tutorial_conds","text":"[view_source]","title":"projects.tutorials.minigrid_tutorial_conds"},{"location":"api/projects/tutorials/navtopartner_robothor_rgb_ppo/","text":"projects.tutorials.navtopartner_robothor_rgb_ppo # [view_source] NavToPartnerRoboThorRGBPPOExperimentConfig # class NavToPartnerRoboThorRGBPPOExperimentConfig ( ExperimentConfig ) [view_source] A Multi-Agent Navigation experiment configuration in RoboThor.","title":"navtopartner_robothor_rgb_ppo"},{"location":"api/projects/tutorials/navtopartner_robothor_rgb_ppo/#projectstutorialsnavtopartner_robothor_rgb_ppo","text":"[view_source]","title":"projects.tutorials.navtopartner_robothor_rgb_ppo"},{"location":"api/projects/tutorials/navtopartner_robothor_rgb_ppo/#navtopartnerrobothorrgbppoexperimentconfig","text":"class NavToPartnerRoboThorRGBPPOExperimentConfig ( ExperimentConfig ) [view_source] A Multi-Agent Navigation experiment configuration in RoboThor.","title":"NavToPartnerRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/tutorials/object_nav_ithor_dagger_then_ppo_one_object/","text":"projects.tutorials.object_nav_ithor_dagger_then_ppo_one_object # [view_source] ObjectNavThorDaggerThenPPOExperimentConfig # class ObjectNavThorDaggerThenPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ) [view_source] A simple object navigation experiment in THOR. Training with DAgger and then PPO.","title":"object_nav_ithor_dagger_then_ppo_one_object"},{"location":"api/projects/tutorials/object_nav_ithor_dagger_then_ppo_one_object/#projectstutorialsobject_nav_ithor_dagger_then_ppo_one_object","text":"[view_source]","title":"projects.tutorials.object_nav_ithor_dagger_then_ppo_one_object"},{"location":"api/projects/tutorials/object_nav_ithor_dagger_then_ppo_one_object/#objectnavthordaggerthenppoexperimentconfig","text":"class ObjectNavThorDaggerThenPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ) [view_source] A simple object navigation experiment in THOR. Training with DAgger and then PPO.","title":"ObjectNavThorDaggerThenPPOExperimentConfig"},{"location":"api/projects/tutorials/object_nav_ithor_dagger_then_ppo_one_object_viz/","text":"projects.tutorials.object_nav_ithor_dagger_then_ppo_one_object_viz # [view_source] ObjectNavThorDaggerThenPPOVizExperimentConfig # class ObjectNavThorDaggerThenPPOVizExperimentConfig ( ObjectNavThorDaggerThenPPOExperimentConfig ) [view_source] A simple object navigation experiment in THOR. Training with DAgger and then PPO + using viz for test.","title":"object_nav_ithor_dagger_then_ppo_one_object_viz"},{"location":"api/projects/tutorials/object_nav_ithor_dagger_then_ppo_one_object_viz/#projectstutorialsobject_nav_ithor_dagger_then_ppo_one_object_viz","text":"[view_source]","title":"projects.tutorials.object_nav_ithor_dagger_then_ppo_one_object_viz"},{"location":"api/projects/tutorials/object_nav_ithor_dagger_then_ppo_one_object_viz/#objectnavthordaggerthenppovizexperimentconfig","text":"class ObjectNavThorDaggerThenPPOVizExperimentConfig ( ObjectNavThorDaggerThenPPOExperimentConfig ) [view_source] A simple object navigation experiment in THOR. Training with DAgger and then PPO + using viz for test.","title":"ObjectNavThorDaggerThenPPOVizExperimentConfig"},{"location":"api/projects/tutorials/object_nav_ithor_ppo_one_object/","text":"projects.tutorials.object_nav_ithor_ppo_one_object # [view_source] ObjectNavThorPPOExperimentConfig # class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ) [view_source] A simple object navigation experiment in THOR. Training with PPO.","title":"object_nav_ithor_ppo_one_object"},{"location":"api/projects/tutorials/object_nav_ithor_ppo_one_object/#projectstutorialsobject_nav_ithor_ppo_one_object","text":"[view_source]","title":"projects.tutorials.object_nav_ithor_ppo_one_object"},{"location":"api/projects/tutorials/object_nav_ithor_ppo_one_object/#objectnavthorppoexperimentconfig","text":"class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ) [view_source] A simple object navigation experiment in THOR. Training with PPO.","title":"ObjectNavThorPPOExperimentConfig"},{"location":"api/projects/tutorials/pointnav_habitat_rgb_ddppo/","text":"projects.tutorials.pointnav_habitat_rgb_ddppo # [view_source] PointNavHabitatRGBPPOTutorialExperimentConfig # class PointNavHabitatRGBPPOTutorialExperimentConfig ( ExperimentConfig ) [view_source] A Point Navigation experiment configuration in Habitat.","title":"pointnav_habitat_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_habitat_rgb_ddppo/#projectstutorialspointnav_habitat_rgb_ddppo","text":"[view_source]","title":"projects.tutorials.pointnav_habitat_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_habitat_rgb_ddppo/#pointnavhabitatrgbppotutorialexperimentconfig","text":"class PointNavHabitatRGBPPOTutorialExperimentConfig ( ExperimentConfig ) [view_source] A Point Navigation experiment configuration in Habitat.","title":"PointNavHabitatRGBPPOTutorialExperimentConfig"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/","text":"projects.tutorials.pointnav_ithor_rgb_ddppo # [view_source] PointNaviThorRGBPPOExperimentConfig # class PointNaviThorRGBPPOExperimentConfig ( ExperimentConfig ) [view_source] A Point Navigation experiment configuration in iTHOR.","title":"pointnav_ithor_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#projectstutorialspointnav_ithor_rgb_ddppo","text":"[view_source]","title":"projects.tutorials.pointnav_ithor_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#pointnavithorrgbppoexperimentconfig","text":"class PointNaviThorRGBPPOExperimentConfig ( ExperimentConfig ) [view_source] A Point Navigation experiment configuration in iTHOR.","title":"PointNaviThorRGBPPOExperimentConfig"},{"location":"api/tests/hierarchical_policies/test_minigrid_conditional/","text":"tests.hierarchical_policies.test_minigrid_conditional # [view_source]","title":"test_minigrid_conditional"},{"location":"api/tests/hierarchical_policies/test_minigrid_conditional/#testshierarchical_policiestest_minigrid_conditional","text":"[view_source]","title":"tests.hierarchical_policies.test_minigrid_conditional"},{"location":"api/tests/mapping/test_ai2thor_mapping/","text":"tests.mapping.test_ai2thor_mapping # [view_source]","title":"test_ai2thor_mapping"},{"location":"api/tests/mapping/test_ai2thor_mapping/#testsmappingtest_ai2thor_mapping","text":"[view_source]","title":"tests.mapping.test_ai2thor_mapping"},{"location":"api/tests/multiprocessing/test_frozen_attribs/","text":"tests.multiprocessing.test_frozen_attribs # [view_source]","title":"test_frozen_attribs"},{"location":"api/tests/multiprocessing/test_frozen_attribs/#testsmultiprocessingtest_frozen_attribs","text":"[view_source]","title":"tests.multiprocessing.test_frozen_attribs"},{"location":"api/tests/sync_algs_cpu/test_to_to_obj_trains/","text":"tests.sync_algs_cpu.test_to_to_obj_trains # [view_source]","title":"test_to_to_obj_trains"},{"location":"api/tests/sync_algs_cpu/test_to_to_obj_trains/#testssync_algs_cputest_to_to_obj_trains","text":"[view_source]","title":"tests.sync_algs_cpu.test_to_to_obj_trains"},{"location":"api/tests/utils/test_spaces/","text":"tests.utils.test_spaces # [view_source]","title":"test_spaces"},{"location":"api/tests/utils/test_spaces/#testsutilstest_spaces","text":"[view_source]","title":"tests.utils.test_spaces"},{"location":"api/tests/vision/test_pillow_rescaling/","text":"tests.vision.test_pillow_rescaling # [view_source]","title":"test_pillow_rescaling"},{"location":"api/tests/vision/test_pillow_rescaling/#testsvisiontest_pillow_rescaling","text":"[view_source]","title":"tests.vision.test_pillow_rescaling"},{"location":"getting_started/abstractions/","text":"Primary abstractions # Our package relies on a collection of fundamental abstractions to define how, and in what task, an agent should be trained and evaluated. A subset of these abstractions are described in plain language below. Each of the below sections end with a link to the (formal) documentation of the abstraction as well as a link to an example implementation of the abstraction (if relevant). The following provides a high-level illustration of how these abstractions interact. Experiment configuration # In allenact , experiments are defined by implementing the abstract ExperimentConfig class. The methods of this implementation are then called during training/inference to properly set up the desired experiment. For example, the ExperimentConfig.create_model method will be called at the beginning of training to create the model to be trained. See either the \"designing your first minigrid experiment\" or the \"designing an experiment for point navigation\" tutorials to get an in-depth description of how these experiment configurations are defined in practice. See also the abstract ExperimentConfig class and an example implementation . Task sampler # A task sampler is responsible for generating a sequence of tasks for agents to solve. The sequence of tasks can be randomly generated (e.g. in training) or extracted from an ordered pool (e.g. in validation or testing). See the abstract TaskSampler class and an example implementation . Task # Tasks define the scope of the interaction between agents and an environment (including the action types agents are allowed to execute), as well as metrics to evaluate the agents' performance. For example, we might define a task ObjectNaviThorGridTask in which agents receive observations obtained from the environment (e.g. RGB images) or directly from the task (e.g. a target object class) and are allowed to execute actions such as MoveAhead , RotateRight , RotateLeft , and End whenever agents determine they have reached their target. The metrics might include a success indicator or some quantitative metric on the optimality of the followed path. See the abstract Task class and an example implementation . Sensor # Sensors provide observations extracted from an environment (e.g. RGB or depth images) or directly from a task (e.g. the end point in point navigation or target object class in semantic navigation) that can be directly consumed by agents. See the abstract Sensor class and an example implementation . Actor critic model # The actor-critic agent is responsible for computing batched action probabilities and state values given the observations provided by sensors, internal state representations, previous actions, and potentially other inputs. See the abstract ActorCriticModel class and an example implementation . Training pipeline # The training pipeline, defined in the ExperimentConfig 's training_pipeline method , contains one or more training stages where different losses can be combined or sequentially applied . Losses # Actor-critic losses compute a combination of action loss and value loss out of collected experience that can be used to train actor-critic models with back-propagation, e.g. PPO or A2C. See the AbstractActorCriticLoss class and an example implementation . Off-policy losses implement generic training iterations in which a batch of data is run through a model (that can be a subgraph of an ActorCriticModel ) and a loss is computed on the model's output. See the AbstractOffPolicyLoss class and an example implementation .","title":"Primary abstractions"},{"location":"getting_started/abstractions/#primary-abstractions","text":"Our package relies on a collection of fundamental abstractions to define how, and in what task, an agent should be trained and evaluated. A subset of these abstractions are described in plain language below. Each of the below sections end with a link to the (formal) documentation of the abstraction as well as a link to an example implementation of the abstraction (if relevant). The following provides a high-level illustration of how these abstractions interact.","title":"Primary abstractions"},{"location":"getting_started/abstractions/#experiment-configuration","text":"In allenact , experiments are defined by implementing the abstract ExperimentConfig class. The methods of this implementation are then called during training/inference to properly set up the desired experiment. For example, the ExperimentConfig.create_model method will be called at the beginning of training to create the model to be trained. See either the \"designing your first minigrid experiment\" or the \"designing an experiment for point navigation\" tutorials to get an in-depth description of how these experiment configurations are defined in practice. See also the abstract ExperimentConfig class and an example implementation .","title":"Experiment configuration"},{"location":"getting_started/abstractions/#task-sampler","text":"A task sampler is responsible for generating a sequence of tasks for agents to solve. The sequence of tasks can be randomly generated (e.g. in training) or extracted from an ordered pool (e.g. in validation or testing). See the abstract TaskSampler class and an example implementation .","title":"Task sampler"},{"location":"getting_started/abstractions/#task","text":"Tasks define the scope of the interaction between agents and an environment (including the action types agents are allowed to execute), as well as metrics to evaluate the agents' performance. For example, we might define a task ObjectNaviThorGridTask in which agents receive observations obtained from the environment (e.g. RGB images) or directly from the task (e.g. a target object class) and are allowed to execute actions such as MoveAhead , RotateRight , RotateLeft , and End whenever agents determine they have reached their target. The metrics might include a success indicator or some quantitative metric on the optimality of the followed path. See the abstract Task class and an example implementation .","title":"Task"},{"location":"getting_started/abstractions/#sensor","text":"Sensors provide observations extracted from an environment (e.g. RGB or depth images) or directly from a task (e.g. the end point in point navigation or target object class in semantic navigation) that can be directly consumed by agents. See the abstract Sensor class and an example implementation .","title":"Sensor"},{"location":"getting_started/abstractions/#actor-critic-model","text":"The actor-critic agent is responsible for computing batched action probabilities and state values given the observations provided by sensors, internal state representations, previous actions, and potentially other inputs. See the abstract ActorCriticModel class and an example implementation .","title":"Actor critic model"},{"location":"getting_started/abstractions/#training-pipeline","text":"The training pipeline, defined in the ExperimentConfig 's training_pipeline method , contains one or more training stages where different losses can be combined or sequentially applied .","title":"Training pipeline"},{"location":"getting_started/abstractions/#losses","text":"Actor-critic losses compute a combination of action loss and value loss out of collected experience that can be used to train actor-critic models with back-propagation, e.g. PPO or A2C. See the AbstractActorCriticLoss class and an example implementation . Off-policy losses implement generic training iterations in which a batch of data is run through a model (that can be a subgraph of an ActorCriticModel ) and a loss is computed on the model's output. See the AbstractOffPolicyLoss class and an example implementation .","title":"Losses"},{"location":"getting_started/running-your-first-experiment/","text":"Running your first experiment # Assuming you have installed the full library , you can run your first experiment by calling PYTHONPATH = . python allenact/main.py minigrid_tutorial -b projects/tutorials -m 8 -o experiment_output/minigrid -s 12345 from the allenact root directory. With -b projects/tutorials we tell allenact that minigrid_tutorial experiment config file will be found in the projects/tutorials directory. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o experiment_output/minigrid we set the output folder into which results and logs will be saved. With -s 12345 we set the random seed. If everything was installed correctly, a simple model will be trained (and validated) in the MiniGrid environment and a new folder experiment_output/minigrid will be created containing: a checkpoints/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with model weight checkpoints, a used_configs/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with all used configuration files, and a tensorboard log file under tb/MiniGridTutorial/LOCAL_TIME_STR/ . Here LOCAL_TIME_STR is a string that records the time when the experiment was started (e.g. the string \"2020-08-21_18-19-47\" corresponds to an experiment started on August 21st 2020, 47 seconds past 6:19pm. If we have Tensorboard installed, we can track training progress with tensorboard --logdir experiment_output/minigrid/tb which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: A detailed tutorial describing how the minigrid_tutorial experiment configuration was created can be found here . To run your own custom experiment simply define a new experiment configuration in a file projects/YOUR_PROJECT_NAME/experiments/my_custom_experiment.py after which you may run it with PYTHONPATH=. python allenact/main.py my_custom_experiment -b projects/YOUR_PROJECT_NAME/experiments .","title":"Run your first experiment"},{"location":"getting_started/running-your-first-experiment/#running-your-first-experiment","text":"Assuming you have installed the full library , you can run your first experiment by calling PYTHONPATH = . python allenact/main.py minigrid_tutorial -b projects/tutorials -m 8 -o experiment_output/minigrid -s 12345 from the allenact root directory. With -b projects/tutorials we tell allenact that minigrid_tutorial experiment config file will be found in the projects/tutorials directory. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o experiment_output/minigrid we set the output folder into which results and logs will be saved. With -s 12345 we set the random seed. If everything was installed correctly, a simple model will be trained (and validated) in the MiniGrid environment and a new folder experiment_output/minigrid will be created containing: a checkpoints/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with model weight checkpoints, a used_configs/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with all used configuration files, and a tensorboard log file under tb/MiniGridTutorial/LOCAL_TIME_STR/ . Here LOCAL_TIME_STR is a string that records the time when the experiment was started (e.g. the string \"2020-08-21_18-19-47\" corresponds to an experiment started on August 21st 2020, 47 seconds past 6:19pm. If we have Tensorboard installed, we can track training progress with tensorboard --logdir experiment_output/minigrid/tb which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: A detailed tutorial describing how the minigrid_tutorial experiment configuration was created can be found here . To run your own custom experiment simply define a new experiment configuration in a file projects/YOUR_PROJECT_NAME/experiments/my_custom_experiment.py after which you may run it with PYTHONPATH=. python allenact/main.py my_custom_experiment -b projects/YOUR_PROJECT_NAME/experiments .","title":"Running your first experiment"},{"location":"getting_started/structure/","text":"Structure of the codebase # The codebase consists of the following directories: allenact , datasets , docs , overrides , allenact_plugins , pretrained_model_ckpts , projects , scripts , and tests . Below, we explain the overall structure and how different components of the codebase are organized. allenact directory # Contains runtime algorithms for on-policy and off-policy training and inference, base abstractions used throughout the code base and basic models to be used as building blocks in future models. allenact.algorithms includes on-policy and off-policy training nd inference algorithms and abstractions for losses, policies, rollout storage, etc. allenact.base_abstractions includes the base ExperimentConfig , distributions, base Sensor , TaskSampler , Task , etc. allenact.embodiedai includes basic CNN, and RNN state encoders, besides basic ActorCriticModel implementations for embodied AI tasks. datasets directory # A directory made to store task-specific datasets. For example, the script datasets/download_navigation_datasets.sh can be used to automatically download task dataset files for Point Navigation within the RoboTHOR environment and it will place these files into a new datasets/robothor-pointnav directory. docs directory # Contains documentation for the framework, including guides for installation and first experiments, how-to's for the definition and usage of different abstractions, tutorials and per-project documentation. overrides directory # Files within this directory are used to the look and structure of the documentation generated when running mkdocs . See our FAQ for information on how to generate this documentation for yourself. allenact_plugins directory # Contains implementations of ActorCriticModel s and Task s in different environments. Each plugin folder is named as {environment}_plugin and contains three subfolders: configs to host useful configuration for the environment or tasks. data to store data to be consumed by the environment or tasks. scripts to setup the plugin or gather and process data. pretrained_model_ckpts directory # Directory into which pretrained model checkpoints will be saved. See also the pretrained_model_ckpts/download_navigation_model_ckpts.sh which can be used to download such checkpoints. projects directory # Contains project-specific code like experiment configurations and scripts to process results, generate visualizations or prepare data. scripts directory # Includes framework-wide scripts to build the documentation, format code, run_tests and start an xserver. The latter can be used for OpenGL-based environments having super-user privileges in Linux, assuming NVIDIA drivers and xserver-xorg are installed. tests directory # Includes unit tests for allenact . allenact.utils directory # It includes different types of utilities, mainly divided into: allenact.utils.experiment_utils , including the TrainingPipeline , PipelineStage and other utilities to configure an experiment. allenact.utils.model_utils , including generic CNN creation, forward-pass helpers and other utilities. allenact.utils.tensor_utils , including functions to batch observations, convert tensors into video, scale image tensors, etc. allenact.utils.viz_utils , including a VizSuite class that can be instantiated with different visualization plugins during inference. allenact.utils.system , including logging and networking helpers. Other utils files, including allenact.utils.misc_utils , contain a number of helper functions for different purposes.","title":"Structure of the codebase"},{"location":"getting_started/structure/#structure-of-the-codebase","text":"The codebase consists of the following directories: allenact , datasets , docs , overrides , allenact_plugins , pretrained_model_ckpts , projects , scripts , and tests . Below, we explain the overall structure and how different components of the codebase are organized.","title":"Structure of the codebase"},{"location":"getting_started/structure/#allenact-directory","text":"Contains runtime algorithms for on-policy and off-policy training and inference, base abstractions used throughout the code base and basic models to be used as building blocks in future models. allenact.algorithms includes on-policy and off-policy training nd inference algorithms and abstractions for losses, policies, rollout storage, etc. allenact.base_abstractions includes the base ExperimentConfig , distributions, base Sensor , TaskSampler , Task , etc. allenact.embodiedai includes basic CNN, and RNN state encoders, besides basic ActorCriticModel implementations for embodied AI tasks.","title":"allenact directory"},{"location":"getting_started/structure/#datasets-directory","text":"A directory made to store task-specific datasets. For example, the script datasets/download_navigation_datasets.sh can be used to automatically download task dataset files for Point Navigation within the RoboTHOR environment and it will place these files into a new datasets/robothor-pointnav directory.","title":"datasets directory"},{"location":"getting_started/structure/#docs-directory","text":"Contains documentation for the framework, including guides for installation and first experiments, how-to's for the definition and usage of different abstractions, tutorials and per-project documentation.","title":"docs directory"},{"location":"getting_started/structure/#overrides-directory","text":"Files within this directory are used to the look and structure of the documentation generated when running mkdocs . See our FAQ for information on how to generate this documentation for yourself.","title":"overrides directory"},{"location":"getting_started/structure/#allenact_plugins-directory","text":"Contains implementations of ActorCriticModel s and Task s in different environments. Each plugin folder is named as {environment}_plugin and contains three subfolders: configs to host useful configuration for the environment or tasks. data to store data to be consumed by the environment or tasks. scripts to setup the plugin or gather and process data.","title":"allenact_plugins directory"},{"location":"getting_started/structure/#pretrained_model_ckpts-directory","text":"Directory into which pretrained model checkpoints will be saved. See also the pretrained_model_ckpts/download_navigation_model_ckpts.sh which can be used to download such checkpoints.","title":"pretrained_model_ckpts directory"},{"location":"getting_started/structure/#projects-directory","text":"Contains project-specific code like experiment configurations and scripts to process results, generate visualizations or prepare data.","title":"projects directory"},{"location":"getting_started/structure/#scripts-directory","text":"Includes framework-wide scripts to build the documentation, format code, run_tests and start an xserver. The latter can be used for OpenGL-based environments having super-user privileges in Linux, assuming NVIDIA drivers and xserver-xorg are installed.","title":"scripts directory"},{"location":"getting_started/structure/#tests-directory","text":"Includes unit tests for allenact .","title":"tests directory"},{"location":"getting_started/structure/#allenactutils-directory","text":"It includes different types of utilities, mainly divided into: allenact.utils.experiment_utils , including the TrainingPipeline , PipelineStage and other utilities to configure an experiment. allenact.utils.model_utils , including generic CNN creation, forward-pass helpers and other utilities. allenact.utils.tensor_utils , including functions to batch observations, convert tensors into video, scale image tensors, etc. allenact.utils.viz_utils , including a VizSuite class that can be instantiated with different visualization plugins during inference. allenact.utils.system , including logging and networking helpers. Other utils files, including allenact.utils.misc_utils , contain a number of helper functions for different purposes.","title":"allenact.utils directory"},{"location":"howtos/changing-rewards-and-losses/","text":"Changing rewards and losses # In order to train actor-critic agents, we need to specify rewards at the task level, and losses at the training pipeline level. Rewards # We will use the object navigation task in iTHOR as a running example. We can see how the ObjectNaviThorGridTask._step(self, action: int) -> RLStepResult method computes the reward for the latest action by invoking a function like: def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Any reward shaping can be easily added by e.g. modifying the definition of an existing class: class NavigationWithShaping ( allenact_plugins . ithor_plugin . ithor_tasks . ObjectNaviThorGridTask ): def judge ( self ) -> float : reward = super () . judge () if self . previous_state is not None : reward += float ( my_reward_shaping_function ( self . previous_state , self . current_state , )) self . previous_state = self . current_state return reward Losses # We support A2C , PPO , and imitation losses amongst others. We can easily include DAgger or variations thereof by assuming the availability of an expert providing optimal actions to agents and combining imitation and PPO losses in different ways through multiple stages: class MyExperimentConfig ( allenact . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) ppo_steps = int ( 3e4 ) ppo_steps2 = int ( 1e6 ) ... return allenact . utils . experiment_utils . TrainingPipeline ( named_losses = { \"imitation_loss\" : allenact . algorithms . onpolicy_sync . losses . imitation . Imitation (), \"ppo_loss\" : allenact . algorithms . onpolicy_sync . losses . ppo . PPO ( ** allenact . algorithms . onpolicy_sync . losses . ppo . PPOConfig , ), }, ... pipeline_stages = [ allenact . utils . experiment_utils . PipelineStage ( loss_names = [ \"imitation_loss\" , \"ppo_loss\" ], teacher_forcing = allenact . utils . experiment_utils . LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), allenact . utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = ppo_steps ), allenact . utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps2 , ), ], )","title":"Change rewards and losses"},{"location":"howtos/changing-rewards-and-losses/#changing-rewards-and-losses","text":"In order to train actor-critic agents, we need to specify rewards at the task level, and losses at the training pipeline level.","title":"Changing rewards and losses"},{"location":"howtos/changing-rewards-and-losses/#rewards","text":"We will use the object navigation task in iTHOR as a running example. We can see how the ObjectNaviThorGridTask._step(self, action: int) -> RLStepResult method computes the reward for the latest action by invoking a function like: def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Any reward shaping can be easily added by e.g. modifying the definition of an existing class: class NavigationWithShaping ( allenact_plugins . ithor_plugin . ithor_tasks . ObjectNaviThorGridTask ): def judge ( self ) -> float : reward = super () . judge () if self . previous_state is not None : reward += float ( my_reward_shaping_function ( self . previous_state , self . current_state , )) self . previous_state = self . current_state return reward","title":"Rewards"},{"location":"howtos/changing-rewards-and-losses/#losses","text":"We support A2C , PPO , and imitation losses amongst others. We can easily include DAgger or variations thereof by assuming the availability of an expert providing optimal actions to agents and combining imitation and PPO losses in different ways through multiple stages: class MyExperimentConfig ( allenact . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) ppo_steps = int ( 3e4 ) ppo_steps2 = int ( 1e6 ) ... return allenact . utils . experiment_utils . TrainingPipeline ( named_losses = { \"imitation_loss\" : allenact . algorithms . onpolicy_sync . losses . imitation . Imitation (), \"ppo_loss\" : allenact . algorithms . onpolicy_sync . losses . ppo . PPO ( ** allenact . algorithms . onpolicy_sync . losses . ppo . PPOConfig , ), }, ... pipeline_stages = [ allenact . utils . experiment_utils . PipelineStage ( loss_names = [ \"imitation_loss\" , \"ppo_loss\" ], teacher_forcing = allenact . utils . experiment_utils . LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), allenact . utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = ppo_steps ), allenact . utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps2 , ), ], )","title":"Losses"},{"location":"howtos/defining-a-new-model/","text":"Defining a new model # All actor-critic models must implement the interface described by the ActorCriticModel class . This interface includes two methods that need to be implemented: recurrent_memory_specification , returning a description of the model's recurrent memory; and forward , returning an ActorCriticOutput given the current observation, hidden state and previous actions. For convenience, we provide a recurrent network module and a simple CNN module from the Habitat baseline navigation models, that will be used in this example. Actor-critic model interface # As an example, let's build an object navigation agent. class ObjectNavBaselineActorCritic ( ActorCriticModel [ CategoricalDistr ]): \"\"\"Baseline recurrent actor critic model for object-navigation. # Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type `gym.spaces.Discrete`). observation_space : The observation space expected by the agent. This observation space should include (optionally) 'rgb' images and 'depth' images and is required to have a component corresponding to the goal `goal_sensor_uuid`. goal_sensor_uuid : The uuid of the sensor of the goal object. See `GoalObjectTypeThorSensor` as an example of such a sensor. hidden_size : The hidden size of the GRU RNN. object_type_embedding_dim: The dimensionality of the embedding corresponding to the goal object type. \"\"\" def __init__ ( self , action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , rgb_uuid : Optional [ str ], depth_uuid : Optional [ str ], hidden_size = 512 , object_type_embedding_dim = 8 , trainable_masked_hidden_state : bool = False , num_rnn_layers = 1 , rnn_type = \"GRU\" , ): \"\"\"Initializer. See class documentation for parameter definitions. \"\"\" super () . __init__ ( action_space = action_space , observation_space = observation_space ) self . goal_sensor_uuid = goal_sensor_uuid self . _n_object_types = self . observation_space . spaces [ self . goal_sensor_uuid ] . n self . _hidden_size = hidden_size self . object_type_embedding_size = object_type_embedding_dim self . visual_encoder = SimpleCNN ( observation_space = self . observation_space , output_size = self . _hidden_size , rgb_uuid = rgb_uuid , depth_uuid = depth_uuid , ) self . state_encoder = RNNStateEncoder ( ( 0 if self . is_blind else self . _hidden_size ) + object_type_embedding_dim , self . _hidden_size , trainable_masked_hidden_state = trainable_masked_hidden_state , num_layers = num_rnn_layers , rnn_type = rnn_type , ) self . actor = LinearActorHead ( self . _hidden_size , action_space . n ) self . critic = LinearCriticHead ( self . _hidden_size ) self . object_type_embedding = nn . Embedding ( num_embeddings = self . _n_object_types , embedding_dim = object_type_embedding_dim , ) self . train () @property def recurrent_hidden_state_size ( self ) -> int : \"\"\"The recurrent hidden state size of the model.\"\"\" return self . _hidden_size @property def is_blind ( self ) -> bool : \"\"\"True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).\"\"\" return self . visual_encoder . is_blind @property def num_recurrent_layers ( self ) -> int : \"\"\"Number of recurrent hidden layers.\"\"\" return self . state_encoder . num_recurrent_layers def _recurrent_memory_specification ( self ): return dict ( rnn = ( ( ( \"layer\" , self . num_recurrent_layers ), ( \"sampler\" , None ), ( \"hidden\" , self . recurrent_hidden_state_size ), ), torch . float32 , ) ) def get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ] ) -> torch . FloatTensor : \"\"\"Get the object type encoding from input batched observations.\"\"\" # noinspection PyTypeChecker return self . object_type_embedding ( # type:ignore observations [ self . goal_sensor_uuid ] . to ( torch . int64 ) ) def forward ( # type:ignore self , observations : ObservationType , memory : Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor , ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]]: \"\"\"Processes input batched observations to produce new actor and critic values. Processes input batched observations (along with prior hidden states, previous actions, and masks denoting which recurrent hidden states should be masked) and returns an `ActorCriticOutput` object containing the model's policy (distribution over actions) and evaluation of the current state (value). # Parameters observations : Batched input observations. memory : `Memory` containing the hidden states from initial timepoints. prev_actions : Tensor of previous actions taken. masks : Masks applied to hidden states. See `RNNStateEncoder`. # Returns Tuple of the `ActorCriticOutput` and recurrent hidden state. \"\"\" target_encoding = self . get_object_type_encoding ( cast ( Dict [ str , torch . FloatTensor ], observations ) ) x = [ target_encoding ] if not self . is_blind : perception_embed = self . visual_encoder ( observations ) x = [ perception_embed ] + x x_cat = torch . cat ( x , dim =- 1 ) # type: ignore x_out , rnn_hidden_states = self . state_encoder ( x_cat , memory . tensor ( \"rnn\" ), masks ) return ( ActorCriticOutput ( distributions = self . actor ( x_out ), values = self . critic ( x_out ), extras = {} ), memory . set_tensor ( \"rnn\" , rnn_hidden_states ), )","title":"Define a new model"},{"location":"howtos/defining-a-new-model/#defining-a-new-model","text":"All actor-critic models must implement the interface described by the ActorCriticModel class . This interface includes two methods that need to be implemented: recurrent_memory_specification , returning a description of the model's recurrent memory; and forward , returning an ActorCriticOutput given the current observation, hidden state and previous actions. For convenience, we provide a recurrent network module and a simple CNN module from the Habitat baseline navigation models, that will be used in this example.","title":"Defining a new model"},{"location":"howtos/defining-a-new-model/#actor-critic-model-interface","text":"As an example, let's build an object navigation agent. class ObjectNavBaselineActorCritic ( ActorCriticModel [ CategoricalDistr ]): \"\"\"Baseline recurrent actor critic model for object-navigation. # Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type `gym.spaces.Discrete`). observation_space : The observation space expected by the agent. This observation space should include (optionally) 'rgb' images and 'depth' images and is required to have a component corresponding to the goal `goal_sensor_uuid`. goal_sensor_uuid : The uuid of the sensor of the goal object. See `GoalObjectTypeThorSensor` as an example of such a sensor. hidden_size : The hidden size of the GRU RNN. object_type_embedding_dim: The dimensionality of the embedding corresponding to the goal object type. \"\"\" def __init__ ( self , action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , rgb_uuid : Optional [ str ], depth_uuid : Optional [ str ], hidden_size = 512 , object_type_embedding_dim = 8 , trainable_masked_hidden_state : bool = False , num_rnn_layers = 1 , rnn_type = \"GRU\" , ): \"\"\"Initializer. See class documentation for parameter definitions. \"\"\" super () . __init__ ( action_space = action_space , observation_space = observation_space ) self . goal_sensor_uuid = goal_sensor_uuid self . _n_object_types = self . observation_space . spaces [ self . goal_sensor_uuid ] . n self . _hidden_size = hidden_size self . object_type_embedding_size = object_type_embedding_dim self . visual_encoder = SimpleCNN ( observation_space = self . observation_space , output_size = self . _hidden_size , rgb_uuid = rgb_uuid , depth_uuid = depth_uuid , ) self . state_encoder = RNNStateEncoder ( ( 0 if self . is_blind else self . _hidden_size ) + object_type_embedding_dim , self . _hidden_size , trainable_masked_hidden_state = trainable_masked_hidden_state , num_layers = num_rnn_layers , rnn_type = rnn_type , ) self . actor = LinearActorHead ( self . _hidden_size , action_space . n ) self . critic = LinearCriticHead ( self . _hidden_size ) self . object_type_embedding = nn . Embedding ( num_embeddings = self . _n_object_types , embedding_dim = object_type_embedding_dim , ) self . train () @property def recurrent_hidden_state_size ( self ) -> int : \"\"\"The recurrent hidden state size of the model.\"\"\" return self . _hidden_size @property def is_blind ( self ) -> bool : \"\"\"True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).\"\"\" return self . visual_encoder . is_blind @property def num_recurrent_layers ( self ) -> int : \"\"\"Number of recurrent hidden layers.\"\"\" return self . state_encoder . num_recurrent_layers def _recurrent_memory_specification ( self ): return dict ( rnn = ( ( ( \"layer\" , self . num_recurrent_layers ), ( \"sampler\" , None ), ( \"hidden\" , self . recurrent_hidden_state_size ), ), torch . float32 , ) ) def get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ] ) -> torch . FloatTensor : \"\"\"Get the object type encoding from input batched observations.\"\"\" # noinspection PyTypeChecker return self . object_type_embedding ( # type:ignore observations [ self . goal_sensor_uuid ] . to ( torch . int64 ) ) def forward ( # type:ignore self , observations : ObservationType , memory : Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor , ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]]: \"\"\"Processes input batched observations to produce new actor and critic values. Processes input batched observations (along with prior hidden states, previous actions, and masks denoting which recurrent hidden states should be masked) and returns an `ActorCriticOutput` object containing the model's policy (distribution over actions) and evaluation of the current state (value). # Parameters observations : Batched input observations. memory : `Memory` containing the hidden states from initial timepoints. prev_actions : Tensor of previous actions taken. masks : Masks applied to hidden states. See `RNNStateEncoder`. # Returns Tuple of the `ActorCriticOutput` and recurrent hidden state. \"\"\" target_encoding = self . get_object_type_encoding ( cast ( Dict [ str , torch . FloatTensor ], observations ) ) x = [ target_encoding ] if not self . is_blind : perception_embed = self . visual_encoder ( observations ) x = [ perception_embed ] + x x_cat = torch . cat ( x , dim =- 1 ) # type: ignore x_out , rnn_hidden_states = self . state_encoder ( x_cat , memory . tensor ( \"rnn\" ), masks ) return ( ActorCriticOutput ( distributions = self . actor ( x_out ), values = self . critic ( x_out ), extras = {} ), memory . set_tensor ( \"rnn\" , rnn_hidden_states ), )","title":"Actor-critic model interface"},{"location":"howtos/defining-a-new-task/","text":"Defining a new task # In order to use new tasks in our experiments, we need to define two classes: A Task , including, among others, a step implementation providing a RLStepResult , a metrics method providing quantitative performance measurements for agents and, optionally, a query_expert method that can be used e.g. with an imitation loss during training. A TaskSampler , that allows instantiating new Tasks for the agents to solve during training, validation and testing. Task # Let's define a semantic navigation task, where agents have to navigate from a starting point in an environment to an object of a specific class using a minimal amount of steps and deciding when the goal has been reached. We need to define the methods action_space , render , _step , reached_terminal_state , class_action_names , close , metrics , and query_expert from the base Task definition. Initialization, action space and termination # Let's start with the definition of the action space and task initialization: ... from allenact_plugins.ithor_plugin.ithor_constants import ( MOVE_AHEAD , ROTATE_LEFT , ROTATE_RIGHT , LOOK_DOWN , LOOK_UP , END , ) ... class ObjectNaviThorGridTask ( Task [ IThorEnvironment ]): _actions = ( MOVE_AHEAD , ROTATE_LEFT , ROTATE_RIGHT , LOOK_DOWN , LOOK_UP , END ) def __init__ ( self , env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) -> None : super () . __init__ ( env = env , sensors = sensors , task_info = task_info , max_steps = max_steps , ** kwargs ) self . _took_end_action : bool = False self . _success : Optional [ bool ] = False @property def action_space ( self ): return gym . spaces . Discrete ( len ( self . _actions )) @classmethod def class_action_names ( cls ) -> Tuple [ str , ... ]: return cls . _actions def reached_terminal_state ( self ) -> bool : return self . _took_end_action def close ( self ) -> None : self . env . stop () ... Step method # Next, we define the main method _step that will be called every time the agent produces a new action: class ObjectNaviThorGridTask ( Task [ IThorEnvironment ]): ... def _step ( self , action : Union [ int , Sequence [ int ]]) -> RLStepResult : assert isinstance ( action , int ) action = cast ( int , action ) action_str = self . class_action_names ()[ action ] if action_str == END : self . _took_end_action = True self . _success = self . is_goal_object_visible () self . last_action_success = self . _success else : self . env . step ({ \"action\" : action_str }) self . last_action_success = self . env . last_action_success step_result = RLStepResult ( observation = self . get_observations (), reward = self . judge (), done = self . is_done (), info = { \"last_action_success\" : self . last_action_success }, ) return step_result ... def is_goal_object_visible ( self ) -> bool : return any ( o [ \"objectType\" ] == self . task_info [ \"object_type\" ] for o in self . env . visible_objects () ) def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Metrics, rendering and expert actions # Finally, we define methods to render and evaluate the current task, and optionally generate expert actions to be used e.g. for DAgger training. def render ( self , mode : str = \"rgb\" , * args , ** kwargs ) -> numpy . ndarray : assert mode == \"rgb\" , \"only rgb rendering is implemented\" return self . env . current_frame def metrics ( self ) -> Dict [ str , Any ]: if not self . is_done (): return {} else : return { \"success\" : self . _success , \"ep_length\" : self . num_steps_taken ()} def query_expert ( self , ** kwargs ) -> Tuple [ int , bool ]: return my_objnav_expert_implementation ( self ) TaskSampler # We also need to define the corresponding TaskSampler, which must contain implementations for methods __len__ , total_unique , last_sampled_task , next_task , close , reset , and set_seed . Currently, an additional method all_observation_spaces_equal is used to ensure compatibility with the current RolloutStorage . Let's define a tasks sampler able to provide an infinite number of object navigation tasks for AI2-THOR. Initialization and termination # class ObjectNavTaskSampler ( TaskSampler ): def __init__ ( self , scenes : List [ str ], object_types : str , sensors : List [ Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . Space , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , * args , ** kwargs ) -> None : self . env_args = env_args self . scenes = scenes self . object_types = object_types self . grid_size = 0.25 self . env : Optional [ IThorEnvironment ] = None self . sensors = sensors self . max_steps = max_steps self . _action_sapce = action_space self . scene_id : Optional [ int ] = None self . _last_sampled_task : Optional [ ObjectNaviThorGridTask ] = None set_seed ( seed ) self . reset () def close ( self ) -> None : if self . env is not None : self . env . stop () def reset ( self ): self . scene_id = 0 def _create_environment ( self ) -> IThorEnvironment : env = IThorEnvironment ( make_agents_visible = False , object_open_speed = 0.05 , restrict_to_initially_reachable_points = True , ** self . env_args , ) return env Task sampling # Finally, we need to define methods to determine the number of available tasks (possibly infinite) and sample tasks: @property def length ( self ) -> Union [ int , float ]: return float ( \"inf\" ) @property def total_unique ( self ) -> Optional [ Union [ int , float ]]: return None @property def last_sampled_task ( self ) -> Optional [ ObjectNaviThorGridTask ]: return self . _last_sampled_task @property def all_observation_spaces_equal ( self ) -> bool : return True def next_task ( self ) -> Optional [ ObjectNaviThorGridTask ]: self . scene_id = random . randint ( 0 , len ( self . scenes ) - 1 ) self . scene = self . scenes [ self . scene_id ] if self . env is not None : if scene != self . env . scene_name : self . env . reset ( scene ) else : self . env = self . _create_environment () self . env . reset ( scene_name = scene ) self . env . randomize_agent_location () task_info = { \"object_type\" : random . sample ( self . object_types , 1 )} self . _last_sampled_task = ObjectNaviThorGridTask ( env = self . env , sensors = self . sensors , task_info = task_info , max_steps = self . max_steps , action_space = self . _action_sapce , ) return self . _last_sampled_task","title":"Define a new task"},{"location":"howtos/defining-a-new-task/#defining-a-new-task","text":"In order to use new tasks in our experiments, we need to define two classes: A Task , including, among others, a step implementation providing a RLStepResult , a metrics method providing quantitative performance measurements for agents and, optionally, a query_expert method that can be used e.g. with an imitation loss during training. A TaskSampler , that allows instantiating new Tasks for the agents to solve during training, validation and testing.","title":"Defining a new task"},{"location":"howtos/defining-a-new-task/#task","text":"Let's define a semantic navigation task, where agents have to navigate from a starting point in an environment to an object of a specific class using a minimal amount of steps and deciding when the goal has been reached. We need to define the methods action_space , render , _step , reached_terminal_state , class_action_names , close , metrics , and query_expert from the base Task definition.","title":"Task"},{"location":"howtos/defining-a-new-task/#initialization-action-space-and-termination","text":"Let's start with the definition of the action space and task initialization: ... from allenact_plugins.ithor_plugin.ithor_constants import ( MOVE_AHEAD , ROTATE_LEFT , ROTATE_RIGHT , LOOK_DOWN , LOOK_UP , END , ) ... class ObjectNaviThorGridTask ( Task [ IThorEnvironment ]): _actions = ( MOVE_AHEAD , ROTATE_LEFT , ROTATE_RIGHT , LOOK_DOWN , LOOK_UP , END ) def __init__ ( self , env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) -> None : super () . __init__ ( env = env , sensors = sensors , task_info = task_info , max_steps = max_steps , ** kwargs ) self . _took_end_action : bool = False self . _success : Optional [ bool ] = False @property def action_space ( self ): return gym . spaces . Discrete ( len ( self . _actions )) @classmethod def class_action_names ( cls ) -> Tuple [ str , ... ]: return cls . _actions def reached_terminal_state ( self ) -> bool : return self . _took_end_action def close ( self ) -> None : self . env . stop () ...","title":"Initialization, action space and termination"},{"location":"howtos/defining-a-new-task/#step-method","text":"Next, we define the main method _step that will be called every time the agent produces a new action: class ObjectNaviThorGridTask ( Task [ IThorEnvironment ]): ... def _step ( self , action : Union [ int , Sequence [ int ]]) -> RLStepResult : assert isinstance ( action , int ) action = cast ( int , action ) action_str = self . class_action_names ()[ action ] if action_str == END : self . _took_end_action = True self . _success = self . is_goal_object_visible () self . last_action_success = self . _success else : self . env . step ({ \"action\" : action_str }) self . last_action_success = self . env . last_action_success step_result = RLStepResult ( observation = self . get_observations (), reward = self . judge (), done = self . is_done (), info = { \"last_action_success\" : self . last_action_success }, ) return step_result ... def is_goal_object_visible ( self ) -> bool : return any ( o [ \"objectType\" ] == self . task_info [ \"object_type\" ] for o in self . env . visible_objects () ) def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward )","title":"Step method"},{"location":"howtos/defining-a-new-task/#metrics-rendering-and-expert-actions","text":"Finally, we define methods to render and evaluate the current task, and optionally generate expert actions to be used e.g. for DAgger training. def render ( self , mode : str = \"rgb\" , * args , ** kwargs ) -> numpy . ndarray : assert mode == \"rgb\" , \"only rgb rendering is implemented\" return self . env . current_frame def metrics ( self ) -> Dict [ str , Any ]: if not self . is_done (): return {} else : return { \"success\" : self . _success , \"ep_length\" : self . num_steps_taken ()} def query_expert ( self , ** kwargs ) -> Tuple [ int , bool ]: return my_objnav_expert_implementation ( self )","title":"Metrics, rendering and expert actions"},{"location":"howtos/defining-a-new-task/#tasksampler","text":"We also need to define the corresponding TaskSampler, which must contain implementations for methods __len__ , total_unique , last_sampled_task , next_task , close , reset , and set_seed . Currently, an additional method all_observation_spaces_equal is used to ensure compatibility with the current RolloutStorage . Let's define a tasks sampler able to provide an infinite number of object navigation tasks for AI2-THOR.","title":"TaskSampler"},{"location":"howtos/defining-a-new-task/#initialization-and-termination","text":"class ObjectNavTaskSampler ( TaskSampler ): def __init__ ( self , scenes : List [ str ], object_types : str , sensors : List [ Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . Space , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , * args , ** kwargs ) -> None : self . env_args = env_args self . scenes = scenes self . object_types = object_types self . grid_size = 0.25 self . env : Optional [ IThorEnvironment ] = None self . sensors = sensors self . max_steps = max_steps self . _action_sapce = action_space self . scene_id : Optional [ int ] = None self . _last_sampled_task : Optional [ ObjectNaviThorGridTask ] = None set_seed ( seed ) self . reset () def close ( self ) -> None : if self . env is not None : self . env . stop () def reset ( self ): self . scene_id = 0 def _create_environment ( self ) -> IThorEnvironment : env = IThorEnvironment ( make_agents_visible = False , object_open_speed = 0.05 , restrict_to_initially_reachable_points = True , ** self . env_args , ) return env","title":"Initialization and termination"},{"location":"howtos/defining-a-new-task/#task-sampling","text":"Finally, we need to define methods to determine the number of available tasks (possibly infinite) and sample tasks: @property def length ( self ) -> Union [ int , float ]: return float ( \"inf\" ) @property def total_unique ( self ) -> Optional [ Union [ int , float ]]: return None @property def last_sampled_task ( self ) -> Optional [ ObjectNaviThorGridTask ]: return self . _last_sampled_task @property def all_observation_spaces_equal ( self ) -> bool : return True def next_task ( self ) -> Optional [ ObjectNaviThorGridTask ]: self . scene_id = random . randint ( 0 , len ( self . scenes ) - 1 ) self . scene = self . scenes [ self . scene_id ] if self . env is not None : if scene != self . env . scene_name : self . env . reset ( scene ) else : self . env = self . _create_environment () self . env . reset ( scene_name = scene ) self . env . randomize_agent_location () task_info = { \"object_type\" : random . sample ( self . object_types , 1 )} self . _last_sampled_task = ObjectNaviThorGridTask ( env = self . env , sensors = self . sensors , task_info = task_info , max_steps = self . max_steps , action_space = self . _action_sapce , ) return self . _last_sampled_task","title":"Task sampling"},{"location":"howtos/defining-a-new-training-pipeline/","text":"Defining a new training pipeline # Defining a new training pipeline, or even new learning algorithms, is straightforward with the modular design in AllenAct . A convenience Builder object allows us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. On-policy # We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 2 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) ... Alternatively, we could use a more complex pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows: class ObjectNavThorDaggerThenPPOExperimentConfig ( ExperimentConfig ): ... SENSORS = [ ... ExpertActionSensor ( nactions = 6 ), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 1e4 ) # Much smaller number of steps as we're using imitation learning ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 1 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), \"imitation_loss\" : Imitation (), # We add an imitation loss. }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) Off-policy # We can also define off-policy stages where an external dataset is used, in this case, for Behavior Cloning: class BCOffPolicyBabyAIGoToLocalExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = int ( 1e7 ) num_steps = 128 return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 0 , # no on-policy training update_repeats = 0 , # no on-policy training num_steps = num_steps // 4 , # rollouts from environment tasks named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss ( total_episodes_in_epoch = int ( 1e6 ) # dataset contains 1M episodes ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # We only train from off-policy data: offpolicy_component = OffPolicyPipelineComponent ( data_iterator_builder = lambda ** kwargs : create_minigrid_offpolicy_data_iterator ( path = DATASET_PATH , # external dataset nrollouts = 128 , # per trainer batch size rollout_len = num_steps , # For truncated-BPTT instr_len = 5 , ** kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = 16 , # 16 batches per rollout ), ), ], ) Note that, in this example, 128 / 4 = 32 steps will be sampled from tasks in a MiniGrid environment (which can be useful to track the agent's performance), while a subgraph of the model (in this case the entire Actor) is trained from batches of 128-step truncated episodes sampled from an offline dataset stored under DATASET_PATH .","title":"Define a new training pipeline"},{"location":"howtos/defining-a-new-training-pipeline/#defining-a-new-training-pipeline","text":"Defining a new training pipeline, or even new learning algorithms, is straightforward with the modular design in AllenAct . A convenience Builder object allows us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers.","title":"Defining a new training pipeline"},{"location":"howtos/defining-a-new-training-pipeline/#on-policy","text":"We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 2 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) ... Alternatively, we could use a more complex pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows: class ObjectNavThorDaggerThenPPOExperimentConfig ( ExperimentConfig ): ... SENSORS = [ ... ExpertActionSensor ( nactions = 6 ), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 1e4 ) # Much smaller number of steps as we're using imitation learning ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 1 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), \"imitation_loss\" : Imitation (), # We add an imitation loss. }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), )","title":"On-policy"},{"location":"howtos/defining-a-new-training-pipeline/#off-policy","text":"We can also define off-policy stages where an external dataset is used, in this case, for Behavior Cloning: class BCOffPolicyBabyAIGoToLocalExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = int ( 1e7 ) num_steps = 128 return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 0 , # no on-policy training update_repeats = 0 , # no on-policy training num_steps = num_steps // 4 , # rollouts from environment tasks named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss ( total_episodes_in_epoch = int ( 1e6 ) # dataset contains 1M episodes ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # We only train from off-policy data: offpolicy_component = OffPolicyPipelineComponent ( data_iterator_builder = lambda ** kwargs : create_minigrid_offpolicy_data_iterator ( path = DATASET_PATH , # external dataset nrollouts = 128 , # per trainer batch size rollout_len = num_steps , # For truncated-BPTT instr_len = 5 , ** kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = 16 , # 16 batches per rollout ), ), ], ) Note that, in this example, 128 / 4 = 32 steps will be sampled from tasks in a MiniGrid environment (which can be useful to track the agent's performance), while a subgraph of the model (in this case the entire Actor) is trained from batches of 128-step truncated episodes sampled from an offline dataset stored under DATASET_PATH .","title":"Off-policy"},{"location":"howtos/defining-an-experiment/","text":"Defining an experiment # Let's look at an example experiment configuration for an object navigation example with an actor-critic agent observing RGB images from the environment and target object classes from the task. This is a simplified example where the agent is confined to a single iTHOR scene ( FloorPlan1 ) and needs to find a single object (a tomato). To see how one might running a \"full\"/\"hard\" version of navigation within AI2-THOR, see our tutorials PointNav in RoboTHOR and Swapping in a new environment . The interface to be implemented by the experiment specification is defined in allenact.base_abstractions.experiment_config . If you'd like to skip ahead and see the finished configuration, see here . We begin by making the following imports: from math import ceil from typing import Dict , Any , List , Optional import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from allenact.algorithms.onpolicy_sync.losses import PPO from allenact.algorithms.onpolicy_sync.losses.ppo import PPOConfig from allenact.base_abstractions.experiment_config import ExperimentConfig from allenact.base_abstractions.sensor import SensorSuite from allenact.base_abstractions.task import TaskSampler from allenact_plugins.ithor_plugin.ithor_sensors import RGBSensorThor , GoalObjectTypeThorSensor from allenact_plugins.ithor_plugin.ithor_task_samplers import ObjectNavTaskSampler from allenact_plugins.ithor_plugin.ithor_tasks import ObjectNaviThorGridTask from projects.objectnav_baselines.models.object_nav_models import ( ObjectNavBaselineActorCritic , ) from allenact.utils.experiment_utils import Builder , PipelineStage , TrainingPipeline , LinearDecay Now first method to implement is tag , which provides a string identifying the experiment: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def tag ( cls ): return \"ObjectNavThorPPO\" ... Model creation # Next, create_model will be used to instantiate an baseline object navigation actor-critic model : class ObjectNavThorExperimentConfig ( ExperimentConfig ): ... # A simple setting, train/valid/test are all the same single scene # and we're looking for a single object OBJECT_TYPES = [ \"Tomato\" ] TRAIN_SCENES = [ \"FloorPlan1_physics\" ] VALID_SCENES = [ \"FloorPlan1_physics\" ] TEST_SCENES = [ \"FloorPlan1_physics\" ] # Setting up sensors and basic environment details SCREEN_SIZE = 224 SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), GoalObjectTypeThorSensor ( object_types = OBJECT_TYPES ), ] ... @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ObjectNavBaselineActorCritic ( action_space = gym . spaces . Discrete ( len ( ObjectNaviThorGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , rgb_uuid = cls . SENSORS [ 0 ] . uuid , depth_uuid = None , goal_sensor_uuid = \"goal_object_type_ind\" , hidden_size = 512 , object_type_embedding_dim = 8 , ) ... Training pipeline # We now implement a training pipeline which trains with a single stage using PPO. In the below we use Builder objects, which allow us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. This is necessary when instantiating things like PyTorch optimizers who take as input the list of parameters associated with our agent's model (something we can't know until the create_model function has been called). class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 2 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) ... Alternatively, we could use a more sophisticated pipeline that begins training with dataset aggregation ( DAgger ) before moving to training with PPO. This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows class ObjectNavThorDaggerThenPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ): ... SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), GoalObjectTypeThorSensor ( object_types = OBJECT_TYPES ), ExpertActionSensor ( nactions = 6 ), # Notice that we have added an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 1e4 ) # Much smaller number of steps as we're using imitation learning ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 1 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), \"imitation_loss\" : Imitation (), # We add an imitation loss. }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) A version of our experiment config file for which we have implemented this two-stage training can be found here . This two-stage configuration ObjectNavThorDaggerThenPPOExperimentConfig is actually implemented as a subclass of ObjectNavThorPPOExperimentConfig . This is a common pattern used in AllenAct and lets one skip a great deal of boilerplate when defining a new experiment as a slight modification of an old one. Of course one must then be careful: changes to the superclass configuration will propagate to all subclassed configurations. Machine configuration # In machine_params we define machine configuration parameters that will be used for training, validation and test: class ObjectNavThorPPOExperimentConfig ( allenact . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ): num_gpus = torch . cuda . device_count () has_gpu = num_gpus != 0 if mode == \"train\" : nprocesses = 20 if has_gpu else 4 gpu_ids = [ 0 ] if has_gpu else [] elif mode == \"valid\" : nprocesses = 1 gpu_ids = [ 1 % num_gpus ] if has_gpu else [] elif mode == \"test\" : nprocesses = 1 gpu_ids = [ 0 ] if has_gpu else [] else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids } ... In the above we use the availability of cuda ( torch.cuda.device_count() != 0 ) to determine whether we should use parameters appropriate for local machines or for a server. We might optionally add a list of sampler_devices to assign devices (likely those not used for running our agent) to task sampling workers. Task sampling # The above has defined the model we'd like to use, the types of losses we wish to use during training, and the machine specific parameters that should be used during training. Critically we have not yet defined which task we wish to train our agent to complete. This is done by implementing the ExperimentConfig.make_sampler_fn function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return ObjectNavTaskSampler ( ** kwargs ) ... Now, before training starts, our trainer will know to generate a collection of task samplers using make_sampler_fn for training (and possibly validation or testing). The kwargs parameters used in the above function call can be different for each training process, we implement such differences using the ExperimentConfig.train_task_sampler_args function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_SCENES , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_period\" ] = \"manual\" res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) return res ... Now training process i out of n total processes will be instantiated with the parameters ObjectNavThorPPOExperimentConfig.train_task_sampler_args(i, n, ...) . Similar functions ( valid_task_sampler_args and test_task_sampler_args ) exist for generating validation and test parameters. Note also that with this function we can assign devices to run our environment for each worker. See the documentation of ExperimentConfig for more information. Running the experiment # We are now in the position to run the experiment (with seed 12345) using the command python main.py object_nav_ithor_ppo_one_object -b projects/tutorials -s 12345","title":"Define an experiment"},{"location":"howtos/defining-an-experiment/#defining-an-experiment","text":"Let's look at an example experiment configuration for an object navigation example with an actor-critic agent observing RGB images from the environment and target object classes from the task. This is a simplified example where the agent is confined to a single iTHOR scene ( FloorPlan1 ) and needs to find a single object (a tomato). To see how one might running a \"full\"/\"hard\" version of navigation within AI2-THOR, see our tutorials PointNav in RoboTHOR and Swapping in a new environment . The interface to be implemented by the experiment specification is defined in allenact.base_abstractions.experiment_config . If you'd like to skip ahead and see the finished configuration, see here . We begin by making the following imports: from math import ceil from typing import Dict , Any , List , Optional import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from allenact.algorithms.onpolicy_sync.losses import PPO from allenact.algorithms.onpolicy_sync.losses.ppo import PPOConfig from allenact.base_abstractions.experiment_config import ExperimentConfig from allenact.base_abstractions.sensor import SensorSuite from allenact.base_abstractions.task import TaskSampler from allenact_plugins.ithor_plugin.ithor_sensors import RGBSensorThor , GoalObjectTypeThorSensor from allenact_plugins.ithor_plugin.ithor_task_samplers import ObjectNavTaskSampler from allenact_plugins.ithor_plugin.ithor_tasks import ObjectNaviThorGridTask from projects.objectnav_baselines.models.object_nav_models import ( ObjectNavBaselineActorCritic , ) from allenact.utils.experiment_utils import Builder , PipelineStage , TrainingPipeline , LinearDecay Now first method to implement is tag , which provides a string identifying the experiment: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def tag ( cls ): return \"ObjectNavThorPPO\" ...","title":"Defining an  experiment"},{"location":"howtos/defining-an-experiment/#model-creation","text":"Next, create_model will be used to instantiate an baseline object navigation actor-critic model : class ObjectNavThorExperimentConfig ( ExperimentConfig ): ... # A simple setting, train/valid/test are all the same single scene # and we're looking for a single object OBJECT_TYPES = [ \"Tomato\" ] TRAIN_SCENES = [ \"FloorPlan1_physics\" ] VALID_SCENES = [ \"FloorPlan1_physics\" ] TEST_SCENES = [ \"FloorPlan1_physics\" ] # Setting up sensors and basic environment details SCREEN_SIZE = 224 SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), GoalObjectTypeThorSensor ( object_types = OBJECT_TYPES ), ] ... @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ObjectNavBaselineActorCritic ( action_space = gym . spaces . Discrete ( len ( ObjectNaviThorGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , rgb_uuid = cls . SENSORS [ 0 ] . uuid , depth_uuid = None , goal_sensor_uuid = \"goal_object_type_ind\" , hidden_size = 512 , object_type_embedding_dim = 8 , ) ...","title":"Model creation"},{"location":"howtos/defining-an-experiment/#training-pipeline","text":"We now implement a training pipeline which trains with a single stage using PPO. In the below we use Builder objects, which allow us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. This is necessary when instantiating things like PyTorch optimizers who take as input the list of parameters associated with our agent's model (something we can't know until the create_model function has been called). class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 2 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) ... Alternatively, we could use a more sophisticated pipeline that begins training with dataset aggregation ( DAgger ) before moving to training with PPO. This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows class ObjectNavThorDaggerThenPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ): ... SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), GoalObjectTypeThorSensor ( object_types = OBJECT_TYPES ), ExpertActionSensor ( nactions = 6 ), # Notice that we have added an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 1e4 ) # Much smaller number of steps as we're using imitation learning ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 1 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), \"imitation_loss\" : Imitation (), # We add an imitation loss. }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) A version of our experiment config file for which we have implemented this two-stage training can be found here . This two-stage configuration ObjectNavThorDaggerThenPPOExperimentConfig is actually implemented as a subclass of ObjectNavThorPPOExperimentConfig . This is a common pattern used in AllenAct and lets one skip a great deal of boilerplate when defining a new experiment as a slight modification of an old one. Of course one must then be careful: changes to the superclass configuration will propagate to all subclassed configurations.","title":"Training pipeline"},{"location":"howtos/defining-an-experiment/#machine-configuration","text":"In machine_params we define machine configuration parameters that will be used for training, validation and test: class ObjectNavThorPPOExperimentConfig ( allenact . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ): num_gpus = torch . cuda . device_count () has_gpu = num_gpus != 0 if mode == \"train\" : nprocesses = 20 if has_gpu else 4 gpu_ids = [ 0 ] if has_gpu else [] elif mode == \"valid\" : nprocesses = 1 gpu_ids = [ 1 % num_gpus ] if has_gpu else [] elif mode == \"test\" : nprocesses = 1 gpu_ids = [ 0 ] if has_gpu else [] else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids } ... In the above we use the availability of cuda ( torch.cuda.device_count() != 0 ) to determine whether we should use parameters appropriate for local machines or for a server. We might optionally add a list of sampler_devices to assign devices (likely those not used for running our agent) to task sampling workers.","title":"Machine configuration"},{"location":"howtos/defining-an-experiment/#task-sampling","text":"The above has defined the model we'd like to use, the types of losses we wish to use during training, and the machine specific parameters that should be used during training. Critically we have not yet defined which task we wish to train our agent to complete. This is done by implementing the ExperimentConfig.make_sampler_fn function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return ObjectNavTaskSampler ( ** kwargs ) ... Now, before training starts, our trainer will know to generate a collection of task samplers using make_sampler_fn for training (and possibly validation or testing). The kwargs parameters used in the above function call can be different for each training process, we implement such differences using the ExperimentConfig.train_task_sampler_args function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_SCENES , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_period\" ] = \"manual\" res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) return res ... Now training process i out of n total processes will be instantiated with the parameters ObjectNavThorPPOExperimentConfig.train_task_sampler_args(i, n, ...) . Similar functions ( valid_task_sampler_args and test_task_sampler_args ) exist for generating validation and test parameters. Note also that with this function we can assign devices to run our environment for each worker. See the documentation of ExperimentConfig for more information.","title":"Task sampling"},{"location":"howtos/defining-an-experiment/#running-the-experiment","text":"We are now in the position to run the experiment (with seed 12345) using the command python main.py object_nav_ithor_ppo_one_object -b projects/tutorials -s 12345","title":"Running the experiment"},{"location":"howtos/running-a-multi-agent-experiment/","text":"To-do #","title":"To-do"},{"location":"howtos/running-a-multi-agent-experiment/#to-do","text":"","title":"To-do"},{"location":"howtos/visualizing-results/","text":"To-do #","title":"To-do"},{"location":"howtos/visualizing-results/#to-do","text":"","title":"To-do"},{"location":"installation/download-datasets/","text":"Downloading datasets # Note: These instructions assume you have installed the full library and, generally, installed specific plugin requirements . The below provides instructions on how to download datasets necessary for defining the train, validation, and test sets used within the ObjectNav/PointNav tasks in the iTHOR and RoboTHOR environments. Point Navigation (PointNav) # RoboTHOR # To get the PointNav dataset for RoboTHOR run the following command: bash datasets/download_navigation_datasets.sh robothor-pointnav This will download the dataset into datasets/robothor-pointnav . iTHOR # To get the PointNav dataset for iTHOR run the following command: bash datasets/download_navigation_datasets.sh ithor-pointnav This will download the dataset into datasets/ithor-pointnav . Object Navigation (ObjectNav) # RoboTHOR # To get the ObjectNav dataset for RoboTHOR run the following command: bash datasets/download_navigation_datasets.sh robothor-objectnav This will download the dataset into datasets/robothor-objectnav . iTHOR # To get the ObjectNav dataset for iTHOR run the following command: bash datasets/download_navigation_datasets.sh ithor-objectnav This will download the dataset into datasets/ithor-objectnav .","title":"Download datasets"},{"location":"installation/download-datasets/#downloading-datasets","text":"Note: These instructions assume you have installed the full library and, generally, installed specific plugin requirements . The below provides instructions on how to download datasets necessary for defining the train, validation, and test sets used within the ObjectNav/PointNav tasks in the iTHOR and RoboTHOR environments.","title":"Downloading datasets"},{"location":"installation/download-datasets/#point-navigation-pointnav","text":"","title":"Point Navigation (PointNav)"},{"location":"installation/download-datasets/#robothor","text":"To get the PointNav dataset for RoboTHOR run the following command: bash datasets/download_navigation_datasets.sh robothor-pointnav This will download the dataset into datasets/robothor-pointnav .","title":"RoboTHOR"},{"location":"installation/download-datasets/#ithor","text":"To get the PointNav dataset for iTHOR run the following command: bash datasets/download_navigation_datasets.sh ithor-pointnav This will download the dataset into datasets/ithor-pointnav .","title":"iTHOR"},{"location":"installation/download-datasets/#object-navigation-objectnav","text":"","title":"Object Navigation (ObjectNav)"},{"location":"installation/download-datasets/#robothor_1","text":"To get the ObjectNav dataset for RoboTHOR run the following command: bash datasets/download_navigation_datasets.sh robothor-objectnav This will download the dataset into datasets/robothor-objectnav .","title":"RoboTHOR"},{"location":"installation/download-datasets/#ithor_1","text":"To get the ObjectNav dataset for iTHOR run the following command: bash datasets/download_navigation_datasets.sh ithor-objectnav This will download the dataset into datasets/ithor-objectnav .","title":"iTHOR"},{"location":"installation/installation-allenact/","text":"Installation of AllenAct # Note 1: This library has been tested only in python 3.6 . The following assumes you have a working version of python 3.6 installed locally. Note 2: If you are installing allenact intending to use a GPU for training/inference and your current machine uses an older version of CUDA you may need to manually install the version of PyTorch that supports your CUDA version. In such a case, after installing the below requirements, you should follow the directions for installing PyTorch with older versions of CUDA available on the PyTorch homepage . In order to install allenact and/or its requirements we recommend creating a new python virtual environment and installing all of the below requirements into this virtual environment. Alternatively, we also document how to install a conda environment with all the requirements, which is especially useful if you plan to train models in Habitat . Different ways to use allenact # There are three main installation paths depending on how you wish to use allenact . You want to use the allenact abstractions and training engine for your own task/environment and don't really care about using any of our plugins that offer additional support (in the form of models, sensors, task samplers, etc.) for select tasks/environments like AI2-THOR, Habitat, and MiniGrid. If this sounds like you, install the standalone framework . You want to use allenact as above but would also like to use some of our additional plugins. If this sounds like you, install the framework and plugins . You want full access to everything in allenact (including all plugins and all of our projects and baselines) and want to have the option to edit the internal implementation of allenact to suit your desire. If this sounds like you, install the full library . Standalone framework # You can install allenact easily using pip: pip install allenact If you'd like to install the latest development version of allenact (possibly unstable) directly from GitHub see the next section. Bleeding edge pip install # To install the latest allenact framework, you can use pip install -e \"git+https://github.com/allenai/allenact.git@main#egg=allenact&subdirectory=allenact\" and, similarly, you can also use pip install -e \"git+https://github.com/allenai/allenact.git@main#egg=allenact_plugins[all]&subdirectory=allenact_plugins\" to install all plugins. Depending on your machine configuration, you may need to use pip3 instead of pip in the commands above. Framework and plugins # To install allenact and all available plugins, run pip install allenact allenact_plugins [ all ] which will install allenact and allenact_plugins packages along with the requirements for all of the plugins (when possible). If you only want to install the requirements for some subset of plugins, you can specify these plugins with the allenact_plugins[plugin1,plugin2] notation. For instance, to install requirements for the ithor_plugin and the minigrid_plugin , simply run: pip install allenact allenact_plugins [ ithor,minigrid ] A list of all available plugins can be found here . Full library # Clone the allenact repository to your local machine and move into the top-level directory git clone git@github.com:allenai/allenact.git cd allenact Below we describe two alternative ways to install all dependencies via pip or conda . Installing requirements with pip # All requirements for allenact (not including plugin requirements) may be installed by running the following command: pip install -r requirements.txt ; pip install -r dev_requirements.txt To install plugin requirements, see below. Plugins extra requirements # To install the specific requirements of each plugin, we need to additionally call pip install -r allenact_plugins/<PLUGIN_NAME>_plugin/extra_requirements.txt from the top-level directory. Installing a conda environment # If you are unfamiliar with Conda, please familiarize yourself with their introductory documentation . If you have not already, you will need to first install Conda (i.e. Anaconda or Miniconda) on your machine. We suggest installing Miniconda as it's relatively lightweight. The conda folder contains YAML files specifying Conda environments compatible with AllenAct. These environment files include: environment-base.yml - A base environment file to be used on all machines (it includes PyTorch with the latest cudatoolkit ). environment-dev.yml - Additional dev dependencies. environment-<CUDA_VERSION>.yml - Additional dependencies, where <CUDA_VERSION> is the CUDA version used on your machine (if you are using linux, you might find this version by running /usr/local/cuda/bin/nvcc --version ). environment-cpu.yml - Additional dependencies to be used on machines where GPU support is not needed (everything will be run on the CPU). For the moment let's assume you're using environment-base.yml above. To install a conda environment with name allenact using this file you can simply run the following ( this will take a few minutes ): conda env create --file ./conda/environment-base.yml --name allenact The above is very simple but has the side effect of creating a new src directory where it will place some of AllenAct's dependencies. To get around this, instead of running the above you can instead run the commands: export MY_ENV_NAME = allenact export CONDA_BASE = \" $( dirname $( dirname \" ${ CONDA_EXE } \" )) \" PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env create --file ./conda/environment-base.yml --name $MY_ENV_NAME These additional commands tell conda to place these dependencies under the ${CONDA_BASE}/envs/${MY_ENV_NAME}/pipsrc directory rather than under src , this is more in line with where we'd expect dependencies to be placed when running pip install ... . If needed, you can use one of the environment-<CUDA_VERSION>.yml environment files to install the proper version of the cudatoolkit by running: conda env update --file ./conda/environment-<CUDA_VERSION>.yml --name allenact or the CPU-only version: conda env update --file ./conda/environment-cpu.yml --name allenact Using the conda environment # Now that you've installed the conda environment as above, you can activate it by running: conda activate allenact after which you can run everything as you would normally. Installing supported environments with conda # Each supported plugin contains a YAML environment file that can be applied upon the existing allenact environment. To install the specific requirements of each plugin, we need to additionally call PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env update --file allenact_plugins/<PLUGIN_NAME>_plugin/extra_environment.yml --name $MY_ENV_NAME from the top-level directory. Habitat: Note that, for habitat, we provide two environment types, regarding whether our machine is connected to a display. More details can be found here .","title":"Install AllenAct"},{"location":"installation/installation-allenact/#installation-of-allenact","text":"Note 1: This library has been tested only in python 3.6 . The following assumes you have a working version of python 3.6 installed locally. Note 2: If you are installing allenact intending to use a GPU for training/inference and your current machine uses an older version of CUDA you may need to manually install the version of PyTorch that supports your CUDA version. In such a case, after installing the below requirements, you should follow the directions for installing PyTorch with older versions of CUDA available on the PyTorch homepage . In order to install allenact and/or its requirements we recommend creating a new python virtual environment and installing all of the below requirements into this virtual environment. Alternatively, we also document how to install a conda environment with all the requirements, which is especially useful if you plan to train models in Habitat .","title":"Installation of AllenAct"},{"location":"installation/installation-allenact/#different-ways-to-use-allenact","text":"There are three main installation paths depending on how you wish to use allenact . You want to use the allenact abstractions and training engine for your own task/environment and don't really care about using any of our plugins that offer additional support (in the form of models, sensors, task samplers, etc.) for select tasks/environments like AI2-THOR, Habitat, and MiniGrid. If this sounds like you, install the standalone framework . You want to use allenact as above but would also like to use some of our additional plugins. If this sounds like you, install the framework and plugins . You want full access to everything in allenact (including all plugins and all of our projects and baselines) and want to have the option to edit the internal implementation of allenact to suit your desire. If this sounds like you, install the full library .","title":"Different ways to use allenact"},{"location":"installation/installation-allenact/#standalone-framework","text":"You can install allenact easily using pip: pip install allenact If you'd like to install the latest development version of allenact (possibly unstable) directly from GitHub see the next section.","title":"Standalone framework"},{"location":"installation/installation-allenact/#bleeding-edge-pip-install","text":"To install the latest allenact framework, you can use pip install -e \"git+https://github.com/allenai/allenact.git@main#egg=allenact&subdirectory=allenact\" and, similarly, you can also use pip install -e \"git+https://github.com/allenai/allenact.git@main#egg=allenact_plugins[all]&subdirectory=allenact_plugins\" to install all plugins. Depending on your machine configuration, you may need to use pip3 instead of pip in the commands above.","title":"Bleeding edge pip install"},{"location":"installation/installation-allenact/#framework-and-plugins","text":"To install allenact and all available plugins, run pip install allenact allenact_plugins [ all ] which will install allenact and allenact_plugins packages along with the requirements for all of the plugins (when possible). If you only want to install the requirements for some subset of plugins, you can specify these plugins with the allenact_plugins[plugin1,plugin2] notation. For instance, to install requirements for the ithor_plugin and the minigrid_plugin , simply run: pip install allenact allenact_plugins [ ithor,minigrid ] A list of all available plugins can be found here .","title":"Framework and plugins"},{"location":"installation/installation-allenact/#full-library","text":"Clone the allenact repository to your local machine and move into the top-level directory git clone git@github.com:allenai/allenact.git cd allenact Below we describe two alternative ways to install all dependencies via pip or conda .","title":"Full library"},{"location":"installation/installation-allenact/#installing-requirements-with-pip","text":"All requirements for allenact (not including plugin requirements) may be installed by running the following command: pip install -r requirements.txt ; pip install -r dev_requirements.txt To install plugin requirements, see below.","title":"Installing requirements with pip"},{"location":"installation/installation-allenact/#plugins-extra-requirements","text":"To install the specific requirements of each plugin, we need to additionally call pip install -r allenact_plugins/<PLUGIN_NAME>_plugin/extra_requirements.txt from the top-level directory.","title":"Plugins extra requirements"},{"location":"installation/installation-allenact/#installing-a-conda-environment","text":"If you are unfamiliar with Conda, please familiarize yourself with their introductory documentation . If you have not already, you will need to first install Conda (i.e. Anaconda or Miniconda) on your machine. We suggest installing Miniconda as it's relatively lightweight. The conda folder contains YAML files specifying Conda environments compatible with AllenAct. These environment files include: environment-base.yml - A base environment file to be used on all machines (it includes PyTorch with the latest cudatoolkit ). environment-dev.yml - Additional dev dependencies. environment-<CUDA_VERSION>.yml - Additional dependencies, where <CUDA_VERSION> is the CUDA version used on your machine (if you are using linux, you might find this version by running /usr/local/cuda/bin/nvcc --version ). environment-cpu.yml - Additional dependencies to be used on machines where GPU support is not needed (everything will be run on the CPU). For the moment let's assume you're using environment-base.yml above. To install a conda environment with name allenact using this file you can simply run the following ( this will take a few minutes ): conda env create --file ./conda/environment-base.yml --name allenact The above is very simple but has the side effect of creating a new src directory where it will place some of AllenAct's dependencies. To get around this, instead of running the above you can instead run the commands: export MY_ENV_NAME = allenact export CONDA_BASE = \" $( dirname $( dirname \" ${ CONDA_EXE } \" )) \" PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env create --file ./conda/environment-base.yml --name $MY_ENV_NAME These additional commands tell conda to place these dependencies under the ${CONDA_BASE}/envs/${MY_ENV_NAME}/pipsrc directory rather than under src , this is more in line with where we'd expect dependencies to be placed when running pip install ... . If needed, you can use one of the environment-<CUDA_VERSION>.yml environment files to install the proper version of the cudatoolkit by running: conda env update --file ./conda/environment-<CUDA_VERSION>.yml --name allenact or the CPU-only version: conda env update --file ./conda/environment-cpu.yml --name allenact","title":"Installing a conda environment"},{"location":"installation/installation-allenact/#using-the-conda-environment","text":"Now that you've installed the conda environment as above, you can activate it by running: conda activate allenact after which you can run everything as you would normally.","title":"Using the conda environment"},{"location":"installation/installation-allenact/#installing-supported-environments-with-conda","text":"Each supported plugin contains a YAML environment file that can be applied upon the existing allenact environment. To install the specific requirements of each plugin, we need to additionally call PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env update --file allenact_plugins/<PLUGIN_NAME>_plugin/extra_environment.yml --name $MY_ENV_NAME from the top-level directory. Habitat: Note that, for habitat, we provide two environment types, regarding whether our machine is connected to a display. More details can be found here .","title":"Installing supported environments with conda"},{"location":"installation/installation-framework/","text":"Installation of supported environments # In general, each supported environment can be installed by just following the instructions to install the full library and specific requirements of every plugin either via pip or via Conda . Below we provide additional installation instructions for a number of environments that we support and provide some guidance for problems commonly experienced when using these environments. Installation of iTHOR ( ithor plugin) # The first time you will run an experiment with iTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes automatically. However, the datasets must be manually downloaded as described here . Trying to use iTHOR on a machine without an attached display? Note: These instructions assume you have installed the full library . If you wish to run iTHOR on a machine without an attached display (for instance, a remote server such as an AWS machine) you will also need to run a script that launches xserver processes on your GPUs. This can be done with the following command: sudo python scripts/startx.py & Notice that you need to run the command with sudo (i.e. administrator privileges). If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine). Installation of RoboTHOR ( robothor plugin) # RoboTHOR is installed in the same way as iTHOR . For more information see the above section on installing iTHOR . Installation of Habitat # Installing habitat requires Installing the habitat-lab and habitat-sim packages. This may be done by either following the directions provided by Habitat themselves or by using our conda installation instructions below. Downloading the scene assets (i.e. the Gibson or Matterport scene files) relevant to whichever task you're interested in. Unfortunately we cannot legally distribute these files to you directly. Instead you will need to download these yourself. See here for how you can download the Gibson files and here for directions on how to download the Matterport flies. Downloading the dataset files for the task you're interested in (e.g. PointNav, ObjectNav, etc). See here for links to these dataset files. Using conda # Habitat has recently released the option to install their simulator using conda which avoids having to manually build dependencies or use Docker. This does not guarantee that the installation process is completely painless (it is difficult to avoid all possible build issues) but we've found it to be a nice alternative to using Docker. To use this installation option please first install an AllenAct conda environment using the instructions available here . After installing this environment, you can then install habitat-sim and habitat-lab by running: If you are on a machine with an attached display: export MY_ENV_NAME = allenact export CONDA_BASE = \" $( dirname $( dirname \" ${ CONDA_EXE } \" )) \" PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env update --file allenact_plugins/habitat_plugin/extra_environment.yml --name $MY_ENV_NAME If you are on a machine without an attached display (e.g. a server), replace the last command by: PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env update --file allenact_plugins/habitat_plugin/extra_environment_headless.yml --name $MY_ENV_NAME After these steps, feel free to proceed to download the required scene assets and task-specific dataset files as described above.","title":"Install environments"},{"location":"installation/installation-framework/#installation-of-supported-environments","text":"In general, each supported environment can be installed by just following the instructions to install the full library and specific requirements of every plugin either via pip or via Conda . Below we provide additional installation instructions for a number of environments that we support and provide some guidance for problems commonly experienced when using these environments.","title":"Installation of supported environments"},{"location":"installation/installation-framework/#installation-of-ithor-ithor-plugin","text":"The first time you will run an experiment with iTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes automatically. However, the datasets must be manually downloaded as described here . Trying to use iTHOR on a machine without an attached display? Note: These instructions assume you have installed the full library . If you wish to run iTHOR on a machine without an attached display (for instance, a remote server such as an AWS machine) you will also need to run a script that launches xserver processes on your GPUs. This can be done with the following command: sudo python scripts/startx.py & Notice that you need to run the command with sudo (i.e. administrator privileges). If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine).","title":"Installation of iTHOR (ithor plugin)"},{"location":"installation/installation-framework/#installation-of-robothor-robothor-plugin","text":"RoboTHOR is installed in the same way as iTHOR . For more information see the above section on installing iTHOR .","title":"Installation of RoboTHOR (robothor plugin)"},{"location":"installation/installation-framework/#installation-of-habitat","text":"Installing habitat requires Installing the habitat-lab and habitat-sim packages. This may be done by either following the directions provided by Habitat themselves or by using our conda installation instructions below. Downloading the scene assets (i.e. the Gibson or Matterport scene files) relevant to whichever task you're interested in. Unfortunately we cannot legally distribute these files to you directly. Instead you will need to download these yourself. See here for how you can download the Gibson files and here for directions on how to download the Matterport flies. Downloading the dataset files for the task you're interested in (e.g. PointNav, ObjectNav, etc). See here for links to these dataset files.","title":"Installation of Habitat"},{"location":"installation/installation-framework/#using-conda","text":"Habitat has recently released the option to install their simulator using conda which avoids having to manually build dependencies or use Docker. This does not guarantee that the installation process is completely painless (it is difficult to avoid all possible build issues) but we've found it to be a nice alternative to using Docker. To use this installation option please first install an AllenAct conda environment using the instructions available here . After installing this environment, you can then install habitat-sim and habitat-lab by running: If you are on a machine with an attached display: export MY_ENV_NAME = allenact export CONDA_BASE = \" $( dirname $( dirname \" ${ CONDA_EXE } \" )) \" PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env update --file allenact_plugins/habitat_plugin/extra_environment.yml --name $MY_ENV_NAME If you are on a machine without an attached display (e.g. a server), replace the last command by: PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env update --file allenact_plugins/habitat_plugin/extra_environment_headless.yml --name $MY_ENV_NAME After these steps, feel free to proceed to download the required scene assets and task-specific dataset files as described above.","title":"Using conda"},{"location":"notebooks/firstbook/","text":"To-do #","title":"To-do"},{"location":"notebooks/firstbook/#to-do","text":"","title":"To-do"},{"location":"projects/advisor_2020/","text":"Experiments for Advisor # TODO: # Add details taken from https://unnat.github.io/advisor/. Cite the arxiv paper. Give a list of things you can run with bash commands. Ideally be able to recreate a large set of experiments.","title":"Experiments for Advisor"},{"location":"projects/advisor_2020/#experiments-for-advisor","text":"","title":"Experiments for Advisor"},{"location":"projects/advisor_2020/#todo","text":"Add details taken from https://unnat.github.io/advisor/. Cite the arxiv paper. Give a list of things you can run with bash commands. Ideally be able to recreate a large set of experiments.","title":"TODO:"},{"location":"projects/babyai_baselines/","text":"Baseline experiments for the BabyAI environment # We perform a collection of baseline experiments within the BabyAI environment on the GoToLocal task, see the projects/babyai_baselines/experiments/go_to_local directory. For instance, to train a model using PPO, run python main.py go_to_local.ppo --experiment_base projects/babyai_baselines/experiments Note that these experiments will be quite slow when not using a GPU as the BabyAI model architecture is surprisingly large. Specifying a GPU (if available) can be done from the command line using hooks we created using gin-config . E.g. to train using the 0th GPU device, add --gp \"machine_params.gpu_id = 0\" to the above command.","title":"BabyAI baselines"},{"location":"projects/babyai_baselines/#baseline-experiments-for-the-babyai-environment","text":"We perform a collection of baseline experiments within the BabyAI environment on the GoToLocal task, see the projects/babyai_baselines/experiments/go_to_local directory. For instance, to train a model using PPO, run python main.py go_to_local.ppo --experiment_base projects/babyai_baselines/experiments Note that these experiments will be quite slow when not using a GPU as the BabyAI model architecture is surprisingly large. Specifying a GPU (if available) can be done from the command line using hooks we created using gin-config . E.g. to train using the 0th GPU device, add --gp \"machine_params.gpu_id = 0\" to the above command.","title":"Baseline experiments for the BabyAI environment"},{"location":"projects/ithor_rearrangement/","text":"AI2-THOR Rearrangement Challenge Welcome to the 2021 AI2-THOR Rearrangement Challenge hosted at the CVPR'21 Embodied-AI Workshop . The goal of this challenge is to build a model/agent that move objects in a room to restore them to a given initial configuration. Please follow the instructions below to get started. If you have any questions please file an issue or post in the #rearrangement-challenge channel on our Ask PRIOR slack . Contents # \ud83d\udcbb Installation \ud83d\udcdd Rearrangement Task Description \ud83d\udee4\ufe0f Challenge Tracks and Datasets \u261d\ufe0f+\u270c\ufe0f The 1- and 2-Phase Tracks \ud83d\udcca Datasets \ud83d\udee4\ufe0f Submitting to the Leaderboard \ud83d\uddbc\ufe0f Allowed Observations \ud83c\udfc3 Allowed Actions \ud83c\udf7d\ufe0f Setting up Rearrangement \ud83c\udf7d\ufe0f Setting up Rearrangement \u2728 Learning by example \ud83c\udf0e The Rearrange THOR Environment class \ud83c\udfd2 The Rearrange Task Sampler class \ud83d\udeb6\ud83d\udd00 The Walkthrough Task and Unshuffle Task classes \ud83d\uddfa\ufe0f Object Poses \ud83c\udfc6 Evaluation \ud83d\udccf When are poses (approximately) equal? \ud83d\udcaf Computing metrics \ud83c\udfcb Training Baseline Models with AllenAct \ud83d\udcaa Pretrained Models \ud83d\udcc4 Citation \ud83d\udcbb Installation # To begin, clone this repository locally git clone git@github.com:allenai/ai2thor-rearrangement.git See here for a summary of the most important files/directories in this repository Here's a quick summary of the most important files/directories in this repository: * `example.py` an example script showing how rearrangement tasks can be instantiated for training and validation. * `baseline_configs/` - `rearrange_base.py` The base configuration file which defines the challenge parameters (e.g. screen size, allowed actions, etc). - `one_phase/*.py` - Baseline experiment configurations for the 1-phase challenge track. - `two_phase/*.py` - Baseline experiment configurations for the 2-phase challenge track. - `walkthrough/*.py` - Baseline experiment configurations if one wants to train the walkthrough phase in isolation. * `rearrange/` - `baseline_models.py` - A collection of baseline models for the 1- and 2-phase challenge tasks. These Actor-Critic models use a CNN->RNN architecture and can be trained using the experiment configs under the `baseline_configs/[one/two]_phase/` directories. - `constants.py` - Constants used to define the rearrangement task. These include the step size taken by the agent, the unique id of the the THOR build we use, etc. - `environment.py` - The definition of the `RearrangeTHOREnvironment` class that wraps the AI2-THOR environment and enables setting up rearrangement tasks. - `expert.py` - The definition of a heuristic expert (`GreedyUnshuffleExpert`) which uses privileged information (e.g. the scene graph & knowledge of exact object poses) to solve the rearrangement task. This heuristic expert is meant to be used to produce expert actions for use with imitation learning techinques. See the `query_expert` method within the `rearrange.tasks.UnshuffleTask` class for an example of how such an action can be generated. - `losses.py` - Losses (outside of those provided by AllenAct by default) used to train our baseline agents. - `sensors.py` - Sensors which provide observations to our agents during training. E.g. the `RGBRearrangeSensor` obtains RGB images from the environment and returns them for use by the agent. - `tasks.py` - Definitions of the `UnshuffleTask`, `WalkthroughTask`, and `RearrangeTaskSampler` classes. For more information on how these are used, see the [Setting up Rearrangement](#%EF%B8%8F-setting-up-rearrangement) section. - `utils.py` - Standalone utility functions (e.g. computing IoU between 3D bounding boxes). You can then install requirements by running pip install -r requirements.txt or, if you prefer using conda, we can create a thor-rearrange environment with our requirements by running export MY_ENV_NAME = thor-rearrange export CONDA_BASE = \" $( dirname $( dirname \" ${ CONDA_EXE } \" )) \" PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env create --file environment.yml --name $MY_ENV_NAME Why not just run conda env create --file environment.yml --name thor-rearrange by itself? If you were to run `conda env create --file environment.yml --name thor-rearrange` nothing would break but we have some pip requirements in our `environment.yml` file and, by default, these are saved in a local `./src` directory. By explicitly specifying the `PIP_SRC` variable we can have it place these pip-installed packages in a nicer (more hidden) location. Python 3.6+ \ud83d\udc0d. Each of the actions supports typing within Python . AI2-THOR 2.7.2 \ud83e\uddde. To ensure reproducible results, we're restricting all users to use the exact same version of AI2-THOR . AllenAct \ud83c\udfcb\ud83d\udcaa. We ues the AllenAct reinforcement learning framework for generating baseline models, baseline training pipelines, and for several of their helpful abstractions/utilities. SciPy \ud83e\uddd1\u200d\ud83d\udd2c. We utilize SciPy for evaluation. It helps calculate the IoU between 3D bounding boxes. \ud83d\udcdd Rearrangement Task Description # Overview \ud83e\udd16. Our rearrangement task involves moving and modifying (i.e. opening/closing) randomly placed objects within a room to obtain a goal configuration. There are 2 phases: Walkthrough \ud83d\udc40. The agent walks around the room and observes the objects in their ideal goal state. Unshuffle \ud83c\udfcb. After the walkthrough phase, we randomly change between 1 to 5 objects in the room. The agent's goal is to identify which objects have changed and reset those objects to their state from the walkthrough phase. Changes to an object's state may include changes to its position, orientation, or openness. \ud83d\udee4\ufe0f Challenge Tracks and Datasets # \u261d\ufe0f+\u270c\ufe0f The 1- and 2-Phase Tracks # For this 2021 challenge we have two distinct tracks: 1-Phase Track (Easier). In this track we merge both of the above phases into a single phase. At every step the agent obtains observations from the walkthrough (goal) state as well as the shuffled state. This allows the agent to directly compare aligned images from the two world-states and thus makes it much easier to determine if an object is, or is not, in its goal pose. 2-Phase Track (Harder). In this track, the walkthrough and unshuffle phases occur sequentially and so, once in the unshuffle phase, the agent no longer has any access to the walkthrough state except through any memory it has saved. \ud83d\udcca Datasets # For this challenge we have four distinct dataset splits: \"train\" , \"train_unseen\" , \"val\" , and \"test\" . The train and train_unseen splits use floor plans 1-20, 200-220, 300-320, and 400-420 within AI2-THOR, the \"val\" split uses floor plans 21-25, 221-225, 321-325, and 421-425, and finally the \"test\" split uses scenes 26-30, 226-230, 326-330, and 426-430. These dataset splits are stored as the compressed pickle -serialized files data/*.pkl.gz . While you are freely (and encouraged) to enhance the training set as you see fit, you should never train your agent within any of the test scenes. For evaluation, your model will need to be evaluated on each of the above splits and the results submitted to our leaderboard link (see section below). As the \"train\" and \"train_unseen\" sets are quite large, we do not expect you to evaluate on their entirety. Instead we select ~1000 datapoints from each of these sets for use in evaluation. For convenience, we provide the data/combined.pkl.gz file which contains the \"train\" , \"train_unseen\" , \"val\" , and \"test\" datapoints that should be used for evaluation. Split # Total Episodes # Episodes for Eval Path train 4000 1200 data/train.pkl.gz train_unseen 3800 1140 data/train_unseen.pkl.gz val 1000 1000 data/val.pkl.gz test 1000 1000 data/test.pkl.gz combined 4340 4340 data/combined.pkl.gz \ud83d\udee4\ufe0f Submitting to the Leaderboard # We are tracking challenge participant entries using the AI2 Leaderboard . Submissions can be made to the 1-phase leaderboard here and submissions to the 2-phase leaderboard can be made here . Submissions should include your agent's trajectories for all tasks contained within the combined.pkl.gz dataset, this \"combined\" dataset includes tasks for the train, train_unseen, validation, and test sets. For an example as to how to iterate through all the datapoints in this dataset and save the resulting metrics in our expected submission format see here . A (full) example the expected submission format for the 1-phase task can be found here and, for the 2-phase task, can be found here . Note that this submission format is a gzip'ed json file where the json file has the form { \"UNIQUE_ID_OF_TASK_0\": YOUR_AGENTS_METRICS_AND_TRAJECTORY_FOR_TASK_0, \"UNIQUE_ID_OF_TASK_1\": YOUR_AGENTS_METRICS_AND_TRAJECTORY_FOR_TASK_1, ... } these metrics and unique IDs can be easily obtained when iterating over the dataset (see the above example). Alternatively: if you run inference on the combined dataset using AllenAct (see below for more details) then you can simply (1) gzip the metrics*.json file saved when running inference, (2) rename this file submission.json.gz , and (3) submit this file to the leaderboard directly. \ud83d\uddbc\ufe0f Allowed Observations # In both of these tracks, agents should make decisions based off of egocentric sensor readings. The types of sensors allowed/provided for this challenge include: RGB images - having shape 224x224x3 and an FOV of 90 degrees. Depth maps - having shape 224x224 and an FOV of 90 degrees. Perfect egomotion - We allow for agents to know precisely how far (and in which direction) they have moved as well as how many degrees they have rotated. While you are absolutely free to use any sensor information you would like during training (e.g. pretraining your CNN using semantic segmentations from AI2-THOR or using a scene graph to compute expert actions for imitation learning) such additional sensor information should not be used at inference time. \ud83c\udfc3 Allowed Actions # A total of 82 actions are available to our agents, these include: Navigation * Move[Ahead/Left/Right/Back] - Results in the agent moving 0.25m in the specified direction if doing so would not result in the agent colliding with something. Rotate[Right/Left] - Results in the agent rotating 90 degrees clockwise (if Right ) or counterclockwise (if Left ). This action may fail if the agent is holding an object and rotating would cause the object to collide. Look[Up/Down] - Results in the agent raising or lowering its camera angle by 30 degrees (up to a max of 60 degrees below horizontal and 30 degrees above horizontal). Object Interaction Pickup[OBJECT_TYPE] - Where OBJECT_TYPE is one of the 62 pickupable object types, see constants.py . This action results in the agent picking up a visible object of type OBJECT_TYPE if: (a) the agent is not already holding an object, (b) the agent is close enough to the object (within 1.5m), and picking up the object would not result in it colliding with objects in front of the agent. If there are multiple objects of type OBJECT_TYPE then the object closest to the agent is chosen. Open[OBJECT_TYPE] - Where OBJECT_TYPE is one of the 10 opennable object types that are not also pickupable, see constants.py . If an object whose openness is different from the openness in the goal state is visible and within 1.5m of the agent, this object's openness is changed to its value in the goal state. PlaceObject - Results in the agent dropping its held object. If the held object's goal state is visible and within 1.5m of the agent, it is placed into that goal state. Otherwise, a heuristic is used to place the object on a nearby surface. Done action Done - Results in the walkthrough or unshuffle phase immediately terminating. \ud83c\udf7d\ufe0f Setting up Rearrangement # \u2728 Learning by example # See the example.py file for an example of how you can instantiate the 1- and 2-phase variants of our rearrangement task. \ud83c\udf0e The Rearrange THOR Environment class # The rearrange.environment.RearrangeTHOREnvironment class provides a wrapper around the AI2-THOR environment and is designed to 1. Make it easy to set up a AI2-THOR scene in a particular state ready for rearrangement. 1. Provides utilities to make it easy to evaluate (see e.g. the poses and compare_poses methods) how close the current state of the environment is to the goal state. 1. Provide an API with which the agent may interact with the environment. \ud83c\udfd2 The Rearrange Task Sampler class # You'll notice that the above RearrangeTHOREnvironment is not explicitly instantiated by the example.py script and, instead, we create rearrange.tasks.RearrangeTaskSampler objects using the TwoPhaseRGBBaseExperimentConfig.make_sampler_fn and OnePhaseRGBBaseExperimentConfig.make_sampler_fn . This is because the RearrangeTHOREnvironment is very flexible and doesn't know anything about training/validation/test datasets, the types of actions we want our agent to be restricted to use, or precisely which types of sensor observations we want to give our agents (e.g. RGB images, depth maps, etc). All of these extra details are managed by the RearrangeTaskSampler which iteratively creates new tasks for our agent to complete when calling the next_task method. During training, these new tasks can be sampled indefinitely while, during validation or testing, the tasks will only be sampled until the validation/test datasets are exhausted. This sampling is best understood by example so please go over the example.py file. \ud83d\udeb6\ud83d\udd00 The Walkthrough Task and Unshuffle Task classes # As described above, the RearrangeTaskSampler samples tasks for our agent to complete, these tasks correspond to instantiations of the rearrange.tasks.WalkthroughTask and rearrange.tasks.UnshuffleTask classes. For the 2-phase challenge track, the RearrangeTaskSampler will first sample a new WalkthroughTask after which it will sample a corresponding UnshuffleTask where the agent must return the objects to their poses at the start of the WalkthroughTask . \ud83d\uddfa\ufe0f Object Poses # Accessing object poses \ud83e\uddd8. The poses of all objects in the environment can be accessed using the RearrangeTHOREnvironment.poses property, i.e. unshuffle_start_poses , walkthrough_start_poses , current_poses = env . poses # where env is an RearrangeTHOREnvironment instance Reading an object's pose \ud83d\udcd6. Here, unshuffle_start_poses , walkthrough_start_poses , and current_poses evaluate to a list of dictionaries and are defined as: unshuffle_start_poses stores a list of object poses if the agent were to do nothing to the env during the unshuffling phase. walkthrough_start_poses stores a list of object poses that the agent sees during the walkthrough phase. current_poses stores a list of object poses in the current state of the environment (i.e. possibly after the unshuffle agent makes all its changes to the env during the unshuffling phase). Each dictionary contains the object's pose in a form similar to: { \"type\" : \"Candle\" , \"position\" : { \"x\" : - 0.3012670874595642 , \"y\" : 0.7431036233901978 , \"z\" : - 2.040205240249634 }, \"rotation\" : { \"x\" : 2.958569288253784 , \"y\" : 0.027708930894732475 , \"z\" : 0.6745457053184509 }, \"openness\" : None , \"pickupable\" : True , \"broken\" : False , \"objectId\" : \"Candle|-00.30|+00.74|-02.04\" , \"name\" : \"Candle_977f7f43\" , \"parentReceptacles\" : [ \"Bathtub|-01.28|+00.28|-02.53\" ], \"bounding_box\" : [ [ - 0.27043721079826355 , 0.6975823640823364 , - 2.0129783153533936 ], [ - 0.3310248851776123 , 0.696869969367981 , - 2.012985944747925 ], [ - 0.3310534358024597 , 0.6999208927154541 , - 2.072017192840576 ], [ - 0.27046576142311096 , 0.7006332278251648 , - 2.072009563446045 ], [ - 0.272365003824234 , 0.8614493608474731 , - 2.0045082569122314 ], [ - 0.3329526484012604 , 0.8607369661331177 , - 2.0045158863067627 ], [ - 0.3329811990261078 , 0.8637878894805908 , - 2.063547134399414 ], [ - 0.27239352464675903 , 0.8645002245903015 , - 2.063539505004883 ] ] } Matching objects across poses \ud83e\udd1d. Across unshuffle_start_poses , walkthrough_start_poses , and current_poses , the ith entry in each list will always correspond to the same object across each pose list. So, unshuffle_start_poses[5] will refer to the same object as walkthrough_start_poses[5] and current_poses[5] . Most scenes have around 70 objects, among which, 10 to 20 are pickupable by the agent. Pose keys \ud83d\udd11. openness specifies the [0:1] percentage that an object is opened. For objects where the openness value does not fit (e.g., Bowl , Spoon ), the openness value is None . bounding_box is only given for moveable objects, where the set of moveable objects may consist of couches or chairs, that are not necessarily pickupable. For pickupable objects, the bounding_box is aligned to the object's relative axes. For moveable objects that are non-pickupable, the object is aligned to the global axes. broken states if the object broke from the agent's actions during the unshuffling phase. The initial pose or goal pose for each object will never be broken. But, if the agent decides to pick up an object, and drop it on a hard surface, it's possible that the object can break. \ud83c\udfc6 Evaluation # To evaluate the quality of a rearrangement agent we compute several metrics measuring how well the agent has managed to move objects so that their final poses are (approximately) equal to their goal poses. \ud83d\udccf When are poses (approximately) equal? # Recall that we represent the pose of an object as a combination of its: 1. Openness \ud83d\udcd6. - A value in [0,1] which measures how far the object has been opened. 2. Position \ud83d\udccd, Rotation \ud83d\ude43, and bounding box \ud83d\udce6 - The 3D position, rotation, and bounding box of each object. 3. Broken - A boolean indicating if the object has been broken (all goal object poses are unbroken). The openness between its goal state and predicted state is off by less than 20 percent. The openness check is only applied to objects that can open. The object's 3D bounding box from its goal pose and the predicted pose must have an IoU over 0.5. The positional check is only relevant to objects that can move. To measure if two object poses are approximately equal we use the following criterion: 1. \u274c If any object pose is broken. 1. \u274c If the object is opennable but not pickupable (e.g. a cabinet) and the the openness values between the two poses differ by more than 0.2. 1. \u274c The two 3D bounding boxes of pickupable objects have an IoU under 0.5. 1. \u2714\ufe0f None of the above criteria are met so the poses are not broken, are close in openness values, and have sufficiently high IoU. \ud83d\udcaf Computing metrics # Suppose that task is an instance of an UnshuffleTask which your agent has taken actions until reaching a terminal state (e.g. either the agent has taken the maximum number of steps or it has taken the \"done\" action). Then metrics regarding the agent's performance can be computed by calling the task.metrics() function. This will return a dictionary of the form { \"task_info\" : { \"scene\" : \"FloorPlan420\" , \"index\" : 7 , \"stage\" : \"train\" }, \"ep_length\" : 176 , \"unshuffle/ep_length\" : 7 , \"unshuffle/reward\" : 0.5058389582634852 , \"unshuffle/start_energy\" : 0.5058389582634852 , \"unshuffle/end_energy\" : 0.0 , \"unshuffle/prop_fixed\" : 1.0 , \"unshuffle/prop_fixed_strict\" : 1.0 , \"unshuffle/num_misplaced\" : 0 , \"unshuffle/num_newly_misplaced\" : 0 , \"unshuffle/num_initially_misplaced\" : 1 , \"unshuffle/num_fixed\" : 1 , \"unshuffle/num_broken\" : 0 , \"unshuffle/change_energy\" : 0.5058464936498058 , \"unshuffle/num_changed\" : 1 , \"unshuffle/prop_misplaced\" : 0.0 , \"unshuffle/energy_prop\" : 0.0 , \"unshuffle/success\" : 0.0 , \"walkthrough/ep_length\" : 169 , \"walkthrough/reward\" : 1.82 , \"walkthrough/num_explored_xz\" : 17 , \"walkthrough/num_explored_xzr\" : 46 , \"walkthrough/prop_visited_xz\" : 0.5151515151515151 , \"walkthrough/prop_visited_xzr\" : 0.3484848484848485 , \"walkthrough/num_obj_seen\" : 11 , \"walkthrough/prop_obj_seen\" : 0.9166666666666666 } Of the above metrics, the most important (those used for comparing models) are * Success rate ( \"unshuffle/success\" ) - This is the most unforgiving of our metrics and equals 1 if all object poses are in their goal states after the unshuffle phase. * % Misplaced ( \"unshuffle/prop_misplaced\" ) - The above sucess metric is quite strict, requiring exact rearrangement of all objects, and also does not additionally penalize an agent for moving objects that should not be moved. This metric equals the number of misplaced objects at the end of the episode divided by the number of misplaced objects at the start of the episode. Note that this metric can be larger than 1 if the agent, during the unshuffle stage, misplaces more objects than were originally misplaced at the start. * % Fixed Strict ( \"unshuffle/prop_fixed_strict\" ) - This metric equals 0 if, at the end of the unshuffle task, the agent has misplaced any new objects (i.e. it has incorrectly moved an object that started in its correct position). Otherwise, if it has not misplaced new objects, then this equals (# objects which started in the wrong pose but are now in the correct pose) / (# objects which started in an incorrect pose), i.e. the proportion of objects who had their pose fixed . * % Energy Remaining ( \"unshuffle/energy_prop\" ) - The above metrics do not give any partial credit if, for example, the agent moves an object across a room and towards its goal pose but fails to place it so that has a sufficiently high IOU with the goal. To allow for partial credit, we define an energy function D that monotonically decreases to 0 as two poses get closer together (see code for full details) and which equals zero if two poses are approximately equal. This metric is then defined as the amount of energy remaining at the end of the unshuffle episode divided by the total energy at the start of the unshuffle episode, i.e. equals (sum of energy between all goal/current object poses at end of the unshuffle phase) / (sum of energy between all goal/current object poses at the start of the unshuffle phase). \ud83c\udfcb Training Baseline Models with AllenAct # We use the AllenAct framework for training our baseline rearrangement models, this framework is automatically installed when installing the requirements for this project . Before running training or inference you'll first have to add the ai2thor-rearrangement directory to your PYTHONPATH (so that python and AllenAct knows where to for various modules). To do this you can run the following: cd YOUR/PATH/TO/ai2thor-rearrangement export PYTHONPATH = $PYTHONPATH : $PWD Let's say you want to train a model for the 1-phase challenge. This can be easily done by running the command allenact -o rearrange_out -b . baseline_configs/one_phase/one_phase_rgb_resnet_dagger.py This will train (using DAgger, a form of imitation learning) a model which uses a pretrained (with frozen weights) ResNet18 as the visual backbone that feeds into a recurrent neural network (a GRU) before producing action probabilities and a value estimate. Results from this training are then saved to rearrange_out where you can find model checkpoints, tensorboard plots, and configuration files that can be used if you, in the future, forget precisely what the details of your experiment were. A similar model can be trained for the 2-phase challenge by running allenact -o rearrange_out -b . baseline_configs/two_phase/two_phase_rgb_resnet_ppowalkthrough_ilunshuffle.py \ud83d\udcaa Pretrained Models # We currently provide the following pretrained models (see our paper for details on these models): Model % Fixed Strict (Test) Pretrained Model 1-Phase ResNet18 IL 6.3% (link) 1-Phase ResNet18 PPO 5.3% (link) 1-Phase Simple IL 4.8% (link) 1-Phase Simple PPO 4.6% (link) 2-Phase ResNet18 IL+PPO 0.66% (link) These models can be downloaded at from the above links and should be placed into the pretrained_model_ckpts directory. You can then, for example, run inference for the 1-Phase ResNet18 IL model using AllenAct by running: export CURRENT_TIME = $( date '+%Y-%m-%d_%H-%M-%S' ) # This is just to record when you ran this inference allenact baseline_configs/one_phase/one_phase_rgb_resnet_dagger.py \\ -c pretrained_model_ckpts/exp_OnePhaseRGBResNetDagger_40proc__stage_00__steps_000050058550.pt \\ --extra_tag $CURRENT_TIME \\ --eval this will evaluate this model across all datapoints in the data/combined.pkl.gz dataset which contains data from the train , train_unseen , val , and test sets so that evaluation doesn't have to be run on each set separately. \ud83d\udcc4 Citation # If you use this work, please cite our paper (to appear in CVPR'21): @InProceedings { RoomR , author = {Luca Weihs and Matt Deitke and Aniruddha Kembhavi and Roozbeh Mottaghi} , title = {Visual Room Rearrangement} , booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)} , month = {June} , year = {2021} }","title":"Index"},{"location":"projects/ithor_rearrangement/#contents","text":"\ud83d\udcbb Installation \ud83d\udcdd Rearrangement Task Description \ud83d\udee4\ufe0f Challenge Tracks and Datasets \u261d\ufe0f+\u270c\ufe0f The 1- and 2-Phase Tracks \ud83d\udcca Datasets \ud83d\udee4\ufe0f Submitting to the Leaderboard \ud83d\uddbc\ufe0f Allowed Observations \ud83c\udfc3 Allowed Actions \ud83c\udf7d\ufe0f Setting up Rearrangement \ud83c\udf7d\ufe0f Setting up Rearrangement \u2728 Learning by example \ud83c\udf0e The Rearrange THOR Environment class \ud83c\udfd2 The Rearrange Task Sampler class \ud83d\udeb6\ud83d\udd00 The Walkthrough Task and Unshuffle Task classes \ud83d\uddfa\ufe0f Object Poses \ud83c\udfc6 Evaluation \ud83d\udccf When are poses (approximately) equal? \ud83d\udcaf Computing metrics \ud83c\udfcb Training Baseline Models with AllenAct \ud83d\udcaa Pretrained Models \ud83d\udcc4 Citation","title":"Contents"},{"location":"projects/ithor_rearrangement/#installation","text":"To begin, clone this repository locally git clone git@github.com:allenai/ai2thor-rearrangement.git See here for a summary of the most important files/directories in this repository Here's a quick summary of the most important files/directories in this repository: * `example.py` an example script showing how rearrangement tasks can be instantiated for training and validation. * `baseline_configs/` - `rearrange_base.py` The base configuration file which defines the challenge parameters (e.g. screen size, allowed actions, etc). - `one_phase/*.py` - Baseline experiment configurations for the 1-phase challenge track. - `two_phase/*.py` - Baseline experiment configurations for the 2-phase challenge track. - `walkthrough/*.py` - Baseline experiment configurations if one wants to train the walkthrough phase in isolation. * `rearrange/` - `baseline_models.py` - A collection of baseline models for the 1- and 2-phase challenge tasks. These Actor-Critic models use a CNN->RNN architecture and can be trained using the experiment configs under the `baseline_configs/[one/two]_phase/` directories. - `constants.py` - Constants used to define the rearrangement task. These include the step size taken by the agent, the unique id of the the THOR build we use, etc. - `environment.py` - The definition of the `RearrangeTHOREnvironment` class that wraps the AI2-THOR environment and enables setting up rearrangement tasks. - `expert.py` - The definition of a heuristic expert (`GreedyUnshuffleExpert`) which uses privileged information (e.g. the scene graph & knowledge of exact object poses) to solve the rearrangement task. This heuristic expert is meant to be used to produce expert actions for use with imitation learning techinques. See the `query_expert` method within the `rearrange.tasks.UnshuffleTask` class for an example of how such an action can be generated. - `losses.py` - Losses (outside of those provided by AllenAct by default) used to train our baseline agents. - `sensors.py` - Sensors which provide observations to our agents during training. E.g. the `RGBRearrangeSensor` obtains RGB images from the environment and returns them for use by the agent. - `tasks.py` - Definitions of the `UnshuffleTask`, `WalkthroughTask`, and `RearrangeTaskSampler` classes. For more information on how these are used, see the [Setting up Rearrangement](#%EF%B8%8F-setting-up-rearrangement) section. - `utils.py` - Standalone utility functions (e.g. computing IoU between 3D bounding boxes). You can then install requirements by running pip install -r requirements.txt or, if you prefer using conda, we can create a thor-rearrange environment with our requirements by running export MY_ENV_NAME = thor-rearrange export CONDA_BASE = \" $( dirname $( dirname \" ${ CONDA_EXE } \" )) \" PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env create --file environment.yml --name $MY_ENV_NAME Why not just run conda env create --file environment.yml --name thor-rearrange by itself? If you were to run `conda env create --file environment.yml --name thor-rearrange` nothing would break but we have some pip requirements in our `environment.yml` file and, by default, these are saved in a local `./src` directory. By explicitly specifying the `PIP_SRC` variable we can have it place these pip-installed packages in a nicer (more hidden) location. Python 3.6+ \ud83d\udc0d. Each of the actions supports typing within Python . AI2-THOR 2.7.2 \ud83e\uddde. To ensure reproducible results, we're restricting all users to use the exact same version of AI2-THOR . AllenAct \ud83c\udfcb\ud83d\udcaa. We ues the AllenAct reinforcement learning framework for generating baseline models, baseline training pipelines, and for several of their helpful abstractions/utilities. SciPy \ud83e\uddd1\u200d\ud83d\udd2c. We utilize SciPy for evaluation. It helps calculate the IoU between 3D bounding boxes.","title":"\ud83d\udcbb Installation"},{"location":"projects/ithor_rearrangement/#rearrangement-task-description","text":"Overview \ud83e\udd16. Our rearrangement task involves moving and modifying (i.e. opening/closing) randomly placed objects within a room to obtain a goal configuration. There are 2 phases: Walkthrough \ud83d\udc40. The agent walks around the room and observes the objects in their ideal goal state. Unshuffle \ud83c\udfcb. After the walkthrough phase, we randomly change between 1 to 5 objects in the room. The agent's goal is to identify which objects have changed and reset those objects to their state from the walkthrough phase. Changes to an object's state may include changes to its position, orientation, or openness.","title":"\ud83d\udcdd Rearrangement Task Description"},{"location":"projects/ithor_rearrangement/#challenge-tracks-and-datasets","text":"","title":"\ud83d\udee4\ufe0f Challenge Tracks and Datasets"},{"location":"projects/ithor_rearrangement/#the-1-and-2-phase-tracks","text":"For this 2021 challenge we have two distinct tracks: 1-Phase Track (Easier). In this track we merge both of the above phases into a single phase. At every step the agent obtains observations from the walkthrough (goal) state as well as the shuffled state. This allows the agent to directly compare aligned images from the two world-states and thus makes it much easier to determine if an object is, or is not, in its goal pose. 2-Phase Track (Harder). In this track, the walkthrough and unshuffle phases occur sequentially and so, once in the unshuffle phase, the agent no longer has any access to the walkthrough state except through any memory it has saved.","title":"\u261d\ufe0f+\u270c\ufe0f The 1- and 2-Phase Tracks"},{"location":"projects/ithor_rearrangement/#datasets","text":"For this challenge we have four distinct dataset splits: \"train\" , \"train_unseen\" , \"val\" , and \"test\" . The train and train_unseen splits use floor plans 1-20, 200-220, 300-320, and 400-420 within AI2-THOR, the \"val\" split uses floor plans 21-25, 221-225, 321-325, and 421-425, and finally the \"test\" split uses scenes 26-30, 226-230, 326-330, and 426-430. These dataset splits are stored as the compressed pickle -serialized files data/*.pkl.gz . While you are freely (and encouraged) to enhance the training set as you see fit, you should never train your agent within any of the test scenes. For evaluation, your model will need to be evaluated on each of the above splits and the results submitted to our leaderboard link (see section below). As the \"train\" and \"train_unseen\" sets are quite large, we do not expect you to evaluate on their entirety. Instead we select ~1000 datapoints from each of these sets for use in evaluation. For convenience, we provide the data/combined.pkl.gz file which contains the \"train\" , \"train_unseen\" , \"val\" , and \"test\" datapoints that should be used for evaluation. Split # Total Episodes # Episodes for Eval Path train 4000 1200 data/train.pkl.gz train_unseen 3800 1140 data/train_unseen.pkl.gz val 1000 1000 data/val.pkl.gz test 1000 1000 data/test.pkl.gz combined 4340 4340 data/combined.pkl.gz","title":"\ud83d\udcca Datasets"},{"location":"projects/ithor_rearrangement/#submitting-to-the-leaderboard","text":"We are tracking challenge participant entries using the AI2 Leaderboard . Submissions can be made to the 1-phase leaderboard here and submissions to the 2-phase leaderboard can be made here . Submissions should include your agent's trajectories for all tasks contained within the combined.pkl.gz dataset, this \"combined\" dataset includes tasks for the train, train_unseen, validation, and test sets. For an example as to how to iterate through all the datapoints in this dataset and save the resulting metrics in our expected submission format see here . A (full) example the expected submission format for the 1-phase task can be found here and, for the 2-phase task, can be found here . Note that this submission format is a gzip'ed json file where the json file has the form { \"UNIQUE_ID_OF_TASK_0\": YOUR_AGENTS_METRICS_AND_TRAJECTORY_FOR_TASK_0, \"UNIQUE_ID_OF_TASK_1\": YOUR_AGENTS_METRICS_AND_TRAJECTORY_FOR_TASK_1, ... } these metrics and unique IDs can be easily obtained when iterating over the dataset (see the above example). Alternatively: if you run inference on the combined dataset using AllenAct (see below for more details) then you can simply (1) gzip the metrics*.json file saved when running inference, (2) rename this file submission.json.gz , and (3) submit this file to the leaderboard directly.","title":"\ud83d\udee4\ufe0f Submitting to the Leaderboard"},{"location":"projects/ithor_rearrangement/#allowed-observations","text":"In both of these tracks, agents should make decisions based off of egocentric sensor readings. The types of sensors allowed/provided for this challenge include: RGB images - having shape 224x224x3 and an FOV of 90 degrees. Depth maps - having shape 224x224 and an FOV of 90 degrees. Perfect egomotion - We allow for agents to know precisely how far (and in which direction) they have moved as well as how many degrees they have rotated. While you are absolutely free to use any sensor information you would like during training (e.g. pretraining your CNN using semantic segmentations from AI2-THOR or using a scene graph to compute expert actions for imitation learning) such additional sensor information should not be used at inference time.","title":"\ud83d\uddbc\ufe0f Allowed Observations"},{"location":"projects/ithor_rearrangement/#allowed-actions","text":"A total of 82 actions are available to our agents, these include: Navigation * Move[Ahead/Left/Right/Back] - Results in the agent moving 0.25m in the specified direction if doing so would not result in the agent colliding with something. Rotate[Right/Left] - Results in the agent rotating 90 degrees clockwise (if Right ) or counterclockwise (if Left ). This action may fail if the agent is holding an object and rotating would cause the object to collide. Look[Up/Down] - Results in the agent raising or lowering its camera angle by 30 degrees (up to a max of 60 degrees below horizontal and 30 degrees above horizontal). Object Interaction Pickup[OBJECT_TYPE] - Where OBJECT_TYPE is one of the 62 pickupable object types, see constants.py . This action results in the agent picking up a visible object of type OBJECT_TYPE if: (a) the agent is not already holding an object, (b) the agent is close enough to the object (within 1.5m), and picking up the object would not result in it colliding with objects in front of the agent. If there are multiple objects of type OBJECT_TYPE then the object closest to the agent is chosen. Open[OBJECT_TYPE] - Where OBJECT_TYPE is one of the 10 opennable object types that are not also pickupable, see constants.py . If an object whose openness is different from the openness in the goal state is visible and within 1.5m of the agent, this object's openness is changed to its value in the goal state. PlaceObject - Results in the agent dropping its held object. If the held object's goal state is visible and within 1.5m of the agent, it is placed into that goal state. Otherwise, a heuristic is used to place the object on a nearby surface. Done action Done - Results in the walkthrough or unshuffle phase immediately terminating.","title":"\ud83c\udfc3 Allowed Actions"},{"location":"projects/ithor_rearrangement/#setting-up-rearrangement","text":"","title":"\ud83c\udf7d\ufe0f Setting up Rearrangement"},{"location":"projects/ithor_rearrangement/#learning-by-example","text":"See the example.py file for an example of how you can instantiate the 1- and 2-phase variants of our rearrangement task.","title":"\u2728 Learning by example"},{"location":"projects/ithor_rearrangement/#the-rearrange-thor-environment-class","text":"The rearrange.environment.RearrangeTHOREnvironment class provides a wrapper around the AI2-THOR environment and is designed to 1. Make it easy to set up a AI2-THOR scene in a particular state ready for rearrangement. 1. Provides utilities to make it easy to evaluate (see e.g. the poses and compare_poses methods) how close the current state of the environment is to the goal state. 1. Provide an API with which the agent may interact with the environment.","title":"\ud83c\udf0e The Rearrange THOR Environment class"},{"location":"projects/ithor_rearrangement/#the-rearrange-task-sampler-class","text":"You'll notice that the above RearrangeTHOREnvironment is not explicitly instantiated by the example.py script and, instead, we create rearrange.tasks.RearrangeTaskSampler objects using the TwoPhaseRGBBaseExperimentConfig.make_sampler_fn and OnePhaseRGBBaseExperimentConfig.make_sampler_fn . This is because the RearrangeTHOREnvironment is very flexible and doesn't know anything about training/validation/test datasets, the types of actions we want our agent to be restricted to use, or precisely which types of sensor observations we want to give our agents (e.g. RGB images, depth maps, etc). All of these extra details are managed by the RearrangeTaskSampler which iteratively creates new tasks for our agent to complete when calling the next_task method. During training, these new tasks can be sampled indefinitely while, during validation or testing, the tasks will only be sampled until the validation/test datasets are exhausted. This sampling is best understood by example so please go over the example.py file.","title":"\ud83c\udfd2 The Rearrange Task Sampler class"},{"location":"projects/ithor_rearrangement/#the-walkthrough-task-and-unshuffle-task-classes","text":"As described above, the RearrangeTaskSampler samples tasks for our agent to complete, these tasks correspond to instantiations of the rearrange.tasks.WalkthroughTask and rearrange.tasks.UnshuffleTask classes. For the 2-phase challenge track, the RearrangeTaskSampler will first sample a new WalkthroughTask after which it will sample a corresponding UnshuffleTask where the agent must return the objects to their poses at the start of the WalkthroughTask .","title":"\ud83d\udeb6\ud83d\udd00 The Walkthrough Task and Unshuffle Task classes"},{"location":"projects/ithor_rearrangement/#object-poses","text":"Accessing object poses \ud83e\uddd8. The poses of all objects in the environment can be accessed using the RearrangeTHOREnvironment.poses property, i.e. unshuffle_start_poses , walkthrough_start_poses , current_poses = env . poses # where env is an RearrangeTHOREnvironment instance Reading an object's pose \ud83d\udcd6. Here, unshuffle_start_poses , walkthrough_start_poses , and current_poses evaluate to a list of dictionaries and are defined as: unshuffle_start_poses stores a list of object poses if the agent were to do nothing to the env during the unshuffling phase. walkthrough_start_poses stores a list of object poses that the agent sees during the walkthrough phase. current_poses stores a list of object poses in the current state of the environment (i.e. possibly after the unshuffle agent makes all its changes to the env during the unshuffling phase). Each dictionary contains the object's pose in a form similar to: { \"type\" : \"Candle\" , \"position\" : { \"x\" : - 0.3012670874595642 , \"y\" : 0.7431036233901978 , \"z\" : - 2.040205240249634 }, \"rotation\" : { \"x\" : 2.958569288253784 , \"y\" : 0.027708930894732475 , \"z\" : 0.6745457053184509 }, \"openness\" : None , \"pickupable\" : True , \"broken\" : False , \"objectId\" : \"Candle|-00.30|+00.74|-02.04\" , \"name\" : \"Candle_977f7f43\" , \"parentReceptacles\" : [ \"Bathtub|-01.28|+00.28|-02.53\" ], \"bounding_box\" : [ [ - 0.27043721079826355 , 0.6975823640823364 , - 2.0129783153533936 ], [ - 0.3310248851776123 , 0.696869969367981 , - 2.012985944747925 ], [ - 0.3310534358024597 , 0.6999208927154541 , - 2.072017192840576 ], [ - 0.27046576142311096 , 0.7006332278251648 , - 2.072009563446045 ], [ - 0.272365003824234 , 0.8614493608474731 , - 2.0045082569122314 ], [ - 0.3329526484012604 , 0.8607369661331177 , - 2.0045158863067627 ], [ - 0.3329811990261078 , 0.8637878894805908 , - 2.063547134399414 ], [ - 0.27239352464675903 , 0.8645002245903015 , - 2.063539505004883 ] ] } Matching objects across poses \ud83e\udd1d. Across unshuffle_start_poses , walkthrough_start_poses , and current_poses , the ith entry in each list will always correspond to the same object across each pose list. So, unshuffle_start_poses[5] will refer to the same object as walkthrough_start_poses[5] and current_poses[5] . Most scenes have around 70 objects, among which, 10 to 20 are pickupable by the agent. Pose keys \ud83d\udd11. openness specifies the [0:1] percentage that an object is opened. For objects where the openness value does not fit (e.g., Bowl , Spoon ), the openness value is None . bounding_box is only given for moveable objects, where the set of moveable objects may consist of couches or chairs, that are not necessarily pickupable. For pickupable objects, the bounding_box is aligned to the object's relative axes. For moveable objects that are non-pickupable, the object is aligned to the global axes. broken states if the object broke from the agent's actions during the unshuffling phase. The initial pose or goal pose for each object will never be broken. But, if the agent decides to pick up an object, and drop it on a hard surface, it's possible that the object can break.","title":"\ud83d\uddfa\ufe0f Object Poses"},{"location":"projects/ithor_rearrangement/#evaluation","text":"To evaluate the quality of a rearrangement agent we compute several metrics measuring how well the agent has managed to move objects so that their final poses are (approximately) equal to their goal poses.","title":"\ud83c\udfc6 Evaluation"},{"location":"projects/ithor_rearrangement/#when-are-poses-approximately-equal","text":"Recall that we represent the pose of an object as a combination of its: 1. Openness \ud83d\udcd6. - A value in [0,1] which measures how far the object has been opened. 2. Position \ud83d\udccd, Rotation \ud83d\ude43, and bounding box \ud83d\udce6 - The 3D position, rotation, and bounding box of each object. 3. Broken - A boolean indicating if the object has been broken (all goal object poses are unbroken). The openness between its goal state and predicted state is off by less than 20 percent. The openness check is only applied to objects that can open. The object's 3D bounding box from its goal pose and the predicted pose must have an IoU over 0.5. The positional check is only relevant to objects that can move. To measure if two object poses are approximately equal we use the following criterion: 1. \u274c If any object pose is broken. 1. \u274c If the object is opennable but not pickupable (e.g. a cabinet) and the the openness values between the two poses differ by more than 0.2. 1. \u274c The two 3D bounding boxes of pickupable objects have an IoU under 0.5. 1. \u2714\ufe0f None of the above criteria are met so the poses are not broken, are close in openness values, and have sufficiently high IoU.","title":"\ud83d\udccf When are poses (approximately) equal?"},{"location":"projects/ithor_rearrangement/#computing-metrics","text":"Suppose that task is an instance of an UnshuffleTask which your agent has taken actions until reaching a terminal state (e.g. either the agent has taken the maximum number of steps or it has taken the \"done\" action). Then metrics regarding the agent's performance can be computed by calling the task.metrics() function. This will return a dictionary of the form { \"task_info\" : { \"scene\" : \"FloorPlan420\" , \"index\" : 7 , \"stage\" : \"train\" }, \"ep_length\" : 176 , \"unshuffle/ep_length\" : 7 , \"unshuffle/reward\" : 0.5058389582634852 , \"unshuffle/start_energy\" : 0.5058389582634852 , \"unshuffle/end_energy\" : 0.0 , \"unshuffle/prop_fixed\" : 1.0 , \"unshuffle/prop_fixed_strict\" : 1.0 , \"unshuffle/num_misplaced\" : 0 , \"unshuffle/num_newly_misplaced\" : 0 , \"unshuffle/num_initially_misplaced\" : 1 , \"unshuffle/num_fixed\" : 1 , \"unshuffle/num_broken\" : 0 , \"unshuffle/change_energy\" : 0.5058464936498058 , \"unshuffle/num_changed\" : 1 , \"unshuffle/prop_misplaced\" : 0.0 , \"unshuffle/energy_prop\" : 0.0 , \"unshuffle/success\" : 0.0 , \"walkthrough/ep_length\" : 169 , \"walkthrough/reward\" : 1.82 , \"walkthrough/num_explored_xz\" : 17 , \"walkthrough/num_explored_xzr\" : 46 , \"walkthrough/prop_visited_xz\" : 0.5151515151515151 , \"walkthrough/prop_visited_xzr\" : 0.3484848484848485 , \"walkthrough/num_obj_seen\" : 11 , \"walkthrough/prop_obj_seen\" : 0.9166666666666666 } Of the above metrics, the most important (those used for comparing models) are * Success rate ( \"unshuffle/success\" ) - This is the most unforgiving of our metrics and equals 1 if all object poses are in their goal states after the unshuffle phase. * % Misplaced ( \"unshuffle/prop_misplaced\" ) - The above sucess metric is quite strict, requiring exact rearrangement of all objects, and also does not additionally penalize an agent for moving objects that should not be moved. This metric equals the number of misplaced objects at the end of the episode divided by the number of misplaced objects at the start of the episode. Note that this metric can be larger than 1 if the agent, during the unshuffle stage, misplaces more objects than were originally misplaced at the start. * % Fixed Strict ( \"unshuffle/prop_fixed_strict\" ) - This metric equals 0 if, at the end of the unshuffle task, the agent has misplaced any new objects (i.e. it has incorrectly moved an object that started in its correct position). Otherwise, if it has not misplaced new objects, then this equals (# objects which started in the wrong pose but are now in the correct pose) / (# objects which started in an incorrect pose), i.e. the proportion of objects who had their pose fixed . * % Energy Remaining ( \"unshuffle/energy_prop\" ) - The above metrics do not give any partial credit if, for example, the agent moves an object across a room and towards its goal pose but fails to place it so that has a sufficiently high IOU with the goal. To allow for partial credit, we define an energy function D that monotonically decreases to 0 as two poses get closer together (see code for full details) and which equals zero if two poses are approximately equal. This metric is then defined as the amount of energy remaining at the end of the unshuffle episode divided by the total energy at the start of the unshuffle episode, i.e. equals (sum of energy between all goal/current object poses at end of the unshuffle phase) / (sum of energy between all goal/current object poses at the start of the unshuffle phase).","title":"\ud83d\udcaf Computing metrics"},{"location":"projects/ithor_rearrangement/#training-baseline-models-with-allenact","text":"We use the AllenAct framework for training our baseline rearrangement models, this framework is automatically installed when installing the requirements for this project . Before running training or inference you'll first have to add the ai2thor-rearrangement directory to your PYTHONPATH (so that python and AllenAct knows where to for various modules). To do this you can run the following: cd YOUR/PATH/TO/ai2thor-rearrangement export PYTHONPATH = $PYTHONPATH : $PWD Let's say you want to train a model for the 1-phase challenge. This can be easily done by running the command allenact -o rearrange_out -b . baseline_configs/one_phase/one_phase_rgb_resnet_dagger.py This will train (using DAgger, a form of imitation learning) a model which uses a pretrained (with frozen weights) ResNet18 as the visual backbone that feeds into a recurrent neural network (a GRU) before producing action probabilities and a value estimate. Results from this training are then saved to rearrange_out where you can find model checkpoints, tensorboard plots, and configuration files that can be used if you, in the future, forget precisely what the details of your experiment were. A similar model can be trained for the 2-phase challenge by running allenact -o rearrange_out -b . baseline_configs/two_phase/two_phase_rgb_resnet_ppowalkthrough_ilunshuffle.py","title":"\ud83c\udfcb Training Baseline Models with AllenAct"},{"location":"projects/ithor_rearrangement/#pretrained-models","text":"We currently provide the following pretrained models (see our paper for details on these models): Model % Fixed Strict (Test) Pretrained Model 1-Phase ResNet18 IL 6.3% (link) 1-Phase ResNet18 PPO 5.3% (link) 1-Phase Simple IL 4.8% (link) 1-Phase Simple PPO 4.6% (link) 2-Phase ResNet18 IL+PPO 0.66% (link) These models can be downloaded at from the above links and should be placed into the pretrained_model_ckpts directory. You can then, for example, run inference for the 1-Phase ResNet18 IL model using AllenAct by running: export CURRENT_TIME = $( date '+%Y-%m-%d_%H-%M-%S' ) # This is just to record when you ran this inference allenact baseline_configs/one_phase/one_phase_rgb_resnet_dagger.py \\ -c pretrained_model_ckpts/exp_OnePhaseRGBResNetDagger_40proc__stage_00__steps_000050058550.pt \\ --extra_tag $CURRENT_TIME \\ --eval this will evaluate this model across all datapoints in the data/combined.pkl.gz dataset which contains data from the train , train_unseen , val , and test sets so that evaluation doesn't have to be run on each set separately.","title":"\ud83d\udcaa Pretrained Models"},{"location":"projects/ithor_rearrangement/#citation","text":"If you use this work, please cite our paper (to appear in CVPR'21): @InProceedings { RoomR , author = {Luca Weihs and Matt Deitke and Aniruddha Kembhavi and Roozbeh Mottaghi} , title = {Visual Room Rearrangement} , booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)} , month = {June} , year = {2021} }","title":"\ud83d\udcc4 Citation"},{"location":"projects/objectnav_baselines/","text":"Baseline models ObjectNav (for RoboTHOR/iTHOR) # This project contains the code for training baseline models for the ObjectNav task. In ObjectNav, the agent spawns at a location in an environment and is tasked to explore the environment until it finds an object of a certain type (such as TV or Basketball). Once the agent is confident that it has the object within sight it executes the END action which terminates the episode. If the agent is within a set distance to the target (in our case 1.0 meters) and the target is visible within its observation frame the agent succeeded, otherwise it failed. Provided are experiment configs for training a simple convolutional model with an GRU using RGB , Depth or RGB-D (i.e. RGB+Depth ) as inputs in RoboTHOR and iTHOR . The experiments are set up to train models using the DD-PPO Reinforcement Learning Algorithm. For the RoboTHOR environment we also have and experiment ( objectnav_robothor_rgb_resnetgru_dagger.py ) showing how a model can be trained using DAgger, a form of imitation learning. To train an experiment run the following command from the allenact root directory: python main.py <PATH_TO_EXPERIMENT_CONFIG> -o <PATH_TO_OUTPUT> -c Where <PATH_TO_OUTPUT> is the path of the directory where we want the model weights and logs to be stored and <PATH_TO_EXPERIMENT_CONFIG> is the path to the python file containing the experiment configuration. An example usage of this command would be: python main.py projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnet_ddppo.py -o storage/objectnav-robothor-rgb This trains a simple convolutional neural network with a GRU using RGB input passed through a pretrained ResNet-18 visual encoder on the PointNav task in the RoboTHOR environment and stores the model weights and logs to storage/pointnav-robothor-rgb . RoboTHOR ObjectNav 2021 Challenge # The experiment configs found under the projects/objectnav_baselines/experiments/robothor directory are designed to conform to the requirements of the RoboTHOR ObjectNav 2021 Challenge . Training a baseline # To train a baseline ResNet->GRU model taking RGB-D inputs, run the following command python main.py projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnet_ddppo.py -o storage/objectnav-robothor-rgbd By default, when using a machine with a GPU, the above experiment will attempt to train using 60 parallel processes across all available GPUs. See the TRAIN_GPU_IDS constant in experiments/objectnav_thor_base.py and the NUM_PROCESSES constant in experiments/robothor/objectnav_robothor_base.py if you'd like to change which GPUs are used or how many processes are run respectively. Downloading our pretrained model checkpoint # We provide a pretrained model obtained allowing the above command to run for all 300M training steps and then selecting the model checkpoint with best validation-set performance (for us occuring at ~170M training steps). You can download this model checkpoint by running bash pretrained_model_ckpts/download_navigation_model_ckpts.sh robothor-objectnav-challenge-2021 from the top-level directory. This will download the pretrained model weights and save them at the path pretrained_model_ckpts/robothor-objectnav-challenge-2021/Objectnav-RoboTHOR-RGBD-ResNetGRU-DDPPO/2021-02-09_22-35-15/exp_Objectnav-RoboTHOR-RGBD-ResNetGRU-DDPPO_0.2.0a_300M__stage_00__steps_000170207237.pt Running inference on the pretrained model # You can run inference on the above pretrained model (on the test dataset) by running export SAVED_MODEL_PATH = pretrained_model_ckpts/robothor-objectnav-challenge-2021/Objectnav-RoboTHOR-RGBD-ResNetGRU-DDPPO/2021-02-09_22-35-15/exp_Objectnav-RoboTHOR-RGBD-ResNetGRU-DDPPO_0.2.0a_300M__stage_00__steps_000170207237.pt python main.py projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo.py -c $SAVED_MODEL_PATH --eval To discourage \"cheating\", the test dataset has been scrubbed of the information needed to actually compute the success rate / SPL of your model and so running the above will only save the trajectories your models take. To evaluate these trajectories you will have to submit them to our leaderboard, see here for more details . If you'd like to get a sense of if your model is doing well before submitting to the leaderboard, you can obtain the success rate / SPL of it on our validation dataset. To do this, you can simply comment-out the line TEST_DATASET_DIR = os . path . join ( os . getcwd (), \"datasets/robothor-objectnav/test\" ) within the projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base.py file and rerun the above python main.py ... command (when the test dataset is not given, the code defaults to using the validation set).","title":"ObjectNav baselines"},{"location":"projects/objectnav_baselines/#baseline-models-objectnav-for-robothorithor","text":"This project contains the code for training baseline models for the ObjectNav task. In ObjectNav, the agent spawns at a location in an environment and is tasked to explore the environment until it finds an object of a certain type (such as TV or Basketball). Once the agent is confident that it has the object within sight it executes the END action which terminates the episode. If the agent is within a set distance to the target (in our case 1.0 meters) and the target is visible within its observation frame the agent succeeded, otherwise it failed. Provided are experiment configs for training a simple convolutional model with an GRU using RGB , Depth or RGB-D (i.e. RGB+Depth ) as inputs in RoboTHOR and iTHOR . The experiments are set up to train models using the DD-PPO Reinforcement Learning Algorithm. For the RoboTHOR environment we also have and experiment ( objectnav_robothor_rgb_resnetgru_dagger.py ) showing how a model can be trained using DAgger, a form of imitation learning. To train an experiment run the following command from the allenact root directory: python main.py <PATH_TO_EXPERIMENT_CONFIG> -o <PATH_TO_OUTPUT> -c Where <PATH_TO_OUTPUT> is the path of the directory where we want the model weights and logs to be stored and <PATH_TO_EXPERIMENT_CONFIG> is the path to the python file containing the experiment configuration. An example usage of this command would be: python main.py projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnet_ddppo.py -o storage/objectnav-robothor-rgb This trains a simple convolutional neural network with a GRU using RGB input passed through a pretrained ResNet-18 visual encoder on the PointNav task in the RoboTHOR environment and stores the model weights and logs to storage/pointnav-robothor-rgb .","title":"Baseline models ObjectNav (for RoboTHOR/iTHOR)"},{"location":"projects/objectnav_baselines/#robothor-objectnav-2021-challenge","text":"The experiment configs found under the projects/objectnav_baselines/experiments/robothor directory are designed to conform to the requirements of the RoboTHOR ObjectNav 2021 Challenge .","title":"RoboTHOR ObjectNav 2021 Challenge"},{"location":"projects/objectnav_baselines/#training-a-baseline","text":"To train a baseline ResNet->GRU model taking RGB-D inputs, run the following command python main.py projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnet_ddppo.py -o storage/objectnav-robothor-rgbd By default, when using a machine with a GPU, the above experiment will attempt to train using 60 parallel processes across all available GPUs. See the TRAIN_GPU_IDS constant in experiments/objectnav_thor_base.py and the NUM_PROCESSES constant in experiments/robothor/objectnav_robothor_base.py if you'd like to change which GPUs are used or how many processes are run respectively.","title":"Training a baseline"},{"location":"projects/objectnav_baselines/#downloading-our-pretrained-model-checkpoint","text":"We provide a pretrained model obtained allowing the above command to run for all 300M training steps and then selecting the model checkpoint with best validation-set performance (for us occuring at ~170M training steps). You can download this model checkpoint by running bash pretrained_model_ckpts/download_navigation_model_ckpts.sh robothor-objectnav-challenge-2021 from the top-level directory. This will download the pretrained model weights and save them at the path pretrained_model_ckpts/robothor-objectnav-challenge-2021/Objectnav-RoboTHOR-RGBD-ResNetGRU-DDPPO/2021-02-09_22-35-15/exp_Objectnav-RoboTHOR-RGBD-ResNetGRU-DDPPO_0.2.0a_300M__stage_00__steps_000170207237.pt","title":"Downloading our pretrained model checkpoint"},{"location":"projects/objectnav_baselines/#running-inference-on-the-pretrained-model","text":"You can run inference on the above pretrained model (on the test dataset) by running export SAVED_MODEL_PATH = pretrained_model_ckpts/robothor-objectnav-challenge-2021/Objectnav-RoboTHOR-RGBD-ResNetGRU-DDPPO/2021-02-09_22-35-15/exp_Objectnav-RoboTHOR-RGBD-ResNetGRU-DDPPO_0.2.0a_300M__stage_00__steps_000170207237.pt python main.py projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo.py -c $SAVED_MODEL_PATH --eval To discourage \"cheating\", the test dataset has been scrubbed of the information needed to actually compute the success rate / SPL of your model and so running the above will only save the trajectories your models take. To evaluate these trajectories you will have to submit them to our leaderboard, see here for more details . If you'd like to get a sense of if your model is doing well before submitting to the leaderboard, you can obtain the success rate / SPL of it on our validation dataset. To do this, you can simply comment-out the line TEST_DATASET_DIR = os . path . join ( os . getcwd (), \"datasets/robothor-objectnav/test\" ) within the projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base.py file and rerun the above python main.py ... command (when the test dataset is not given, the code defaults to using the validation set).","title":"Running inference on the pretrained model"},{"location":"projects/pointnav_baselines/","text":"Baseline models for the Point Navigation task in the Habitat, RoboTHOR and iTHOR environments # This project contains the code for training baseline models on the PointNav task. In this setting the agent spawns at a location in an environment and is tasked to move to another location. The agent is given a \"compass\" that tells it the distance and bearing to the target position at every frame. Once the agent is confident that it has reached the end it executes the END action which terminates the episode. If the agent is within a set distance to the target (in our case 0.2 meters) the agent succeeded, else it failed. Provided are experiment configs for training a simple convolutional model with an GRU using RGB , Depth or RGBD as inputs in Habitat , RoboTHOR and iTHOR . The experiments are set up to train models using the DD-PPO Reinforcement Learning Algorithm. To train an experiment run the following command from the allenact root directory: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> Where <PATH_TO_OUTPUT> is the path of the directory where we want the model weights and logs to be stored, <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> is the directory where our experiment file is located and <EXPERIMENT_NAME> is the name of the python module containing the experiment. An example usage of this command would be: python main.py -o storage/pointnav-robothor-depth -b projects/pointnav_baselines/experiments/robothor/ pointnav_robothor_depth_simpleconvgru_ddppo This trains a simple convolutional neural network with a GRU using Depth input on the PointNav task in the RoboTHOR environment and stores the model weights and logs to storage/pointnav-robothor-rgb .","title":"PointNav baselines"},{"location":"projects/pointnav_baselines/#baseline-models-for-the-point-navigation-task-in-the-habitat-robothor-and-ithor-environments","text":"This project contains the code for training baseline models on the PointNav task. In this setting the agent spawns at a location in an environment and is tasked to move to another location. The agent is given a \"compass\" that tells it the distance and bearing to the target position at every frame. Once the agent is confident that it has reached the end it executes the END action which terminates the episode. If the agent is within a set distance to the target (in our case 0.2 meters) the agent succeeded, else it failed. Provided are experiment configs for training a simple convolutional model with an GRU using RGB , Depth or RGBD as inputs in Habitat , RoboTHOR and iTHOR . The experiments are set up to train models using the DD-PPO Reinforcement Learning Algorithm. To train an experiment run the following command from the allenact root directory: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> Where <PATH_TO_OUTPUT> is the path of the directory where we want the model weights and logs to be stored, <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> is the directory where our experiment file is located and <EXPERIMENT_NAME> is the name of the python module containing the experiment. An example usage of this command would be: python main.py -o storage/pointnav-robothor-depth -b projects/pointnav_baselines/experiments/robothor/ pointnav_robothor_depth_simpleconvgru_ddppo This trains a simple convolutional neural network with a GRU using Depth input on the PointNav task in the RoboTHOR environment and stores the model weights and logs to storage/pointnav-robothor-rgb .","title":"Baseline models for the Point Navigation task in the Habitat, RoboTHOR and iTHOR environments"},{"location":"projects/two_body_problem_2019/","text":"Experiments for the Two Body Problem paper # TODO: # Add details taken from https://prior.allenai.org/projects/two-body-problem Cite the CVPR paper. Give a list of things you can run with bash commands. At least a subset of the experiments.","title":"Experiments for the Two Body Problem paper"},{"location":"projects/two_body_problem_2019/#experiments-for-the-two-body-problem-paper","text":"","title":"Experiments for the Two Body Problem paper"},{"location":"projects/two_body_problem_2019/#todo","text":"Add details taken from https://prior.allenai.org/projects/two-body-problem Cite the CVPR paper. Give a list of things you can run with bash commands. At least a subset of the experiments.","title":"TODO:"},{"location":"tutorials/","text":"AllenAct Tutorials # Note The provided commands to execute these tutorials assume you have installed the full library and the specific requirements for each used plugin. We provide several tutorials to help ramp up researchers to the field of Embodied-AI as well as to the AllenAct framework. Navigation in MiniGrid # We train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. This tutorial presents: Writing an experiment configuration file with a simple training pipeline from scratch. Using one of the supported environments with minimal user effort. Training, validation and testing your experiment from the command line. Follow the tutorial here. PointNav in RoboTHOR # We train an agent on the Point Navigation task within the RoboTHOR Embodied-AI environment. This tutorial presents: The basics of the Point Navigation task, a common task in Embodied AI Using an external dataset Writing an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. Testing a pre-trained model Follow the tutorial here. Swapping in a new environment # This tutorial demonstrates how easy it is modify the experiment config created in the RoboTHOR PointNav tutorial to work with the iTHOR and Habitat environments. Follow the tutorial here. Using a pretrained model # This tutorial shows how to run inference on one or more checkpoints of a pretrained model and generate visualizations of different types. Follow the tutorial here. Off-policy training # This tutorial shows how to train an Actor using an off-policy dataset with expert actions. Follow the tutorial here. OpenAI gym for continuous control # We train an agent to complete the LunarLanderContinuous-v2 task from OpenAI gym . This tutorial presents: A gym plugin fopr AllenAct . A continuous control example with multiple actors using PPO. Follow the tutorial here.","title":"AllenAct Tutorials"},{"location":"tutorials/#allenact-tutorials","text":"Note The provided commands to execute these tutorials assume you have installed the full library and the specific requirements for each used plugin. We provide several tutorials to help ramp up researchers to the field of Embodied-AI as well as to the AllenAct framework.","title":"AllenAct Tutorials"},{"location":"tutorials/#navigation-in-minigrid","text":"We train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. This tutorial presents: Writing an experiment configuration file with a simple training pipeline from scratch. Using one of the supported environments with minimal user effort. Training, validation and testing your experiment from the command line. Follow the tutorial here.","title":"Navigation in MiniGrid"},{"location":"tutorials/#pointnav-in-robothor","text":"We train an agent on the Point Navigation task within the RoboTHOR Embodied-AI environment. This tutorial presents: The basics of the Point Navigation task, a common task in Embodied AI Using an external dataset Writing an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. Testing a pre-trained model Follow the tutorial here.","title":"PointNav in RoboTHOR"},{"location":"tutorials/#swapping-in-a-new-environment","text":"This tutorial demonstrates how easy it is modify the experiment config created in the RoboTHOR PointNav tutorial to work with the iTHOR and Habitat environments. Follow the tutorial here.","title":"Swapping in a new environment"},{"location":"tutorials/#using-a-pretrained-model","text":"This tutorial shows how to run inference on one or more checkpoints of a pretrained model and generate visualizations of different types. Follow the tutorial here.","title":"Using a pretrained model"},{"location":"tutorials/#off-policy-training","text":"This tutorial shows how to train an Actor using an off-policy dataset with expert actions. Follow the tutorial here.","title":"Off-policy training"},{"location":"tutorials/#openai-gym-for-continuous-control","text":"We train an agent to complete the LunarLanderContinuous-v2 task from OpenAI gym . This tutorial presents: A gym plugin fopr AllenAct . A continuous control example with multiple actors using PPO. Follow the tutorial here.","title":"OpenAI gym for continuous control"},{"location":"tutorials/gym-tutorial/","text":"Tutorial: OpenAI gym for continuous control. # Note The provided commands to execute in this tutorial assume you have installed the full library and the requirements for the gym_plugin . The latter can be installed by pip install -r allenact_plugins/gym_plugin/extra_requirements.txt In this tutorial, we: Introduce the gym_plugin , which enables some of the tasks in OpenAI's gym for training and inference within AllenAct. Show an example of continuous control with an arbitrary action space covering 2 policies for one of the gym tasks. The task # For this tutorial, we'll focus on one of the continuous-control environments under the Box2D group of gym environments: LunarLanderContinuous-v2 . In this task, the goal is to smoothly land a lunar module in a landing pad, as shown below. . To achieve this goal, we need to provide continuous control for a main engine and directional one (2 real values). In order to solve the task, the expected reward is of at least 200 points. The controls for main and directional engines are both in the range [-1.0, 1.0] and the observation space is composed of 8 scalars indicating x and y positions, x and y velocities, lander angle and angular velocity, and left and right ground contact. Note that these 8 scalars provide a full observation of the state. Implementation # For this tutorial, we'll use the readily available gym_plugin , which includes a wrapper for gym environments , a task sampler and task definition , a sensor to wrap the observations provided by the gym environment, and a simple model . The experiment config, similar to the one used for the Navigation in MiniGrid tutorial , is defined as follows: from typing import Dict , Optional , List , Any , cast import gym import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from allenact.algorithms.onpolicy_sync.losses.ppo import PPO from allenact.base_abstractions.experiment_config import ExperimentConfig , TaskSampler from allenact.base_abstractions.sensor import SensorSuite from allenact_plugins.gym_plugin.gym_models import MemorylessActorCritic from allenact_plugins.gym_plugin.gym_sensors import GymBox2DSensor from allenact_plugins.gym_plugin.gym_tasks import GymTaskSampler from allenact.utils.experiment_utils import ( TrainingPipeline , Builder , PipelineStage , LinearDecay , ) from allenact.utils.viz_utils import VizSuite , AgentViewViz class GymTutorialExperimentConfig ( ExperimentConfig ): @classmethod def tag ( cls ) -> str : return \"GymTutorial\" Sensors and Model # As mentioned above, we'll use a GymBox2DSensor to provide full observations from the state of the gym environment to our model. SENSORS = [ GymBox2DSensor ( \"LunarLanderContinuous-v2\" , uuid = \"gym_box_data\" ), ] We define our ActorCriticModel agent using a lightweight implementation with separate MLPs for actors and critic, MemorylessActorCritic . Since this is a model for continuous control, note that the superclass of our model is ActorCriticModel[GaussianDistr] instead of ActorCriticModel[CategoricalDistr] , since we'll use a Gaussian distribution to sample actions. @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return MemorylessActorCritic ( input_uuid = \"gym_box_data\" , action_space = gym . spaces . Box ( - 1.0 , 1.0 , ( 2 ,) ), # 2 actors, each in the range [-1.0, 1.0] observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , action_std = 0.5 , ) Task samplers # We use an available TaskSampler implementation for gym environments that allows to sample GymTasks : GymTaskSampler . Even though it is possible to let the task sampler instantiate the proper sensor for the chosen task name (by passing None ), we use the sensors we created above, which contain a custom identifier for the actual observation space ( gym_box_data ) also used by the model. @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return GymTaskSampler ( ** kwargs ) For convenience, we will use a _get_sampler_args method to generate the task sampler arguments for all three modes, train, valid, test : def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"train\" , seeds = seeds ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"valid\" , seeds = seeds ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"test\" , seeds = seeds ) Similarly to what we do in the Minigrid navigation tutorial, the task sampler samples random tasks for ever, while, during testing (or validation), we sample a fixed number of tasks. def _get_sampler_args ( self , process_ind : int , mode : str , seeds : List [ int ] ) -> Dict [ str , Any ]: \"\"\"Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` \"\"\" if mode == \"train\" : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 3 # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks ) ) deterministic_sampling = ( True # deterministically sample task in validation/testing ) return dict ( gym_env_types = [ \"LunarLanderContinuous-v2\" ], sensors = self . SENSORS , # sensors used to return observations to the agent max_tasks = max_tasks , # see above task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling , # see above seed = seeds [ process_ind ], ) Note that we just sample 3 tasks for validation and testing in this case, which suffice to illustrate the model's success. Machine parameters # Given the simplicity of the task and model, we can just train the model on the CPU. During training, success should reach 100% in less than 10 minutes, whereas solving the task (evaluation reward > 200) might take about 20 minutes (on a laptop CPU). We allocate a larger number of samplers for training (8) than for validation or testing (just 1), and we default to CPU usage by returning an empty list of devices . We also include a video visualizer ( AgentViewViz ) in test mode. @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ) -> Dict [ str , Any ]: visualizer = None if mode == \"test\" : visualizer = VizSuite ( mode = mode , video_viz = AgentViewViz ( label = \"episode_vid\" , max_clip_length = 400 , vector_task_source = ( \"render\" , { \"mode\" : \"rgb_array\" }), fps = 30 , ), ) return { \"nprocesses\" : 8 if mode == \"train\" else 1 , \"devices\" : [], \"visualizer\" : visualizer , } Training pipeline # The last definition is the training pipeline. In this case, we use a PPO stage with linearly decaying learning rate and 80 single-batch update repeats per rollout: @classmethod def training_pipeline ( cls , ** kwargs ) -> TrainingPipeline : ppo_steps = int ( 1.2e6 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = PPO ( clip_param = 0.2 , value_loss_coef = 0.5 , entropy_coef = 0.0 ,), ), # type:ignore pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ), ], optimizer_builder = Builder ( cast ( optim . Optimizer , optim . Adam ), dict ( lr = 1e-3 )), num_mini_batch = 1 , update_repeats = 80 , max_grad_norm = 100 , num_steps = 2000 , gamma = 0.99 , use_gae = False , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 200000 , metric_accumulate_interval = 50000 , lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )}, # type:ignore ), ) Training and validation # We have a complete implementation of this experiment's configuration class in projects/tutorials/gym_tutorial.py . To start training from scratch, we just need to invoke PYTHONPATH = . python allenact/main.py gym_tutorial -b projects/tutorials -m 8 -o /PATH/TO/gym_output -s 54321 -e from the allenact root directory. Note that we include -e to enforce deterministic evaluation. Please refer to the Navigation in MiniGrid tutorial if in doubt of the meaning of the rest of parameters. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/gym_output which will default to the URL http://localhost:6006/ . After 1,200,000 steps, the script will terminate. If everything went well, the valid success rate should quickly converge to 1 and the mean reward to above 250, while the average episode length should stay below or near 300. Testing # The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to evaluate (i.e. test) a collection of checkpoints, we need to pass the --eval flag and specify the directory containing the checkpoints with the --checkpoint CHECKPOINT_DIR option: PYTHONPATH = . python allenact/main.py gym_tutorial \\ -b projects/tutorials \\ -m 1 \\ -o /PATH/TO/gym_output \\ -s 54321 \\ -e \\ --eval \\ --checkpoint /PATH/TO/gym_output/checkpoints/GymTutorial/YOUR_START_DATE \\ --approx_ckpt_step_interval 800000 # Skip some checkpoints The option --approx_ckpt_step_interval 800000 tells AllenAct that we only want to evaluate checkpoints which were saved every ~800000 steps, this lets us avoid evaluating every saved checkpoint. If everything went well, the test success rate should converge to 1, the episode length below or near 300 steps, and the mean reward to above 250. The images tab in tensorboard will contain videos for the sampled test episodes. . If the test command fails with pyglet.canvas.xlib.NoSuchDisplayException: Cannot connect to \"None\" , e.g. when running remotely, try prepending DISPLAY=:0.0 to the command above, assuming you have an xserver running with such display available: DISPLAY = :0.0 PYTHONPATH = . python allenact/main.py gym_tutorial \\ -b projects/tutorials \\ -m 1 \\ -o /PATH/TO/gym_output \\ -s 54321 \\ -e \\ --eval \\ --checkpoint /PATH/TO/gym_output/checkpoints/GymTutorial/YOUR_START_DATE \\ --approx_ckpt_step_interval 800000","title":"OpenAI gym for continuous control"},{"location":"tutorials/gym-tutorial/#tutorial-openai-gym-for-continuous-control","text":"Note The provided commands to execute in this tutorial assume you have installed the full library and the requirements for the gym_plugin . The latter can be installed by pip install -r allenact_plugins/gym_plugin/extra_requirements.txt In this tutorial, we: Introduce the gym_plugin , which enables some of the tasks in OpenAI's gym for training and inference within AllenAct. Show an example of continuous control with an arbitrary action space covering 2 policies for one of the gym tasks.","title":"Tutorial: OpenAI gym for continuous control."},{"location":"tutorials/gym-tutorial/#the-task","text":"For this tutorial, we'll focus on one of the continuous-control environments under the Box2D group of gym environments: LunarLanderContinuous-v2 . In this task, the goal is to smoothly land a lunar module in a landing pad, as shown below. . To achieve this goal, we need to provide continuous control for a main engine and directional one (2 real values). In order to solve the task, the expected reward is of at least 200 points. The controls for main and directional engines are both in the range [-1.0, 1.0] and the observation space is composed of 8 scalars indicating x and y positions, x and y velocities, lander angle and angular velocity, and left and right ground contact. Note that these 8 scalars provide a full observation of the state.","title":"The task"},{"location":"tutorials/gym-tutorial/#implementation","text":"For this tutorial, we'll use the readily available gym_plugin , which includes a wrapper for gym environments , a task sampler and task definition , a sensor to wrap the observations provided by the gym environment, and a simple model . The experiment config, similar to the one used for the Navigation in MiniGrid tutorial , is defined as follows: from typing import Dict , Optional , List , Any , cast import gym import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from allenact.algorithms.onpolicy_sync.losses.ppo import PPO from allenact.base_abstractions.experiment_config import ExperimentConfig , TaskSampler from allenact.base_abstractions.sensor import SensorSuite from allenact_plugins.gym_plugin.gym_models import MemorylessActorCritic from allenact_plugins.gym_plugin.gym_sensors import GymBox2DSensor from allenact_plugins.gym_plugin.gym_tasks import GymTaskSampler from allenact.utils.experiment_utils import ( TrainingPipeline , Builder , PipelineStage , LinearDecay , ) from allenact.utils.viz_utils import VizSuite , AgentViewViz class GymTutorialExperimentConfig ( ExperimentConfig ): @classmethod def tag ( cls ) -> str : return \"GymTutorial\"","title":"Implementation"},{"location":"tutorials/gym-tutorial/#sensors-and-model","text":"As mentioned above, we'll use a GymBox2DSensor to provide full observations from the state of the gym environment to our model. SENSORS = [ GymBox2DSensor ( \"LunarLanderContinuous-v2\" , uuid = \"gym_box_data\" ), ] We define our ActorCriticModel agent using a lightweight implementation with separate MLPs for actors and critic, MemorylessActorCritic . Since this is a model for continuous control, note that the superclass of our model is ActorCriticModel[GaussianDistr] instead of ActorCriticModel[CategoricalDistr] , since we'll use a Gaussian distribution to sample actions. @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return MemorylessActorCritic ( input_uuid = \"gym_box_data\" , action_space = gym . spaces . Box ( - 1.0 , 1.0 , ( 2 ,) ), # 2 actors, each in the range [-1.0, 1.0] observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , action_std = 0.5 , )","title":"Sensors and Model"},{"location":"tutorials/gym-tutorial/#task-samplers","text":"We use an available TaskSampler implementation for gym environments that allows to sample GymTasks : GymTaskSampler . Even though it is possible to let the task sampler instantiate the proper sensor for the chosen task name (by passing None ), we use the sensors we created above, which contain a custom identifier for the actual observation space ( gym_box_data ) also used by the model. @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return GymTaskSampler ( ** kwargs ) For convenience, we will use a _get_sampler_args method to generate the task sampler arguments for all three modes, train, valid, test : def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"train\" , seeds = seeds ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"valid\" , seeds = seeds ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"test\" , seeds = seeds ) Similarly to what we do in the Minigrid navigation tutorial, the task sampler samples random tasks for ever, while, during testing (or validation), we sample a fixed number of tasks. def _get_sampler_args ( self , process_ind : int , mode : str , seeds : List [ int ] ) -> Dict [ str , Any ]: \"\"\"Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` \"\"\" if mode == \"train\" : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 3 # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks ) ) deterministic_sampling = ( True # deterministically sample task in validation/testing ) return dict ( gym_env_types = [ \"LunarLanderContinuous-v2\" ], sensors = self . SENSORS , # sensors used to return observations to the agent max_tasks = max_tasks , # see above task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling , # see above seed = seeds [ process_ind ], ) Note that we just sample 3 tasks for validation and testing in this case, which suffice to illustrate the model's success.","title":"Task samplers"},{"location":"tutorials/gym-tutorial/#machine-parameters","text":"Given the simplicity of the task and model, we can just train the model on the CPU. During training, success should reach 100% in less than 10 minutes, whereas solving the task (evaluation reward > 200) might take about 20 minutes (on a laptop CPU). We allocate a larger number of samplers for training (8) than for validation or testing (just 1), and we default to CPU usage by returning an empty list of devices . We also include a video visualizer ( AgentViewViz ) in test mode. @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ) -> Dict [ str , Any ]: visualizer = None if mode == \"test\" : visualizer = VizSuite ( mode = mode , video_viz = AgentViewViz ( label = \"episode_vid\" , max_clip_length = 400 , vector_task_source = ( \"render\" , { \"mode\" : \"rgb_array\" }), fps = 30 , ), ) return { \"nprocesses\" : 8 if mode == \"train\" else 1 , \"devices\" : [], \"visualizer\" : visualizer , }","title":"Machine parameters"},{"location":"tutorials/gym-tutorial/#training-pipeline","text":"The last definition is the training pipeline. In this case, we use a PPO stage with linearly decaying learning rate and 80 single-batch update repeats per rollout: @classmethod def training_pipeline ( cls , ** kwargs ) -> TrainingPipeline : ppo_steps = int ( 1.2e6 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = PPO ( clip_param = 0.2 , value_loss_coef = 0.5 , entropy_coef = 0.0 ,), ), # type:ignore pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ), ], optimizer_builder = Builder ( cast ( optim . Optimizer , optim . Adam ), dict ( lr = 1e-3 )), num_mini_batch = 1 , update_repeats = 80 , max_grad_norm = 100 , num_steps = 2000 , gamma = 0.99 , use_gae = False , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 200000 , metric_accumulate_interval = 50000 , lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )}, # type:ignore ), )","title":"Training pipeline"},{"location":"tutorials/gym-tutorial/#training-and-validation","text":"We have a complete implementation of this experiment's configuration class in projects/tutorials/gym_tutorial.py . To start training from scratch, we just need to invoke PYTHONPATH = . python allenact/main.py gym_tutorial -b projects/tutorials -m 8 -o /PATH/TO/gym_output -s 54321 -e from the allenact root directory. Note that we include -e to enforce deterministic evaluation. Please refer to the Navigation in MiniGrid tutorial if in doubt of the meaning of the rest of parameters. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/gym_output which will default to the URL http://localhost:6006/ . After 1,200,000 steps, the script will terminate. If everything went well, the valid success rate should quickly converge to 1 and the mean reward to above 250, while the average episode length should stay below or near 300.","title":"Training and validation"},{"location":"tutorials/gym-tutorial/#testing","text":"The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to evaluate (i.e. test) a collection of checkpoints, we need to pass the --eval flag and specify the directory containing the checkpoints with the --checkpoint CHECKPOINT_DIR option: PYTHONPATH = . python allenact/main.py gym_tutorial \\ -b projects/tutorials \\ -m 1 \\ -o /PATH/TO/gym_output \\ -s 54321 \\ -e \\ --eval \\ --checkpoint /PATH/TO/gym_output/checkpoints/GymTutorial/YOUR_START_DATE \\ --approx_ckpt_step_interval 800000 # Skip some checkpoints The option --approx_ckpt_step_interval 800000 tells AllenAct that we only want to evaluate checkpoints which were saved every ~800000 steps, this lets us avoid evaluating every saved checkpoint. If everything went well, the test success rate should converge to 1, the episode length below or near 300 steps, and the mean reward to above 250. The images tab in tensorboard will contain videos for the sampled test episodes. . If the test command fails with pyglet.canvas.xlib.NoSuchDisplayException: Cannot connect to \"None\" , e.g. when running remotely, try prepending DISPLAY=:0.0 to the command above, assuming you have an xserver running with such display available: DISPLAY = :0.0 PYTHONPATH = . python allenact/main.py gym_tutorial \\ -b projects/tutorials \\ -m 1 \\ -o /PATH/TO/gym_output \\ -s 54321 \\ -e \\ --eval \\ --checkpoint /PATH/TO/gym_output/checkpoints/GymTutorial/YOUR_START_DATE \\ --approx_ckpt_step_interval 800000","title":"Testing"},{"location":"tutorials/minigrid-tutorial/","text":"Tutorial: Navigation in MiniGrid. # In this tutorial, we will train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. We will demonstrate how to: Write an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. This tutorial assumes the installation instructions have already been followed and that, to some extent, this framework's abstractions are known. The extra_requirements for minigrid_plugin and babyai_plugin can be installed with. pip install -r allenact_plugins/minigrid_plugin/extra_requirements.txt ; pip install -r allenact_plugins/babyai_plugin/extra_requirements.txt The task # A MiniGrid-Empty-Random-5x5-v0 task consists of a grid of dimensions 5x5 where an agent spawned at a random location and orientation has to navigate to the visitable bottom right corner cell of the grid by sequences of three possible actions (rotate left/right and move forward). A visualization of the environment with expert steps in a random MiniGrid-Empty-Random-5x5-v0 task looks like The observation for the agent is a subset of the entire grid, simulating a simplified limited field of view, as depicted by the highlighted rectangle (observed subset of the grid) around the agent (red arrow). Gray cells correspond to walls. Experiment configuration file # Our complete experiment consists of: Training a basic actor-critic agent with memory to solve randomly sampled navigation tasks. Validation on a fixed set of tasks (running in parallel with training). A second stage where we test saved checkpoints with a larger fixed set of tasks. The entire configuration for the experiment, including training, validation, and testing, is encapsulated in a single class implementing the ExperimentConfig abstraction. For this tutorial, we will follow the config under projects/tutorials/minigrid_tutorial.py . The ExperimentConfig abstraction is used by the OnPolicyTrainer class (for training) and the OnPolicyInference class (for validation and testing) invoked through the entry script main.py that calls an orchestrating OnPolicyRunner class. It includes: A tag method to identify the experiment. A create_model method to instantiate actor-critic models. A make_sampler_fn method to instantiate task samplers. Three {train,valid,test}_task_sampler_args methods describing initialization parameters for task samplers used in training, validation, and testing; including assignment of workers to devices for simulation. A machine_params method with configuration parameters that will be used for training, validation, and testing. A training_pipeline method describing a possibly multi-staged training pipeline with different types of losses, an optimizer, and other parameters like learning rates, batch sizes, etc. Preliminaries # We first import everything we'll need to define our experiment. from typing import Dict , Optional , List , Any , cast import gym from gym_minigrid.envs import EmptyRandomEnv5x5 import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from allenact.algorithms.onpolicy_sync.losses.ppo import PPO , PPOConfig from allenact.base_abstractions.experiment_config import ExperimentConfig , TaskSampler from allenact.base_abstractions.sensor import SensorSuite from allenact.utils.experiment_utils import ( TrainingPipeline , Builder , PipelineStage , LinearDecay , ) from allenact_plugins.minigrid_plugin.minigrid_models import MiniGridSimpleConvRNN from allenact_plugins.minigrid_plugin.minigrid_sensors import EgocentricMiniGridSensor from allenact_plugins.minigrid_plugin.minigrid_tasks import ( MiniGridTaskSampler , MiniGridTask , ) We now create the MiniGridTutorialExperimentConfig class which we will use to define our experiment. For pedagogical reasons, we will add methods to this class one at a time below with a description of what these classes do. class MiniGridTutorialExperimentConfig ( ExperimentConfig ): An experiment is identified by a tag . @classmethod def tag ( cls ) -> str : return \"MiniGridTutorial\" Sensors and Model # A readily available Sensor type for MiniGrid, EgocentricMiniGridSensor , allows us to extract observations in a format consumable by an ActorCriticModel agent: SENSORS = [ EgocentricMiniGridSensor ( agent_view_size = 5 , view_channels = 3 ), ] The three view_channels include objects, colors and states corresponding to a partial observation of the environment as an image tensor, equivalent to that from ImgObsWrapper in MiniGrid . The relatively large agent_view_size means the view will only be clipped by the environment walls in the forward and lateral directions with respect to the agent's orientation. We define our ActorCriticModel agent using a lightweight implementation with recurrent memory for MiniGrid environments, MiniGridSimpleConvRNN : @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return MiniGridSimpleConvRNN ( action_space = gym . spaces . Discrete ( len ( MiniGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , num_objects = cls . SENSORS [ 0 ] . num_objects , num_colors = cls . SENSORS [ 0 ] . num_colors , num_states = cls . SENSORS [ 0 ] . num_states , ) Task samplers # We use an available TaskSampler implementation for MiniGrid environments that allows to sample both random and deterministic MiniGridTasks , MiniGridTaskSampler : @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return MiniGridTaskSampler ( ** kwargs ) This task sampler will during training (or validation/testing), randomly initialize new tasks for the agent to complete. While it is not quite as important for this task type (as we test our agent in the same setting it is trained on) there are a lot of good reasons we would like to sample tasks differently during training than during validation or testing. One good reason, that is applicable in this tutorial, is that, during training, we would like to be able to sample tasks forever while, during testing, we would like to sample a fixed number of tasks (as otherwise we would never finish testing!). In allenact this is made possible by defining different arguments for the task sampler: def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"train\" ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"valid\" ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"test\" ) where, for convenience, we have defined a _get_sampler_args method: def _get_sampler_args ( self , process_ind : int , mode : str ) -> Dict [ str , Any ]: \"\"\"Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` \"\"\" if mode == \"train\" : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 20 + 20 * ( mode == \"test\" ) # 20 tasks for valid, 40 for test # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks ) ) deterministic_sampling = ( True # deterministically sample task in validation/testing ) return dict ( max_tasks = max_tasks , # see above env_class = self . make_env , # builder for third-party environment (defined below) sensors = self . SENSORS , # sensors used to return observations to the agent env_info = dict (), # parameters for environment builder (none for now) task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling , # see above ) @staticmethod def make_env ( * args , ** kwargs ): return EmptyRandomEnv5x5 () Note that the env_class argument to the Task Sampler is the one determining which task type we are going to train the model for (in this case, MiniGrid-Empty-Random-5x5-v0 from gym-minigrid ) . The sparse reward is given by the environment , and the maximum task length is 100. For training, we opt for a default random sampling, whereas for validation and test we define fixed sets of randomly sampled tasks without needing to explicitly define a dataset. In this toy example, the maximum number of different tasks is 32. For validation we sample 320 tasks using 16 samplers, or 640 for testing, so we can be fairly sure that all possible tasks are visited at least once during evaluation. Machine parameters # Given the simplicity of the task and model, we can quickly train the model on the CPU: @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ) -> Dict [ str , Any ]: return { \"nprocesses\" : 128 if mode == \"train\" else 16 , \"devices\" : [], } We allocate a larger number of samplers for training (128) than for validation or testing (16), and we default to CPU usage by returning an empty list of devices . Training pipeline # The last definition required before starting to train is a training pipeline. In this case, we just use a single PPO stage with linearly decaying learning rate: @classmethod def training_pipeline ( cls , ** kwargs ) -> TrainingPipeline : ppo_steps = int ( 150000 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = PPO ( ** PPOConfig )), # type:ignore pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], optimizer_builder = Builder ( cast ( optim . Optimizer , optim . Adam ), dict ( lr = 1e-4 )), num_mini_batch = 4 , update_repeats = 3 , max_grad_norm = 0.5 , num_steps = 16 , gamma = 0.99 , use_gae = True , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 10000 , metric_accumulate_interval = 1 , lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} # type:ignore ), ) You can see that we use a Builder class to postpone the construction of some of the elements, like the optimizer, for which the model weights need to be known. Training and validation # We have a complete implementation of this experiment's configuration class in projects/tutorials/minigrid_tutorial.py . To start training from scratch, we just need to invoke PYTHONPATH = . python allenact/main.py minigrid_tutorial -b projects/tutorials -m 8 -o /PATH/TO/minigrid_output -s 12345 from the allenact root directory. With -b projects/tutorials we tell allenact that minigrid_tutorial experiment config file will be found in the projects/tutorials directory. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o minigrid_output we set the output folder into which results and logs will be saved. With -s 12345 we set the random seed. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/minigrid_output which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: Testing # The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to evaluate (i.e. test) a particular checkpoint, we need to pass the --eval flag and specify the checkpoint with the --checkpoint CHECKPOINT_PATH option: PYTHONPATH = . python allenact/main.py minigrid_tutorial \\ -b projects/tutorials \\ -m 1 \\ -o /PATH/TO/minigrid_output \\ -s 12345 \\ --eval \\ --checkpoint /PATH/TO/minigrid_output/checkpoints/MiniGridTutorial/YOUR_START_DATE/exp_MiniGridTutorial__stage_00__steps_000000151552.pt Again, if everything went well, the test success rate should converge to 1 and the mean episode length to a value below 4. Detailed results are saved under a metrics subfolder in the output folder. The test curves should look similar to:","title":"Navigation in Minigrid"},{"location":"tutorials/minigrid-tutorial/#tutorial-navigation-in-minigrid","text":"In this tutorial, we will train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. We will demonstrate how to: Write an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. This tutorial assumes the installation instructions have already been followed and that, to some extent, this framework's abstractions are known. The extra_requirements for minigrid_plugin and babyai_plugin can be installed with. pip install -r allenact_plugins/minigrid_plugin/extra_requirements.txt ; pip install -r allenact_plugins/babyai_plugin/extra_requirements.txt","title":"Tutorial: Navigation in MiniGrid."},{"location":"tutorials/minigrid-tutorial/#the-task","text":"A MiniGrid-Empty-Random-5x5-v0 task consists of a grid of dimensions 5x5 where an agent spawned at a random location and orientation has to navigate to the visitable bottom right corner cell of the grid by sequences of three possible actions (rotate left/right and move forward). A visualization of the environment with expert steps in a random MiniGrid-Empty-Random-5x5-v0 task looks like The observation for the agent is a subset of the entire grid, simulating a simplified limited field of view, as depicted by the highlighted rectangle (observed subset of the grid) around the agent (red arrow). Gray cells correspond to walls.","title":"The task"},{"location":"tutorials/minigrid-tutorial/#experiment-configuration-file","text":"Our complete experiment consists of: Training a basic actor-critic agent with memory to solve randomly sampled navigation tasks. Validation on a fixed set of tasks (running in parallel with training). A second stage where we test saved checkpoints with a larger fixed set of tasks. The entire configuration for the experiment, including training, validation, and testing, is encapsulated in a single class implementing the ExperimentConfig abstraction. For this tutorial, we will follow the config under projects/tutorials/minigrid_tutorial.py . The ExperimentConfig abstraction is used by the OnPolicyTrainer class (for training) and the OnPolicyInference class (for validation and testing) invoked through the entry script main.py that calls an orchestrating OnPolicyRunner class. It includes: A tag method to identify the experiment. A create_model method to instantiate actor-critic models. A make_sampler_fn method to instantiate task samplers. Three {train,valid,test}_task_sampler_args methods describing initialization parameters for task samplers used in training, validation, and testing; including assignment of workers to devices for simulation. A machine_params method with configuration parameters that will be used for training, validation, and testing. A training_pipeline method describing a possibly multi-staged training pipeline with different types of losses, an optimizer, and other parameters like learning rates, batch sizes, etc.","title":"Experiment configuration file"},{"location":"tutorials/minigrid-tutorial/#preliminaries","text":"We first import everything we'll need to define our experiment. from typing import Dict , Optional , List , Any , cast import gym from gym_minigrid.envs import EmptyRandomEnv5x5 import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from allenact.algorithms.onpolicy_sync.losses.ppo import PPO , PPOConfig from allenact.base_abstractions.experiment_config import ExperimentConfig , TaskSampler from allenact.base_abstractions.sensor import SensorSuite from allenact.utils.experiment_utils import ( TrainingPipeline , Builder , PipelineStage , LinearDecay , ) from allenact_plugins.minigrid_plugin.minigrid_models import MiniGridSimpleConvRNN from allenact_plugins.minigrid_plugin.minigrid_sensors import EgocentricMiniGridSensor from allenact_plugins.minigrid_plugin.minigrid_tasks import ( MiniGridTaskSampler , MiniGridTask , ) We now create the MiniGridTutorialExperimentConfig class which we will use to define our experiment. For pedagogical reasons, we will add methods to this class one at a time below with a description of what these classes do. class MiniGridTutorialExperimentConfig ( ExperimentConfig ): An experiment is identified by a tag . @classmethod def tag ( cls ) -> str : return \"MiniGridTutorial\"","title":"Preliminaries"},{"location":"tutorials/minigrid-tutorial/#sensors-and-model","text":"A readily available Sensor type for MiniGrid, EgocentricMiniGridSensor , allows us to extract observations in a format consumable by an ActorCriticModel agent: SENSORS = [ EgocentricMiniGridSensor ( agent_view_size = 5 , view_channels = 3 ), ] The three view_channels include objects, colors and states corresponding to a partial observation of the environment as an image tensor, equivalent to that from ImgObsWrapper in MiniGrid . The relatively large agent_view_size means the view will only be clipped by the environment walls in the forward and lateral directions with respect to the agent's orientation. We define our ActorCriticModel agent using a lightweight implementation with recurrent memory for MiniGrid environments, MiniGridSimpleConvRNN : @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return MiniGridSimpleConvRNN ( action_space = gym . spaces . Discrete ( len ( MiniGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , num_objects = cls . SENSORS [ 0 ] . num_objects , num_colors = cls . SENSORS [ 0 ] . num_colors , num_states = cls . SENSORS [ 0 ] . num_states , )","title":"Sensors and Model"},{"location":"tutorials/minigrid-tutorial/#task-samplers","text":"We use an available TaskSampler implementation for MiniGrid environments that allows to sample both random and deterministic MiniGridTasks , MiniGridTaskSampler : @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return MiniGridTaskSampler ( ** kwargs ) This task sampler will during training (or validation/testing), randomly initialize new tasks for the agent to complete. While it is not quite as important for this task type (as we test our agent in the same setting it is trained on) there are a lot of good reasons we would like to sample tasks differently during training than during validation or testing. One good reason, that is applicable in this tutorial, is that, during training, we would like to be able to sample tasks forever while, during testing, we would like to sample a fixed number of tasks (as otherwise we would never finish testing!). In allenact this is made possible by defining different arguments for the task sampler: def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"train\" ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"valid\" ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"test\" ) where, for convenience, we have defined a _get_sampler_args method: def _get_sampler_args ( self , process_ind : int , mode : str ) -> Dict [ str , Any ]: \"\"\"Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` \"\"\" if mode == \"train\" : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 20 + 20 * ( mode == \"test\" ) # 20 tasks for valid, 40 for test # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks ) ) deterministic_sampling = ( True # deterministically sample task in validation/testing ) return dict ( max_tasks = max_tasks , # see above env_class = self . make_env , # builder for third-party environment (defined below) sensors = self . SENSORS , # sensors used to return observations to the agent env_info = dict (), # parameters for environment builder (none for now) task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling , # see above ) @staticmethod def make_env ( * args , ** kwargs ): return EmptyRandomEnv5x5 () Note that the env_class argument to the Task Sampler is the one determining which task type we are going to train the model for (in this case, MiniGrid-Empty-Random-5x5-v0 from gym-minigrid ) . The sparse reward is given by the environment , and the maximum task length is 100. For training, we opt for a default random sampling, whereas for validation and test we define fixed sets of randomly sampled tasks without needing to explicitly define a dataset. In this toy example, the maximum number of different tasks is 32. For validation we sample 320 tasks using 16 samplers, or 640 for testing, so we can be fairly sure that all possible tasks are visited at least once during evaluation.","title":"Task samplers"},{"location":"tutorials/minigrid-tutorial/#machine-parameters","text":"Given the simplicity of the task and model, we can quickly train the model on the CPU: @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ) -> Dict [ str , Any ]: return { \"nprocesses\" : 128 if mode == \"train\" else 16 , \"devices\" : [], } We allocate a larger number of samplers for training (128) than for validation or testing (16), and we default to CPU usage by returning an empty list of devices .","title":"Machine parameters"},{"location":"tutorials/minigrid-tutorial/#training-pipeline","text":"The last definition required before starting to train is a training pipeline. In this case, we just use a single PPO stage with linearly decaying learning rate: @classmethod def training_pipeline ( cls , ** kwargs ) -> TrainingPipeline : ppo_steps = int ( 150000 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = PPO ( ** PPOConfig )), # type:ignore pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], optimizer_builder = Builder ( cast ( optim . Optimizer , optim . Adam ), dict ( lr = 1e-4 )), num_mini_batch = 4 , update_repeats = 3 , max_grad_norm = 0.5 , num_steps = 16 , gamma = 0.99 , use_gae = True , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 10000 , metric_accumulate_interval = 1 , lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} # type:ignore ), ) You can see that we use a Builder class to postpone the construction of some of the elements, like the optimizer, for which the model weights need to be known.","title":"Training pipeline"},{"location":"tutorials/minigrid-tutorial/#training-and-validation","text":"We have a complete implementation of this experiment's configuration class in projects/tutorials/minigrid_tutorial.py . To start training from scratch, we just need to invoke PYTHONPATH = . python allenact/main.py minigrid_tutorial -b projects/tutorials -m 8 -o /PATH/TO/minigrid_output -s 12345 from the allenact root directory. With -b projects/tutorials we tell allenact that minigrid_tutorial experiment config file will be found in the projects/tutorials directory. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o minigrid_output we set the output folder into which results and logs will be saved. With -s 12345 we set the random seed. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/minigrid_output which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to:","title":"Training and validation"},{"location":"tutorials/minigrid-tutorial/#testing","text":"The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to evaluate (i.e. test) a particular checkpoint, we need to pass the --eval flag and specify the checkpoint with the --checkpoint CHECKPOINT_PATH option: PYTHONPATH = . python allenact/main.py minigrid_tutorial \\ -b projects/tutorials \\ -m 1 \\ -o /PATH/TO/minigrid_output \\ -s 12345 \\ --eval \\ --checkpoint /PATH/TO/minigrid_output/checkpoints/MiniGridTutorial/YOUR_START_DATE/exp_MiniGridTutorial__stage_00__steps_000000151552.pt Again, if everything went well, the test success rate should converge to 1 and the mean episode length to a value below 4. Detailed results are saved under a metrics subfolder in the output folder. The test curves should look similar to:","title":"Testing"},{"location":"tutorials/offpolicy-tutorial/","text":"Tutorial: Off-policy training. # Note The provided commands to execute in this tutorial assume you have installed the full library and the extra_requirements for the babyai_plugin and minigrid_plugin . The latter can be installed with: pip install -r allenact_plugins/babyai_plugin/extra_requirements.txt ; pip install -r allenact_plugins/minigrid_plugin/extra_requirements.txt In this tutorial we'll learn how to train an agent from an external dataset by imitating expert actions via Behavior Cloning. We'll use a BabyAI agent to solve GoToLocal tasks on MiniGrid ; see the projects/babyai_baselines/experiments/go_to_local directory for more details. This tutorial assumes AllenAct 's abstractions are known. The task # In a GoToLocal task, the agent immersed in a grid world has to navigate to a specific object in the presence of multiple distractors, requiring the agent to understand go to instructions like \"go to the red ball\". For further details, please consult the original paper . Getting the dataset # We will use a large dataset ( more than 4 GB ) including expert demonstrations for GoToLocal tasks. To download the data we'll run PYTHONPATH = . python allenact_plugins/babyai_plugin/scripts/download_babyai_expert_demos.py GoToLocal from the project's root directory, which will download BabyAI-GoToLocal-v0.pkl and BabyAI-GoToLocal-v0_valid.pkl to the allenact_plugins/babyai_plugin/data/demos directory. We will also generate small versions of the datasets, which will be useful if running on CPU, by calling PYTHONPATH = . python allenact_plugins/babyai_plugin/scripts/truncate_expert_demos.py from the project's root directory, which will generate BabyAI-GoToLocal-v0-small.pkl under the same allenact_plugins/babyai_plugin/data/demos directory. Data iterator # In order to train with an off-policy dataset, we need to define a data Iterator . The Data Iterator merges the functionality of the Dataset and Dataloader in PyTorch, in that it defines the way to both sample data from the dataset and convert them into batches to be used for training. An example of a Data Iterator for BabyAI expert demos might look as follows: class ExpertTrajectoryIterator ( Iterator ): def __init__ ( self , data : List [ Tuple [ str , bytes , List [ int ], MiniGridEnv . Actions ]], nrollouts : int , rollout_len : int , instr_len : Optional [ int ], restrict_max_steps_in_dataset : Optional [ int ] = None , num_data_length_clusters : int = 8 , current_worker : Optional [ int ] = None , num_workers : Optional [ int ] = None , ): ... def add_data_to_rollout_queue ( self , q : queue . Queue , sampler : int ) -> bool : ... def get_data_for_rollout_ind ( self , rollout_ind : int ) -> Dict [ str , np . ndarray ]: ... def __next__ ( self ) -> Dict [ str , torch . Tensor ]: ... A complete example can be found in ExpertTrajectoryIterator . Loss function # Off-policy losses must implement the AbstractOffPolicyLoss interface. In this case, we minimize the cross-entropy between the actor's policy and the expert action: class MiniGridOffPolicyExpertCELoss ( AbstractOffPolicyLoss [ ActorCriticModel ]): def __init__ ( self , total_episodes_in_epoch : Optional [ int ] = None ): super () . __init__ () self . total_episodes_in_epoch = total_episodes_in_epoch def loss ( # type:ignore self , model : ActorCriticModel , batch : ObservationType , memory : Memory , * args , ** kwargs ) -> Tuple [ torch . FloatTensor , Dict [ str , float ], Memory , int ]: rollout_len , nrollouts = cast ( torch . Tensor , batch [ \"minigrid_ego_image\" ]) . shape [ : 2 ] # Initialize Memory if empty if len ( memory ) == 0 : spec = model . recurrent_memory_specification for key in spec : dims_template , dtype = spec [ key ] # get sampler_dim and all_dims from dims_template (and nrollouts) dim_names = [ d [ 0 ] for d in dims_template ] sampler_dim = dim_names . index ( \"sampler\" ) all_dims = [ d [ 1 ] for d in dims_template ] all_dims [ sampler_dim ] = nrollouts memory . check_append ( key = key , tensor = torch . zeros ( * all_dims , dtype = dtype , device = cast ( torch . Tensor , batch [ \"minigrid_ego_image\" ]) . device ), sampler_dim = sampler_dim , ) # Forward data (through the actor and critic) ac_out , memory = model . forward ( observations = batch , memory = memory , prev_actions = None , # type:ignore masks = cast ( torch . FloatTensor , batch [ \"masks\" ]), ) # Compute the loss from the actor's output and expert action expert_ce_loss = - ac_out . distributions . log_prob ( batch [ \"expert_action\" ]) . mean () info = { \"expert_ce\" : expert_ce_loss . item ()} if self . total_episodes_in_epoch is not None : if \"completed_episode_count\" not in memory : memory [ \"completed_episode_count\" ] = 0 memory [ \"completed_episode_count\" ] += ( int ( np . prod ( batch [ \"masks\" ] . shape )) # type: ignore - batch [ \"masks\" ] . sum () . item () # type: ignore ) info [ \"epoch_progress\" ] = ( memory [ \"completed_episode_count\" ] / self . total_episodes_in_epoch ) return expert_ce_loss , info , memory , rollout_len * nrollouts A complete example can be found in MiniGridOffPolicyExpertCELoss . Note that in this case we train the entire actor, but it would also be possible to forward data through a different subgraph of the ActorCriticModel. Experiment configuration # For the experiment configuration, we'll build on top of an existing base BabyAI GoToLocal Experiment Config . The complete ExperimentConfig file for off-policy training is here , but let's focus on the most relevant aspect to enable this type of training: providing an OffPolicyPipelineComponent object as input to a PipelineStage when instantiating the TrainingPipeline in the training_pipeline method. class BCOffPolicyBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ): \"\"\"BC Off-policy imitation.\"\"\" DATASET : Optional [ List [ Tuple [ str , bytes , List [ int ], MiniGridEnv . Actions ]]] = None GPU_ID = 0 if torch . cuda . is_available () else None @classmethod def tag ( cls ): return \"BabyAIGoToLocalBCOffPolicy\" @classmethod def METRIC_ACCUMULATE_INTERVAL ( cls ): # See BaseBabyAIGoToLocalExperimentConfig for how this is used. return 1 @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = cls . TOTAL_IL_TRAIN_STEPS ppo_info = cls . rl_loss_default ( \"ppo\" , steps =- 1 ) num_mini_batch = ppo_info [ \"num_mini_batch\" ] update_repeats = ppo_info [ \"update_repeats\" ] # fmt: off return cls . _training_pipeline ( named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss ( total_episodes_in_epoch = int ( 1e6 ) ), }, pipeline_stages = [ # Single stage, only with off-policy training PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # keep sampling episodes in the stage # Enable off-policy training: offpolicy_component = OffPolicyPipelineComponent ( # Pass a method to instantiate data iterators data_iterator_builder = lambda ** extra_kwargs : create_minigrid_offpolicy_data_iterator ( path = os . path . join ( BABYAI_EXPERT_TRAJECTORIES_DIR , \"BabyAI-GoToLocal-v0 {} .pkl\" . format ( \"\" if torch . cuda . is_available () else \"-small\" ), ), nrollouts = cls . NUM_TRAIN_SAMPLERS // num_mini_batch , # per trainer batch size rollout_len = cls . ROLLOUT_STEPS , instr_len = cls . INSTR_LEN , ** extra_kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = num_mini_batch * update_repeats , # number of batches per rollout ), ), ], # As we don't have any on-policy losses, we set the next # two values to zero to ensure we don't attempt to # compute gradients for on-policy rollouts: num_mini_batch = 0 , update_repeats = 0 , total_train_steps = total_train_steps , ) # fmt: on You'll have noted that it is possible to combine on-policy and off-policy training in the same stage, even though here we apply pure off-policy training. Training # We recommend using a machine with a CUDA-capable GPU for this experiment. In order to start training, we just need to invoke PYTHONPATH = . python allenact/main.py -b projects/tutorials minigrid_offpolicy_tutorial -m 8 -o <OUTPUT_PATH> Note that with the -m 8 option we limit to 8 the number of on-policy task sampling processes used between off-policy updates. If everything goes well, the training success should quickly reach values around 0.7-0.8 on GPU and converge to values close to 1 if given sufficient time to train. If running tensorboard, you'll notice a separate group of scalars named offpolicy with losses, approximate frame rate and other tracked values in addition to the standard train used for on-policy training. A view of the training progress about 5 minutes after starting on a CUDA-capable GPU should look similar to","title":"Off-policy training"},{"location":"tutorials/offpolicy-tutorial/#tutorial-off-policy-training","text":"Note The provided commands to execute in this tutorial assume you have installed the full library and the extra_requirements for the babyai_plugin and minigrid_plugin . The latter can be installed with: pip install -r allenact_plugins/babyai_plugin/extra_requirements.txt ; pip install -r allenact_plugins/minigrid_plugin/extra_requirements.txt In this tutorial we'll learn how to train an agent from an external dataset by imitating expert actions via Behavior Cloning. We'll use a BabyAI agent to solve GoToLocal tasks on MiniGrid ; see the projects/babyai_baselines/experiments/go_to_local directory for more details. This tutorial assumes AllenAct 's abstractions are known.","title":"Tutorial: Off-policy training."},{"location":"tutorials/offpolicy-tutorial/#the-task","text":"In a GoToLocal task, the agent immersed in a grid world has to navigate to a specific object in the presence of multiple distractors, requiring the agent to understand go to instructions like \"go to the red ball\". For further details, please consult the original paper .","title":"The task"},{"location":"tutorials/offpolicy-tutorial/#getting-the-dataset","text":"We will use a large dataset ( more than 4 GB ) including expert demonstrations for GoToLocal tasks. To download the data we'll run PYTHONPATH = . python allenact_plugins/babyai_plugin/scripts/download_babyai_expert_demos.py GoToLocal from the project's root directory, which will download BabyAI-GoToLocal-v0.pkl and BabyAI-GoToLocal-v0_valid.pkl to the allenact_plugins/babyai_plugin/data/demos directory. We will also generate small versions of the datasets, which will be useful if running on CPU, by calling PYTHONPATH = . python allenact_plugins/babyai_plugin/scripts/truncate_expert_demos.py from the project's root directory, which will generate BabyAI-GoToLocal-v0-small.pkl under the same allenact_plugins/babyai_plugin/data/demos directory.","title":"Getting the dataset"},{"location":"tutorials/offpolicy-tutorial/#data-iterator","text":"In order to train with an off-policy dataset, we need to define a data Iterator . The Data Iterator merges the functionality of the Dataset and Dataloader in PyTorch, in that it defines the way to both sample data from the dataset and convert them into batches to be used for training. An example of a Data Iterator for BabyAI expert demos might look as follows: class ExpertTrajectoryIterator ( Iterator ): def __init__ ( self , data : List [ Tuple [ str , bytes , List [ int ], MiniGridEnv . Actions ]], nrollouts : int , rollout_len : int , instr_len : Optional [ int ], restrict_max_steps_in_dataset : Optional [ int ] = None , num_data_length_clusters : int = 8 , current_worker : Optional [ int ] = None , num_workers : Optional [ int ] = None , ): ... def add_data_to_rollout_queue ( self , q : queue . Queue , sampler : int ) -> bool : ... def get_data_for_rollout_ind ( self , rollout_ind : int ) -> Dict [ str , np . ndarray ]: ... def __next__ ( self ) -> Dict [ str , torch . Tensor ]: ... A complete example can be found in ExpertTrajectoryIterator .","title":"Data iterator"},{"location":"tutorials/offpolicy-tutorial/#loss-function","text":"Off-policy losses must implement the AbstractOffPolicyLoss interface. In this case, we minimize the cross-entropy between the actor's policy and the expert action: class MiniGridOffPolicyExpertCELoss ( AbstractOffPolicyLoss [ ActorCriticModel ]): def __init__ ( self , total_episodes_in_epoch : Optional [ int ] = None ): super () . __init__ () self . total_episodes_in_epoch = total_episodes_in_epoch def loss ( # type:ignore self , model : ActorCriticModel , batch : ObservationType , memory : Memory , * args , ** kwargs ) -> Tuple [ torch . FloatTensor , Dict [ str , float ], Memory , int ]: rollout_len , nrollouts = cast ( torch . Tensor , batch [ \"minigrid_ego_image\" ]) . shape [ : 2 ] # Initialize Memory if empty if len ( memory ) == 0 : spec = model . recurrent_memory_specification for key in spec : dims_template , dtype = spec [ key ] # get sampler_dim and all_dims from dims_template (and nrollouts) dim_names = [ d [ 0 ] for d in dims_template ] sampler_dim = dim_names . index ( \"sampler\" ) all_dims = [ d [ 1 ] for d in dims_template ] all_dims [ sampler_dim ] = nrollouts memory . check_append ( key = key , tensor = torch . zeros ( * all_dims , dtype = dtype , device = cast ( torch . Tensor , batch [ \"minigrid_ego_image\" ]) . device ), sampler_dim = sampler_dim , ) # Forward data (through the actor and critic) ac_out , memory = model . forward ( observations = batch , memory = memory , prev_actions = None , # type:ignore masks = cast ( torch . FloatTensor , batch [ \"masks\" ]), ) # Compute the loss from the actor's output and expert action expert_ce_loss = - ac_out . distributions . log_prob ( batch [ \"expert_action\" ]) . mean () info = { \"expert_ce\" : expert_ce_loss . item ()} if self . total_episodes_in_epoch is not None : if \"completed_episode_count\" not in memory : memory [ \"completed_episode_count\" ] = 0 memory [ \"completed_episode_count\" ] += ( int ( np . prod ( batch [ \"masks\" ] . shape )) # type: ignore - batch [ \"masks\" ] . sum () . item () # type: ignore ) info [ \"epoch_progress\" ] = ( memory [ \"completed_episode_count\" ] / self . total_episodes_in_epoch ) return expert_ce_loss , info , memory , rollout_len * nrollouts A complete example can be found in MiniGridOffPolicyExpertCELoss . Note that in this case we train the entire actor, but it would also be possible to forward data through a different subgraph of the ActorCriticModel.","title":"Loss function"},{"location":"tutorials/offpolicy-tutorial/#experiment-configuration","text":"For the experiment configuration, we'll build on top of an existing base BabyAI GoToLocal Experiment Config . The complete ExperimentConfig file for off-policy training is here , but let's focus on the most relevant aspect to enable this type of training: providing an OffPolicyPipelineComponent object as input to a PipelineStage when instantiating the TrainingPipeline in the training_pipeline method. class BCOffPolicyBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ): \"\"\"BC Off-policy imitation.\"\"\" DATASET : Optional [ List [ Tuple [ str , bytes , List [ int ], MiniGridEnv . Actions ]]] = None GPU_ID = 0 if torch . cuda . is_available () else None @classmethod def tag ( cls ): return \"BabyAIGoToLocalBCOffPolicy\" @classmethod def METRIC_ACCUMULATE_INTERVAL ( cls ): # See BaseBabyAIGoToLocalExperimentConfig for how this is used. return 1 @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = cls . TOTAL_IL_TRAIN_STEPS ppo_info = cls . rl_loss_default ( \"ppo\" , steps =- 1 ) num_mini_batch = ppo_info [ \"num_mini_batch\" ] update_repeats = ppo_info [ \"update_repeats\" ] # fmt: off return cls . _training_pipeline ( named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss ( total_episodes_in_epoch = int ( 1e6 ) ), }, pipeline_stages = [ # Single stage, only with off-policy training PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # keep sampling episodes in the stage # Enable off-policy training: offpolicy_component = OffPolicyPipelineComponent ( # Pass a method to instantiate data iterators data_iterator_builder = lambda ** extra_kwargs : create_minigrid_offpolicy_data_iterator ( path = os . path . join ( BABYAI_EXPERT_TRAJECTORIES_DIR , \"BabyAI-GoToLocal-v0 {} .pkl\" . format ( \"\" if torch . cuda . is_available () else \"-small\" ), ), nrollouts = cls . NUM_TRAIN_SAMPLERS // num_mini_batch , # per trainer batch size rollout_len = cls . ROLLOUT_STEPS , instr_len = cls . INSTR_LEN , ** extra_kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = num_mini_batch * update_repeats , # number of batches per rollout ), ), ], # As we don't have any on-policy losses, we set the next # two values to zero to ensure we don't attempt to # compute gradients for on-policy rollouts: num_mini_batch = 0 , update_repeats = 0 , total_train_steps = total_train_steps , ) # fmt: on You'll have noted that it is possible to combine on-policy and off-policy training in the same stage, even though here we apply pure off-policy training.","title":"Experiment configuration"},{"location":"tutorials/offpolicy-tutorial/#training","text":"We recommend using a machine with a CUDA-capable GPU for this experiment. In order to start training, we just need to invoke PYTHONPATH = . python allenact/main.py -b projects/tutorials minigrid_offpolicy_tutorial -m 8 -o <OUTPUT_PATH> Note that with the -m 8 option we limit to 8 the number of on-policy task sampling processes used between off-policy updates. If everything goes well, the training success should quickly reach values around 0.7-0.8 on GPU and converge to values close to 1 if given sufficient time to train. If running tensorboard, you'll notice a separate group of scalars named offpolicy with losses, approximate frame rate and other tracked values in addition to the standard train used for on-policy training. A view of the training progress about 5 minutes after starting on a CUDA-capable GPU should look similar to","title":"Training"},{"location":"tutorials/running-inference-on-a-pretrained-model/","text":"Tutorial: Inference with a pre-trained model. # In this tutorial we will run inference on a pre-trained model for the PointNav task in the RoboTHOR environment. In this task the agent is tasked with going to a specific location within a realistic 3D environment. For information on how to train a PointNav Model see this tutorial We will need to install the full AllenAct library , the robothor_plugin requirements via pip install -r allenact_plugins/robothor_plugin/extra_requirements.txt and download the RoboTHOR Pointnav dataset before we get started. For this tutorial we will download the weights of a model trained on the debug dataset. This can be done with a handy script in the pretrained_model_ckpts directory: bash pretrained_model_ckpts/download_navigation_model_ckpts.sh robothor-pointnav-rgb-resnet This will download the weights for an RGB model that has been trained on the PointNav task in RoboTHOR to pretrained_model_ckpts/robothor-pointnav-rgb-resnet Next we need to run the inference, using the PointNav experiment config from the tutorial on making a PointNav experiment . We can do this with the following command: PYTHONPATH = . python allenact/main.py -o <PATH_TO_OUTPUT> -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> -c <PATH_TO_CHECKPOINT> --eval Where <PATH_TO_OUTPUT> is the location where the results of the test will be dumped, <PATH_TO_CHECKPOINT> is the location of the downloaded model weights, and <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> is a path to the directory where our experiment definition is stored. For our current setup the following command would work: PYTHONPATH = . python allenact/main.py \\ training_a_pointnav_model \\ -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -b projects/tutorials \\ -c pretrained_model_ckpts/robothor-pointnav-rgb-resnet/checkpoints/PointNavRobothorRGBPPO/2020-08-31_12-13-30/exp_PointNavRobothorRGBPPO__stage_00__steps_000039031200.pt \\ --eval For testing on all saved checkpoints we pass a directory to --checkpoint rather than just a single file: PYTHONPATH = . python allenact/main.py \\ training_a_pointnav_model \\ -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -b projects/tutorials \\ -c pretrained_model_ckpts/robothor-pointnav-rgb-resnet/checkpoints/PointNavRobothorRGBPPO/2020-08-31_12-13-30 --eval Visualization # We also show examples of visualizations that can be extracted from the \"valid\" and \"test\" modes. Currently, visualization is still undergoing design changes and does not support multi-agent tasks, but the available functionality is sufficient for pointnav in RoboThor. Following up on the example above, we can make a specialized pontnav ExperimentConfig where we instantiate the base visualization class, VizSuite , defined in allenact.utils.viz_utils , when in test mode. Each visualization type can be thought of as a plugin to the base VizSuite . For example, all episode_ids passed to VizSuite will be processed with each of the instantiated visualization types (possibly with the exception of the AgentViewViz ). In the example below we show how to instantiate different visualization types from 4 different data sources. The data sources available to VizSuite are: Task output (e.g. 2D trajectories) Vector task (e.g. egocentric views) Rollout storage (e.g. recurrent memory, taken action logprobs...) ActorCriticOutput (e.g. action probabilities) The visualization types included below are: TrajectoryViz : Generic 2D trajectory view. AgentViewViz : RGB egocentric view. ActorViz : Action probabilities from ActorCriticOutput[CategoricalDistr] . TensorViz1D : Evolution of a point from RolloutStorage over time. TensorViz2D : Evolution of a vector from RolloutStorage over time. ThorViz : Specialized 2D trajectory view for RoboThor . Note that we need to explicitly set the episode_ids that we wish to visualize. For AgentViewViz we have the option of using a different (typically shorter) list of episodes or enforce the ones used for the rest of visualizations. class PointNavRoboThorRGBPPOVizExperimentConfig ( PointNavRoboThorRGBPPOExperimentConfig ): \"\"\"ExperimentConfig used to demonstrate how to set up visualization code. # Attributes viz_ep_ids : Scene names that will be visualized. viz_video_ids : Scene names that will have videos visualizations associated with them. \"\"\" viz_ep_ids = [ \"FloorPlan_Train1_1_3\" , \"FloorPlan_Train1_1_4\" , \"FloorPlan_Train1_1_5\" , \"FloorPlan_Train1_1_6\" , ] viz_video_ids = [[ \"FloorPlan_Train1_1_3\" ], [ \"FloorPlan_Train1_1_4\" ]] viz : Optional [ VizSuite ] = None def get_viz ( self , mode ): if self . viz is not None : return self . viz self . viz = VizSuite ( episode_ids = self . viz_ep_ids , mode = mode , # Basic 2D trajectory visualizer (task output source): base_trajectory = TrajectoryViz ( path_to_target_location = ( \"task_info\" , \"target\" ,), ), # Egocentric view visualizer (vector task source): egeocentric = AgentViewViz ( max_video_length = 100 , episode_ids = self . viz_video_ids ), # Default action probability visualizer (actor critic output source): action_probs = ActorViz ( figsize = ( 3.25 , 10 ), fontsize = 18 ), # Default taken action logprob visualizer (rollout storage source): taken_action_logprobs = TensorViz1D (), # Same episode mask visualizer (rollout storage source): episode_mask = TensorViz1D ( rollout_source = ( \"masks\" ,)), # Default recurrent memory visualizer (rollout storage source): rnn_memory = TensorViz2D (), # Specialized 2D trajectory visualizer (task output source): thor_trajectory = ThorViz ( figsize = ( 16 , 8 ), viz_rows_cols = ( 448 , 448 ), scenes = ( \"FloorPlan_Train {} _ {} \" , 1 , 1 , 1 , 1 ), ), ) return self . viz def machine_params ( self , mode = \"train\" , ** kwargs ): res = super () . machine_params ( mode , ** kwargs ) if mode == \"test\" : res . set_visualizer ( self . get_viz ( mode )) return res Running test on the same downloaded models, but using the visualization-enabled ExperimentConfig with PYTHONPATH = . python allenact/main.py \\ running_inference_tutorial \\ -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -b projects/tutorials \\ -c pretrained_model_ckpts/robothor-pointnav-rgb-resnet/checkpoints/PointNavRobothorRGBPPO/2020-08-31_12-13-30/exp_PointNavRobothorRGBPPO__stage_00__steps_000039031200.pt \\ --eval generates different types of visualization and logs them in tensorboard. If everything is properly setup and tensorboard includes the robothor-pointnav-rgb-resnet folder, under the IMAGES tab, we should see something similar to","title":"Using a pre-trained model"},{"location":"tutorials/running-inference-on-a-pretrained-model/#tutorial-inference-with-a-pre-trained-model","text":"In this tutorial we will run inference on a pre-trained model for the PointNav task in the RoboTHOR environment. In this task the agent is tasked with going to a specific location within a realistic 3D environment. For information on how to train a PointNav Model see this tutorial We will need to install the full AllenAct library , the robothor_plugin requirements via pip install -r allenact_plugins/robothor_plugin/extra_requirements.txt and download the RoboTHOR Pointnav dataset before we get started. For this tutorial we will download the weights of a model trained on the debug dataset. This can be done with a handy script in the pretrained_model_ckpts directory: bash pretrained_model_ckpts/download_navigation_model_ckpts.sh robothor-pointnav-rgb-resnet This will download the weights for an RGB model that has been trained on the PointNav task in RoboTHOR to pretrained_model_ckpts/robothor-pointnav-rgb-resnet Next we need to run the inference, using the PointNav experiment config from the tutorial on making a PointNav experiment . We can do this with the following command: PYTHONPATH = . python allenact/main.py -o <PATH_TO_OUTPUT> -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> -c <PATH_TO_CHECKPOINT> --eval Where <PATH_TO_OUTPUT> is the location where the results of the test will be dumped, <PATH_TO_CHECKPOINT> is the location of the downloaded model weights, and <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> is a path to the directory where our experiment definition is stored. For our current setup the following command would work: PYTHONPATH = . python allenact/main.py \\ training_a_pointnav_model \\ -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -b projects/tutorials \\ -c pretrained_model_ckpts/robothor-pointnav-rgb-resnet/checkpoints/PointNavRobothorRGBPPO/2020-08-31_12-13-30/exp_PointNavRobothorRGBPPO__stage_00__steps_000039031200.pt \\ --eval For testing on all saved checkpoints we pass a directory to --checkpoint rather than just a single file: PYTHONPATH = . python allenact/main.py \\ training_a_pointnav_model \\ -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -b projects/tutorials \\ -c pretrained_model_ckpts/robothor-pointnav-rgb-resnet/checkpoints/PointNavRobothorRGBPPO/2020-08-31_12-13-30 --eval","title":"Tutorial: Inference with a pre-trained model."},{"location":"tutorials/running-inference-on-a-pretrained-model/#visualization","text":"We also show examples of visualizations that can be extracted from the \"valid\" and \"test\" modes. Currently, visualization is still undergoing design changes and does not support multi-agent tasks, but the available functionality is sufficient for pointnav in RoboThor. Following up on the example above, we can make a specialized pontnav ExperimentConfig where we instantiate the base visualization class, VizSuite , defined in allenact.utils.viz_utils , when in test mode. Each visualization type can be thought of as a plugin to the base VizSuite . For example, all episode_ids passed to VizSuite will be processed with each of the instantiated visualization types (possibly with the exception of the AgentViewViz ). In the example below we show how to instantiate different visualization types from 4 different data sources. The data sources available to VizSuite are: Task output (e.g. 2D trajectories) Vector task (e.g. egocentric views) Rollout storage (e.g. recurrent memory, taken action logprobs...) ActorCriticOutput (e.g. action probabilities) The visualization types included below are: TrajectoryViz : Generic 2D trajectory view. AgentViewViz : RGB egocentric view. ActorViz : Action probabilities from ActorCriticOutput[CategoricalDistr] . TensorViz1D : Evolution of a point from RolloutStorage over time. TensorViz2D : Evolution of a vector from RolloutStorage over time. ThorViz : Specialized 2D trajectory view for RoboThor . Note that we need to explicitly set the episode_ids that we wish to visualize. For AgentViewViz we have the option of using a different (typically shorter) list of episodes or enforce the ones used for the rest of visualizations. class PointNavRoboThorRGBPPOVizExperimentConfig ( PointNavRoboThorRGBPPOExperimentConfig ): \"\"\"ExperimentConfig used to demonstrate how to set up visualization code. # Attributes viz_ep_ids : Scene names that will be visualized. viz_video_ids : Scene names that will have videos visualizations associated with them. \"\"\" viz_ep_ids = [ \"FloorPlan_Train1_1_3\" , \"FloorPlan_Train1_1_4\" , \"FloorPlan_Train1_1_5\" , \"FloorPlan_Train1_1_6\" , ] viz_video_ids = [[ \"FloorPlan_Train1_1_3\" ], [ \"FloorPlan_Train1_1_4\" ]] viz : Optional [ VizSuite ] = None def get_viz ( self , mode ): if self . viz is not None : return self . viz self . viz = VizSuite ( episode_ids = self . viz_ep_ids , mode = mode , # Basic 2D trajectory visualizer (task output source): base_trajectory = TrajectoryViz ( path_to_target_location = ( \"task_info\" , \"target\" ,), ), # Egocentric view visualizer (vector task source): egeocentric = AgentViewViz ( max_video_length = 100 , episode_ids = self . viz_video_ids ), # Default action probability visualizer (actor critic output source): action_probs = ActorViz ( figsize = ( 3.25 , 10 ), fontsize = 18 ), # Default taken action logprob visualizer (rollout storage source): taken_action_logprobs = TensorViz1D (), # Same episode mask visualizer (rollout storage source): episode_mask = TensorViz1D ( rollout_source = ( \"masks\" ,)), # Default recurrent memory visualizer (rollout storage source): rnn_memory = TensorViz2D (), # Specialized 2D trajectory visualizer (task output source): thor_trajectory = ThorViz ( figsize = ( 16 , 8 ), viz_rows_cols = ( 448 , 448 ), scenes = ( \"FloorPlan_Train {} _ {} \" , 1 , 1 , 1 , 1 ), ), ) return self . viz def machine_params ( self , mode = \"train\" , ** kwargs ): res = super () . machine_params ( mode , ** kwargs ) if mode == \"test\" : res . set_visualizer ( self . get_viz ( mode )) return res Running test on the same downloaded models, but using the visualization-enabled ExperimentConfig with PYTHONPATH = . python allenact/main.py \\ running_inference_tutorial \\ -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -b projects/tutorials \\ -c pretrained_model_ckpts/robothor-pointnav-rgb-resnet/checkpoints/PointNavRobothorRGBPPO/2020-08-31_12-13-30/exp_PointNavRobothorRGBPPO__stage_00__steps_000039031200.pt \\ --eval generates different types of visualization and logs them in tensorboard. If everything is properly setup and tensorboard includes the robothor-pointnav-rgb-resnet folder, under the IMAGES tab, we should see something similar to","title":"Visualization"},{"location":"tutorials/training-a-pointnav-model/","text":"Tutorial: PointNav in RoboTHOR. # Introduction # One of the most obvious tasks that an embodied agent should master is navigating the world it inhabits. Before we can teach a robot to cook or clean it first needs to be able to move around. The simplest way to formulate \"moving around\" into a task is by making your agent find a beacon somewhere in the environment. This beacon transmits its location, such that at any time, the agent can get the direction and euclidian distance to the beacon. This particular task is often called Point Navigation, or PointNav for short. PointNav # At first glance, this task seems trivial. If the agent is given the direction and distance of the target at all times, can it not simply follow this signal directly? The answer is no, because agents are often trained on this task in environments that emulate real-world buildings which are not wide-open spaces, but rather contain many smaller rooms. Because of this, the agent has to learn to navigate human spaces and use doors and hallways to efficiently navigate from one side of the building to the other. This task becomes particularly difficult when the agent is tested in an environment that it is not trained in. If the agent does not know how the floor plan of an environment looks, it has to learn to predict the design of man-made structures, to efficiently navigate across them, much like how people instinctively know how to move around a building they have never seen before based on their experience navigating similar buildings. What is an environment anyways? # Environments are worlds in which embodied agents exist. If our embodied agent is simply a neural network that is being trained in a simulator, then that simulator is its environment. Similarly, if our agent is a physical robot then its environment is the real world. The agent interacts with the environment by taking one of several available actions (such as \"move forward\", or \"turn left\"). After each action, the environment produces a new frame that the agent can analyze to determine its next step. For many tasks, including PointNav the agent also has a special \"stop\" action which indicates that the agent thinks it has reached the target. After this action is called the agent will be reset to a new location, regardless if it reached the target. The hope is that after enough training the agent will learn to correctly assess that it has successfully navigated to the target. There are many simulators designed for the training of embodied agents. In this tutorial, we will be using a simulator called RoboTHOR , which is designed specifically to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment that researchers can have access to. RoboTHOR contains 60 different virtual scenes with different floor plans and furniture and 15 validation scenes. It is also important to mention that AllenAct has a class abstraction called Environment. This is not the actual simulator game engine or robotics controller, but rather a shallow wrapper that provides a uniform interface to the actual environment. Learning algorithm # Finally, let us briefly touch on the algorithm that we will use to train our embodied agent to navigate. While AllenAct offers us great flexibility to train models using complex pipelines, we will be using a simple pure reinforcement learning approach for this tutorial. More specifically, we will be using DD-PPO, a decentralized and distributed variant of the ubiquitous PPO algorithm. For those unfamiliar with Reinforcement Learning we highly recommend this tutorial by Andrej Karpathy, and this book by Sutton and Barto. Essentially what we are doing is letting our agent explore the environment on its own, rewarding it for taking actions that bring it closer to its goal and penalizing it for actions that take it away from its goal. We then optimize the agent's model to maximize this reward. Requirements # To train the model on the PointNav task, we need to install the RoboTHOR environment and download the RoboTHOR PointNav dataset The dataset contains a list of episodes with thousands of randomly generated starting positions and target locations for each of the scenes as well as a precomputed cache of distances, containing the shortest path from each point in a scene, to every other point in that scene. This is used to reward the agent for moving closer to the target in terms of geodesic distance - the actual path distance (as opposed to a straight line distance). Config File Setup # Now comes the most important part of the tutorial, we are going to write an experiment config file. If this is your first experience with experiment config files in AllenAct, we suggest that you first see our how-to on defining an experiment which will walk you through creating a simplified experiment config file. Unlike a library that can be imported into python, AllenAct is structured as a framework with a runner script called main.py which will run the experiment specified in a config file. This design forces us to keep meticulous records of exactly which settings were used to produce a particular result, which can be very useful given how expensive RL models are to train. The projects/ directory is home to different projects using AllenAct . Currently it is populated with baselines of popular tasks and tutorials. We already have all the code for this tutorial stored in projects/tutorials/training_a_pointnav_model.py . We will be using this file to run our experiments, but you can create a new directory in projects/ and start writing your experiment there. We start off by importing everything we will need: import glob import os from math import ceil from typing import Dict , Any , List , Optional , Sequence import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from torchvision import models from allenact.algorithms.onpolicy_sync.losses import PPO from allenact.algorithms.onpolicy_sync.losses.ppo import PPOConfig from allenact.base_abstractions.experiment_config import ExperimentConfig , MachineParams from allenact.base_abstractions.preprocessor import SensorPreprocessorGraph from allenact.base_abstractions.sensor import SensorSuite from allenact.base_abstractions.task import TaskSampler from allenact.embodiedai.preprocessors.resnet import ResNetPreprocessor from allenact.utils.experiment_utils import ( Builder , PipelineStage , TrainingPipeline , LinearDecay , evenly_distribute_count_into_bins , ) from allenact_plugins.ithor_plugin.ithor_sensors import RGBSensorThor from allenact_plugins.robothor_plugin.robothor_sensors import GPSCompassSensorRoboThor from allenact_plugins.robothor_plugin.robothor_task_samplers import ( PointNavDatasetTaskSampler , ) from allenact_plugins.robothor_plugin.robothor_tasks import PointNavTask from projects.pointnav_baselines.models.point_nav_models import ( ResnetTensorPointNavActorCritic , ) Next we define a new experiment config class: class PointNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ): \"\"\"A Point Navigation experiment configuration in RoboThor.\"\"\" We then define the task parameters. For PointNav, these include the maximum number of steps our agent can take before being reset (this prevents the agent from wandering on forever), and a configuration for the reward function that we will be using. # Task Parameters MAX_STEPS = 500 REWARD_CONFIG = { \"step_penalty\" : - 0.01 , \"goal_success_reward\" : 10.0 , \"failed_stop_reward\" : 0.0 , \"shaping_weight\" : 1.0 , } In this case, we set the maximum number of steps to 500. We give the agent a reward of -0.01 for each action that it takes (this is to encourage it to reach the goal in as few actions as possible), and a reward of 10.0 if the agent manages to successfully reach its destination. If the agent selects the stop action without reaching the target we do not punish it (although this is sometimes useful for preventing the agent from stopping prematurely). Finally, our agent gets rewarded if it moves closer to the target and gets punished if it moves further away. shaping_weight controls how strong this signal should be and is here set to 1.0. These parameters work well for training an agent on PointNav, but feel free to play around with them. Next, we set the parameters of the simulator itself. Here we select a resolution at which the engine will render every frame (640 by 480) and a resolution at which the image will be fed into the neural network (here it is set to a 224 by 224 box). # Simulator Parameters CAMERA_WIDTH = 640 CAMERA_HEIGHT = 480 SCREEN_SIZE = 224 Next, we set the hardware parameters for the training engine. NUM_PROCESSES sets the total number of parallel processes that will be used to train the model. In general, more processes result in faster training, but since each process is a unique instance of the environment in which we are training they can take up a lot of memory. Depending on the size of the model, the environment, and the hardware we are using, we may need to adjust this number, but for a setup with 8 GTX Titans, 60 processes work fine. 60 also happens to be the number of training scenes in RoboTHOR, which allows each process to load only a single scene into memory, saving time and space. TRAINING_GPUS takes the ids of the GPUS on which the model should be trained. Similarly VALIDATION_GPUS and TESTING_GPUS hold the ids of the GPUS on which the validation and testing will occur. During training, a validation process is constantly running and evaluating the current model, to show the progress on the validation set, so reserving a GPU for validation can be a good idea. If our hardware setup does not include a GPU, these fields can be set to empty lists, as the codebase will default to running everything on the CPU with only 1 process. ADVANCE_SCENE_ROLLOUT_PERIOD : Optional [ int ] = None NUM_PROCESSES = 20 TRAINING_GPUS : Sequence [ int ] = [ 0 ] VALIDATION_GPUS : Sequence [ int ] = [ 0 ] TESTING_GPUS : Sequence [ int ] = [ 0 ] Since we are using a dataset to train our model we need to define the path to where we have stored it. If we download the dataset instructed above we can define the path as follows TRAIN_DATASET_DIR = os . path . join ( os . getcwd (), \"datasets/robothor-pointnav/debug\" ) VAL_DATASET_DIR = os . path . join ( os . getcwd (), \"datasets/robothor-pointnav/debug\" ) Next, we define the sensors. RGBSensorThor is the environment's implementation of an RGB sensor. It takes the raw image outputted by the simulator and resizes it, to the input dimensions for our neural network that we specified above. It also performs normalization if we want. GPSCompassSensorRoboThor is a sensor that tracks the point our agent needs to move to. It tells us the direction and distance to our goal at every time step. SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , uuid = \"rgb_lowres\" , ), GPSCompassSensorRoboThor (), ] For the sake of this example, we are also going to be using a preprocessor with our model. In AllenAct the preprocessor abstraction is designed with large models with frozen weights in mind. These models often hail from the ResNet family and transform the raw pixels that our agent observes in the environment, into a complex embedding, which then gets stored and used as input to our trainable model instead of the original image. Most other preprocessing work is done in the sensor classes (as we just saw with the RGB sensor scaling and normalizing our input), but for the sake of efficiency, all neural network preprocessing should use this abstraction. PREPROCESSORS = [ Builder ( ResNetPreprocessor , { \"input_height\" : SCREEN_SIZE , \"input_width\" : SCREEN_SIZE , \"output_width\" : 7 , \"output_height\" : 7 , \"output_dims\" : 512 , \"pool\" : False , \"torchvision_resnet_model\" : models . resnet18 , \"input_uuids\" : [ \"rgb_lowres\" ], \"output_uuid\" : \"rgb_resnet\" , }, ), ] Next, we must define all of the observation inputs that our model will use. These are just the hardcoded ids of the sensors we are using in the experiment. OBSERVATIONS = [ \"rgb_resnet\" , \"target_coordinates_ind\" , ] Finally, we must define the settings of our simulator. We set the camera dimensions to the values we defined earlier. We set rotateStepDegrees to 30 degrees, which means that every time the agent takes a turn action, they will rotate by 30 degrees. We set grid size to 0.25 which means that every time the agent moves forward, it will do so by 0.25 meters. ENV_ARGS = dict ( width = CAMERA_WIDTH , height = CAMERA_HEIGHT , rotateStepDegrees = 30.0 , visibilityDistance = 1.0 , gridSize = 0.25 , ) Now we move on to the methods that we must define to finish implementing an experiment config. Firstly we have a simple method that just returns the name of the experiment. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" Next, we define the training pipeline. In this function, we specify exactly which algorithm or algorithms we will use to train our model. In this simple example, we are using the PPO loss with a learning rate of 3e-4. We specify 250 million steps of training and a rollout length of 30 with the ppo_steps and num_steps parameters respectively. All the other standard PPO parameters are also present in this function. metric_accumulate_interval sets the frequency at which data is accumulated from all the processes and logged while save_interval sets how often we save the model weights and run validation on them. @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 250000000 ) lr = 3e-4 num_mini_batch = 1 update_repeats = 3 num_steps = 30 save_interval = 5000000 log_interval = 1000 gamma = 0.99 use_gae = True gae_lambda = 0.95 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = log_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( ** PPOConfig )}, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) The machine_params method returns the hardware parameters of each process, based on the list of devices we defined above. def machine_params ( self , mode = \"train\" , ** kwargs ): sampler_devices : List [ int ] = [] if mode == \"train\" : workers_per_device = 1 gpu_ids = ( [] if not torch . cuda . is_available () else list ( self . TRAINING_GPUS ) * workers_per_device ) nprocesses = ( 8 if not torch . cuda . is_available () else evenly_distribute_count_into_bins ( self . NUM_PROCESSES , len ( gpu_ids )) ) sampler_devices = list ( self . TRAINING_GPUS ) elif mode == \"valid\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . VALIDATION_GPUS elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TESTING_GPUS else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) sensor_preprocessor_graph = ( SensorPreprocessorGraph ( source_observation_spaces = SensorSuite ( self . SENSORS ) . observation_spaces , preprocessors = self . PREPROCESSORS , ) if mode == \"train\" or ( ( isinstance ( nprocesses , int ) and nprocesses > 0 ) or ( isinstance ( nprocesses , Sequence ) and sum ( nprocesses ) > 0 ) ) else None ) return MachineParams ( nprocesses = nprocesses , devices = gpu_ids , sampler_devices = sampler_devices if mode == \"train\" else gpu_ids , # ignored with > 1 gpu_ids sensor_preprocessor_graph = sensor_preprocessor_graph , ) Now we define the actual model that we will be using. AllenAct offers first-class support for PyTorch, so any PyTorch model that implements the provided ActorCriticModel class will work here. Here we borrow a modelfrom the pointnav_baselines project (which unsurprisingly contains several PointNav baselines). It is a small convolutional network that expects the output of a ResNet as its rgb input followed by a single-layered GRU. The model accepts as input the number of different actions our agent can perform in the environment through the action_space parameter, which we get from the task definition. We also define the shape of the inputs we are going to be passing to the model with observation_space We specify the names of our sensors with goal_sensor_uuid and rgb_resnet_preprocessor_uuid . Finally, we define the size of our RNN with hidden_layer and the size of the embedding of our goal sensor data (the direction and distance to the target) with goal_dims . @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ResnetTensorPointNavActorCritic ( action_space = gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), observation_space = kwargs [ \"sensor_preprocessor_graph\" ] . observation_spaces , goal_sensor_uuid = \"target_coordinates_ind\" , rgb_resnet_preprocessor_uuid = \"rgb_resnet\" , hidden_size = 512 , goal_dims = 32 , ) We also need to define the task sampler that we will be using. This is a piece of code that generates instances of tasks for our agent to perform (essentially starting locations and targets for PointNav). Since we are getting our tasks from a dataset, the task sampler is a very simple code that just reads the specified file and sets the agent to the next starting locations whenever the agent exceeds the maximum number of steps or selects the stop action. @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavDatasetTaskSampler ( ** kwargs ) You might notice that we did not specify the task sampler's arguments, but are rather passing them in. The reason for this is that each process will have its own task sampler, and we need to specify exactly which scenes each process should work with. If we have several GPUS and many scenes this process of distributing the work can be rather complicated so we define a few helper functions to do just this. @staticmethod def _partition_inds ( n : int , num_parts : int ): return np . round ( np . linspace ( 0 , n , num_parts + 1 , endpoint = True )) . astype ( np . int32 ) def _get_sampler_args_for_scene_split ( self , scenes_dir : str , process_ind : int , total_processes : int , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: path = os . path . join ( scenes_dir , \"*.json.gz\" ) scenes = [ scene . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for scene in glob . glob ( path )] if len ( scenes ) == 0 : raise RuntimeError ( ( \"Could find no scene dataset information in directory {} .\" \" Are you sure you've downloaded them? \" \" If not, see https://allenact.org/installation/download-datasets/ information\" \" on how this can be done.\" ) . format ( scenes_dir ) ) if total_processes > len ( scenes ): # oversample some scenes -> bias if total_processes % len ( scenes ) != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisible by the number of scenes\" ) scenes = scenes * int ( ceil ( total_processes / len ( scenes ))) scenes = scenes [: total_processes * ( len ( scenes ) // total_processes )] else : if len ( scenes ) % total_processes != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisor of the number of scenes\" ) inds = self . _partition_inds ( len ( scenes ), total_processes ) return { \"scenes\" : scenes [ inds [ process_ind ] : inds [ process_ind + 1 ]], \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), \"seed\" : seeds [ process_ind ] if seeds is not None else None , \"deterministic_cudnn\" : deterministic_cudnn , \"rewards_config\" : self . REWARD_CONFIG , } The very last things we need to define are the sampler arguments themselves. We define them separately for a train, validation, and test sampler, but in this case, they are almost the same. The arguments need to include the location of the dataset and distance cache as well as the environment arguments for our simulator, both of which we defined above and are just referencing here. The only consequential differences between these task samplers are the path to the dataset we are using (train or validation) and whether we want to loop over the dataset or not (we want this for training since we want to train for several epochs, but we do not need this for validation and testing). Since the test scenes of RoboTHOR are private we are also testing on our validation set. def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( os . path . join ( self . TRAIN_DATASET_DIR , \"episodes\" ), process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . TRAIN_DATASET_DIR res [ \"loop_dataset\" ] = True res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) res [ \"allow_flipping\" ] = True return res def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( os . path . join ( self . VAL_DATASET_DIR , \"episodes\" ), process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) return res def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( os . path . join ( self . VAL_DATASET_DIR , \"episodes\" ), process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) return res This is it! If we copy all of the code into a file we should be able to run our experiment! Training Model On Debug Dataset # We can test if our installation worked properly by training our model on a small dataset of 4 episodes. This should take about 20 minutes on a computer with a NVIDIA GPU. We can now train a model by running: PYTHONPATH = . python allenact/main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> If using the same configuration as we have set up, the following command should work: PYTHONPATH = . python allenact/main.py training_a_pointnav_model -o storage/robothor-pointnav-rgb-resnet-resnet -b projects/tutorials If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this: Training Model On Full Dataset # We can also train the model on the full dataset by changing back our dataset path and running the same command as above. But be aware, training this takes nearly 2 days on a machine with 8 GPU. Testing Model # To test the performance of a model please refer to this tutorial . Conclusion # In this tutorial, we learned how to create a new PointNav experiment using AllenAct . There are many simple and obvious ways to modify the experiment from here - changing the model, the learning algorithm and the environment each requires very few lines of code changed in the above file, allowing us to explore our embodied ai research ideas across different frameworks with ease.","title":"PointNav in RoboTHOR"},{"location":"tutorials/training-a-pointnav-model/#tutorial-pointnav-in-robothor","text":"","title":"Tutorial: PointNav in RoboTHOR."},{"location":"tutorials/training-a-pointnav-model/#introduction","text":"One of the most obvious tasks that an embodied agent should master is navigating the world it inhabits. Before we can teach a robot to cook or clean it first needs to be able to move around. The simplest way to formulate \"moving around\" into a task is by making your agent find a beacon somewhere in the environment. This beacon transmits its location, such that at any time, the agent can get the direction and euclidian distance to the beacon. This particular task is often called Point Navigation, or PointNav for short.","title":"Introduction"},{"location":"tutorials/training-a-pointnav-model/#pointnav","text":"At first glance, this task seems trivial. If the agent is given the direction and distance of the target at all times, can it not simply follow this signal directly? The answer is no, because agents are often trained on this task in environments that emulate real-world buildings which are not wide-open spaces, but rather contain many smaller rooms. Because of this, the agent has to learn to navigate human spaces and use doors and hallways to efficiently navigate from one side of the building to the other. This task becomes particularly difficult when the agent is tested in an environment that it is not trained in. If the agent does not know how the floor plan of an environment looks, it has to learn to predict the design of man-made structures, to efficiently navigate across them, much like how people instinctively know how to move around a building they have never seen before based on their experience navigating similar buildings.","title":"PointNav"},{"location":"tutorials/training-a-pointnav-model/#what-is-an-environment-anyways","text":"Environments are worlds in which embodied agents exist. If our embodied agent is simply a neural network that is being trained in a simulator, then that simulator is its environment. Similarly, if our agent is a physical robot then its environment is the real world. The agent interacts with the environment by taking one of several available actions (such as \"move forward\", or \"turn left\"). After each action, the environment produces a new frame that the agent can analyze to determine its next step. For many tasks, including PointNav the agent also has a special \"stop\" action which indicates that the agent thinks it has reached the target. After this action is called the agent will be reset to a new location, regardless if it reached the target. The hope is that after enough training the agent will learn to correctly assess that it has successfully navigated to the target. There are many simulators designed for the training of embodied agents. In this tutorial, we will be using a simulator called RoboTHOR , which is designed specifically to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment that researchers can have access to. RoboTHOR contains 60 different virtual scenes with different floor plans and furniture and 15 validation scenes. It is also important to mention that AllenAct has a class abstraction called Environment. This is not the actual simulator game engine or robotics controller, but rather a shallow wrapper that provides a uniform interface to the actual environment.","title":"What is an environment anyways?"},{"location":"tutorials/training-a-pointnav-model/#learning-algorithm","text":"Finally, let us briefly touch on the algorithm that we will use to train our embodied agent to navigate. While AllenAct offers us great flexibility to train models using complex pipelines, we will be using a simple pure reinforcement learning approach for this tutorial. More specifically, we will be using DD-PPO, a decentralized and distributed variant of the ubiquitous PPO algorithm. For those unfamiliar with Reinforcement Learning we highly recommend this tutorial by Andrej Karpathy, and this book by Sutton and Barto. Essentially what we are doing is letting our agent explore the environment on its own, rewarding it for taking actions that bring it closer to its goal and penalizing it for actions that take it away from its goal. We then optimize the agent's model to maximize this reward.","title":"Learning algorithm"},{"location":"tutorials/training-a-pointnav-model/#requirements","text":"To train the model on the PointNav task, we need to install the RoboTHOR environment and download the RoboTHOR PointNav dataset The dataset contains a list of episodes with thousands of randomly generated starting positions and target locations for each of the scenes as well as a precomputed cache of distances, containing the shortest path from each point in a scene, to every other point in that scene. This is used to reward the agent for moving closer to the target in terms of geodesic distance - the actual path distance (as opposed to a straight line distance).","title":"Requirements"},{"location":"tutorials/training-a-pointnav-model/#config-file-setup","text":"Now comes the most important part of the tutorial, we are going to write an experiment config file. If this is your first experience with experiment config files in AllenAct, we suggest that you first see our how-to on defining an experiment which will walk you through creating a simplified experiment config file. Unlike a library that can be imported into python, AllenAct is structured as a framework with a runner script called main.py which will run the experiment specified in a config file. This design forces us to keep meticulous records of exactly which settings were used to produce a particular result, which can be very useful given how expensive RL models are to train. The projects/ directory is home to different projects using AllenAct . Currently it is populated with baselines of popular tasks and tutorials. We already have all the code for this tutorial stored in projects/tutorials/training_a_pointnav_model.py . We will be using this file to run our experiments, but you can create a new directory in projects/ and start writing your experiment there. We start off by importing everything we will need: import glob import os from math import ceil from typing import Dict , Any , List , Optional , Sequence import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from torchvision import models from allenact.algorithms.onpolicy_sync.losses import PPO from allenact.algorithms.onpolicy_sync.losses.ppo import PPOConfig from allenact.base_abstractions.experiment_config import ExperimentConfig , MachineParams from allenact.base_abstractions.preprocessor import SensorPreprocessorGraph from allenact.base_abstractions.sensor import SensorSuite from allenact.base_abstractions.task import TaskSampler from allenact.embodiedai.preprocessors.resnet import ResNetPreprocessor from allenact.utils.experiment_utils import ( Builder , PipelineStage , TrainingPipeline , LinearDecay , evenly_distribute_count_into_bins , ) from allenact_plugins.ithor_plugin.ithor_sensors import RGBSensorThor from allenact_plugins.robothor_plugin.robothor_sensors import GPSCompassSensorRoboThor from allenact_plugins.robothor_plugin.robothor_task_samplers import ( PointNavDatasetTaskSampler , ) from allenact_plugins.robothor_plugin.robothor_tasks import PointNavTask from projects.pointnav_baselines.models.point_nav_models import ( ResnetTensorPointNavActorCritic , ) Next we define a new experiment config class: class PointNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ): \"\"\"A Point Navigation experiment configuration in RoboThor.\"\"\" We then define the task parameters. For PointNav, these include the maximum number of steps our agent can take before being reset (this prevents the agent from wandering on forever), and a configuration for the reward function that we will be using. # Task Parameters MAX_STEPS = 500 REWARD_CONFIG = { \"step_penalty\" : - 0.01 , \"goal_success_reward\" : 10.0 , \"failed_stop_reward\" : 0.0 , \"shaping_weight\" : 1.0 , } In this case, we set the maximum number of steps to 500. We give the agent a reward of -0.01 for each action that it takes (this is to encourage it to reach the goal in as few actions as possible), and a reward of 10.0 if the agent manages to successfully reach its destination. If the agent selects the stop action without reaching the target we do not punish it (although this is sometimes useful for preventing the agent from stopping prematurely). Finally, our agent gets rewarded if it moves closer to the target and gets punished if it moves further away. shaping_weight controls how strong this signal should be and is here set to 1.0. These parameters work well for training an agent on PointNav, but feel free to play around with them. Next, we set the parameters of the simulator itself. Here we select a resolution at which the engine will render every frame (640 by 480) and a resolution at which the image will be fed into the neural network (here it is set to a 224 by 224 box). # Simulator Parameters CAMERA_WIDTH = 640 CAMERA_HEIGHT = 480 SCREEN_SIZE = 224 Next, we set the hardware parameters for the training engine. NUM_PROCESSES sets the total number of parallel processes that will be used to train the model. In general, more processes result in faster training, but since each process is a unique instance of the environment in which we are training they can take up a lot of memory. Depending on the size of the model, the environment, and the hardware we are using, we may need to adjust this number, but for a setup with 8 GTX Titans, 60 processes work fine. 60 also happens to be the number of training scenes in RoboTHOR, which allows each process to load only a single scene into memory, saving time and space. TRAINING_GPUS takes the ids of the GPUS on which the model should be trained. Similarly VALIDATION_GPUS and TESTING_GPUS hold the ids of the GPUS on which the validation and testing will occur. During training, a validation process is constantly running and evaluating the current model, to show the progress on the validation set, so reserving a GPU for validation can be a good idea. If our hardware setup does not include a GPU, these fields can be set to empty lists, as the codebase will default to running everything on the CPU with only 1 process. ADVANCE_SCENE_ROLLOUT_PERIOD : Optional [ int ] = None NUM_PROCESSES = 20 TRAINING_GPUS : Sequence [ int ] = [ 0 ] VALIDATION_GPUS : Sequence [ int ] = [ 0 ] TESTING_GPUS : Sequence [ int ] = [ 0 ] Since we are using a dataset to train our model we need to define the path to where we have stored it. If we download the dataset instructed above we can define the path as follows TRAIN_DATASET_DIR = os . path . join ( os . getcwd (), \"datasets/robothor-pointnav/debug\" ) VAL_DATASET_DIR = os . path . join ( os . getcwd (), \"datasets/robothor-pointnav/debug\" ) Next, we define the sensors. RGBSensorThor is the environment's implementation of an RGB sensor. It takes the raw image outputted by the simulator and resizes it, to the input dimensions for our neural network that we specified above. It also performs normalization if we want. GPSCompassSensorRoboThor is a sensor that tracks the point our agent needs to move to. It tells us the direction and distance to our goal at every time step. SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , uuid = \"rgb_lowres\" , ), GPSCompassSensorRoboThor (), ] For the sake of this example, we are also going to be using a preprocessor with our model. In AllenAct the preprocessor abstraction is designed with large models with frozen weights in mind. These models often hail from the ResNet family and transform the raw pixels that our agent observes in the environment, into a complex embedding, which then gets stored and used as input to our trainable model instead of the original image. Most other preprocessing work is done in the sensor classes (as we just saw with the RGB sensor scaling and normalizing our input), but for the sake of efficiency, all neural network preprocessing should use this abstraction. PREPROCESSORS = [ Builder ( ResNetPreprocessor , { \"input_height\" : SCREEN_SIZE , \"input_width\" : SCREEN_SIZE , \"output_width\" : 7 , \"output_height\" : 7 , \"output_dims\" : 512 , \"pool\" : False , \"torchvision_resnet_model\" : models . resnet18 , \"input_uuids\" : [ \"rgb_lowres\" ], \"output_uuid\" : \"rgb_resnet\" , }, ), ] Next, we must define all of the observation inputs that our model will use. These are just the hardcoded ids of the sensors we are using in the experiment. OBSERVATIONS = [ \"rgb_resnet\" , \"target_coordinates_ind\" , ] Finally, we must define the settings of our simulator. We set the camera dimensions to the values we defined earlier. We set rotateStepDegrees to 30 degrees, which means that every time the agent takes a turn action, they will rotate by 30 degrees. We set grid size to 0.25 which means that every time the agent moves forward, it will do so by 0.25 meters. ENV_ARGS = dict ( width = CAMERA_WIDTH , height = CAMERA_HEIGHT , rotateStepDegrees = 30.0 , visibilityDistance = 1.0 , gridSize = 0.25 , ) Now we move on to the methods that we must define to finish implementing an experiment config. Firstly we have a simple method that just returns the name of the experiment. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" Next, we define the training pipeline. In this function, we specify exactly which algorithm or algorithms we will use to train our model. In this simple example, we are using the PPO loss with a learning rate of 3e-4. We specify 250 million steps of training and a rollout length of 30 with the ppo_steps and num_steps parameters respectively. All the other standard PPO parameters are also present in this function. metric_accumulate_interval sets the frequency at which data is accumulated from all the processes and logged while save_interval sets how often we save the model weights and run validation on them. @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 250000000 ) lr = 3e-4 num_mini_batch = 1 update_repeats = 3 num_steps = 30 save_interval = 5000000 log_interval = 1000 gamma = 0.99 use_gae = True gae_lambda = 0.95 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = log_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( ** PPOConfig )}, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) The machine_params method returns the hardware parameters of each process, based on the list of devices we defined above. def machine_params ( self , mode = \"train\" , ** kwargs ): sampler_devices : List [ int ] = [] if mode == \"train\" : workers_per_device = 1 gpu_ids = ( [] if not torch . cuda . is_available () else list ( self . TRAINING_GPUS ) * workers_per_device ) nprocesses = ( 8 if not torch . cuda . is_available () else evenly_distribute_count_into_bins ( self . NUM_PROCESSES , len ( gpu_ids )) ) sampler_devices = list ( self . TRAINING_GPUS ) elif mode == \"valid\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . VALIDATION_GPUS elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TESTING_GPUS else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) sensor_preprocessor_graph = ( SensorPreprocessorGraph ( source_observation_spaces = SensorSuite ( self . SENSORS ) . observation_spaces , preprocessors = self . PREPROCESSORS , ) if mode == \"train\" or ( ( isinstance ( nprocesses , int ) and nprocesses > 0 ) or ( isinstance ( nprocesses , Sequence ) and sum ( nprocesses ) > 0 ) ) else None ) return MachineParams ( nprocesses = nprocesses , devices = gpu_ids , sampler_devices = sampler_devices if mode == \"train\" else gpu_ids , # ignored with > 1 gpu_ids sensor_preprocessor_graph = sensor_preprocessor_graph , ) Now we define the actual model that we will be using. AllenAct offers first-class support for PyTorch, so any PyTorch model that implements the provided ActorCriticModel class will work here. Here we borrow a modelfrom the pointnav_baselines project (which unsurprisingly contains several PointNav baselines). It is a small convolutional network that expects the output of a ResNet as its rgb input followed by a single-layered GRU. The model accepts as input the number of different actions our agent can perform in the environment through the action_space parameter, which we get from the task definition. We also define the shape of the inputs we are going to be passing to the model with observation_space We specify the names of our sensors with goal_sensor_uuid and rgb_resnet_preprocessor_uuid . Finally, we define the size of our RNN with hidden_layer and the size of the embedding of our goal sensor data (the direction and distance to the target) with goal_dims . @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ResnetTensorPointNavActorCritic ( action_space = gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), observation_space = kwargs [ \"sensor_preprocessor_graph\" ] . observation_spaces , goal_sensor_uuid = \"target_coordinates_ind\" , rgb_resnet_preprocessor_uuid = \"rgb_resnet\" , hidden_size = 512 , goal_dims = 32 , ) We also need to define the task sampler that we will be using. This is a piece of code that generates instances of tasks for our agent to perform (essentially starting locations and targets for PointNav). Since we are getting our tasks from a dataset, the task sampler is a very simple code that just reads the specified file and sets the agent to the next starting locations whenever the agent exceeds the maximum number of steps or selects the stop action. @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavDatasetTaskSampler ( ** kwargs ) You might notice that we did not specify the task sampler's arguments, but are rather passing them in. The reason for this is that each process will have its own task sampler, and we need to specify exactly which scenes each process should work with. If we have several GPUS and many scenes this process of distributing the work can be rather complicated so we define a few helper functions to do just this. @staticmethod def _partition_inds ( n : int , num_parts : int ): return np . round ( np . linspace ( 0 , n , num_parts + 1 , endpoint = True )) . astype ( np . int32 ) def _get_sampler_args_for_scene_split ( self , scenes_dir : str , process_ind : int , total_processes : int , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: path = os . path . join ( scenes_dir , \"*.json.gz\" ) scenes = [ scene . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for scene in glob . glob ( path )] if len ( scenes ) == 0 : raise RuntimeError ( ( \"Could find no scene dataset information in directory {} .\" \" Are you sure you've downloaded them? \" \" If not, see https://allenact.org/installation/download-datasets/ information\" \" on how this can be done.\" ) . format ( scenes_dir ) ) if total_processes > len ( scenes ): # oversample some scenes -> bias if total_processes % len ( scenes ) != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisible by the number of scenes\" ) scenes = scenes * int ( ceil ( total_processes / len ( scenes ))) scenes = scenes [: total_processes * ( len ( scenes ) // total_processes )] else : if len ( scenes ) % total_processes != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisor of the number of scenes\" ) inds = self . _partition_inds ( len ( scenes ), total_processes ) return { \"scenes\" : scenes [ inds [ process_ind ] : inds [ process_ind + 1 ]], \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), \"seed\" : seeds [ process_ind ] if seeds is not None else None , \"deterministic_cudnn\" : deterministic_cudnn , \"rewards_config\" : self . REWARD_CONFIG , } The very last things we need to define are the sampler arguments themselves. We define them separately for a train, validation, and test sampler, but in this case, they are almost the same. The arguments need to include the location of the dataset and distance cache as well as the environment arguments for our simulator, both of which we defined above and are just referencing here. The only consequential differences between these task samplers are the path to the dataset we are using (train or validation) and whether we want to loop over the dataset or not (we want this for training since we want to train for several epochs, but we do not need this for validation and testing). Since the test scenes of RoboTHOR are private we are also testing on our validation set. def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( os . path . join ( self . TRAIN_DATASET_DIR , \"episodes\" ), process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . TRAIN_DATASET_DIR res [ \"loop_dataset\" ] = True res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) res [ \"allow_flipping\" ] = True return res def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( os . path . join ( self . VAL_DATASET_DIR , \"episodes\" ), process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) return res def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( os . path . join ( self . VAL_DATASET_DIR , \"episodes\" ), process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) return res This is it! If we copy all of the code into a file we should be able to run our experiment!","title":"Config File Setup"},{"location":"tutorials/training-a-pointnav-model/#training-model-on-debug-dataset","text":"We can test if our installation worked properly by training our model on a small dataset of 4 episodes. This should take about 20 minutes on a computer with a NVIDIA GPU. We can now train a model by running: PYTHONPATH = . python allenact/main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> If using the same configuration as we have set up, the following command should work: PYTHONPATH = . python allenact/main.py training_a_pointnav_model -o storage/robothor-pointnav-rgb-resnet-resnet -b projects/tutorials If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this:","title":"Training Model On Debug Dataset"},{"location":"tutorials/training-a-pointnav-model/#training-model-on-full-dataset","text":"We can also train the model on the full dataset by changing back our dataset path and running the same command as above. But be aware, training this takes nearly 2 days on a machine with 8 GPU.","title":"Training Model On Full Dataset"},{"location":"tutorials/training-a-pointnav-model/#testing-model","text":"To test the performance of a model please refer to this tutorial .","title":"Testing Model"},{"location":"tutorials/training-a-pointnav-model/#conclusion","text":"In this tutorial, we learned how to create a new PointNav experiment using AllenAct . There are many simple and obvious ways to modify the experiment from here - changing the model, the learning algorithm and the environment each requires very few lines of code changed in the above file, allowing us to explore our embodied ai research ideas across different frameworks with ease.","title":"Conclusion"},{"location":"tutorials/training-pipelines/","text":"Tutorial: IL to RL with a training pipeline #","title":"Tutorial: IL to RL with a training pipeline"},{"location":"tutorials/training-pipelines/#tutorial-il-to-rl-with-a-training-pipeline","text":"","title":"Tutorial: IL to RL with a training pipeline"},{"location":"tutorials/transfering-to-a-different-environment-framework/","text":"Tutorial: Swapping in a new environment # Note The provided paths in this tutorial assume you have installed the full library . Introduction # This tutorial was designed as a continuation of the Robothor PointNav Tutorial and explains how to modify the experiment config created in that tutorial to work with the iTHOR and Habitat environments. Cross-platform support is one of the key design goals of allenact . This is achieved through a total decoupling of the environment code from the engine, model and algorithm code, so that swapping in a new environment is as plug and play as possible. Crucially we will be able to run a model on different environments without touching the model code at all, which will allow us to train neural networks in one environment and test them in another. RoboTHOR to iTHOR # Since both the RoboTHOR and the iTHOR environment stem from the same family and are developed by the same organization, switching between the two is incredibly easy. We only have to change the path parameter to point to an iTHOR dataset rather than the RoboTHOR one. # Dataset Parameters TRAIN_DATASET_DIR = \"datasets/ithor-pointnav/train\" VAL_DATASET_DIR = \"datasets/ithor-pointnav/val\" We also have to download the iTHOR-PointNav dataset, following these instructions . We might also want to modify the tag method to accurately reflect our config but this will not change the behavior at all and is merely a bookkeeping convenience. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" RoboTHOR to Habitat # To train experiments using the Habitat framework we need to install it following these instructions . Since the roboTHOR and Habitat simulators are sufficiently different and have different parameters to configure this transformation takes a bit more effort, but we only need to modify the environment config and TaskSampler (we have to change the former because the habitat simulator accepts a different format of configuration and the latter because the habitat dataset is formatted differently and thus needs to be parsed differently.) As part of our environment modification, we need to switch from using RoboTHOR sensors to using Habitat sensors. The implementation of sensors we provide offer an uniform interface across all the environments so we simply have to swap out our sensor classes: SENSORS = [ DepthSensorHabitat ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_normalization = True , ), TargetCoordinatesSensorHabitat ( coordinate_dims = 2 ), ] Next we need to define the simulator config: CONFIG = get_habitat_config ( \"configs/gibson.yaml\" ) CONFIG . defrost () CONFIG . NUM_PROCESSES = NUM_PROCESSES CONFIG . SIMULATOR_GPU_IDS = TRAIN_GPUS CONFIG . DATASET . SCENES_DIR = HABITAT_SCENE_DATASETS_DIR CONFIG . DATASET . POINTNAVV1 . CONTENT_SCENES = [ \"*\" ] CONFIG . DATASET . DATA_PATH = TRAIN_SCENES CONFIG . SIMULATOR . AGENT_0 . SENSORS = [ \"RGB_SENSOR\" ] CONFIG . SIMULATOR . RGB_SENSOR . WIDTH = CAMERA_WIDTH CONFIG . SIMULATOR . RGB_SENSOR . HEIGHT = CAMERA_HEIGHT CONFIG . SIMULATOR . TURN_ANGLE = 30 CONFIG . SIMULATOR . FORWARD_STEP_SIZE = 0.25 CONFIG . ENVIRONMENT . MAX_EPISODE_STEPS = MAX_STEPS CONFIG . TASK . TYPE = \"Nav-v0\" CONFIG . TASK . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SENSORS = [ \"POINTGOAL_WITH_GPS_COMPASS_SENSOR\" ] CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . GOAL_FORMAT = \"POLAR\" CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . DIMENSIONALITY = 2 CONFIG . TASK . GOAL_SENSOR_UUID = \"pointgoal_with_gps_compass\" CONFIG . TASK . MEASUREMENTS = [ \"DISTANCE_TO_GOAL\" , \"SUCCESS\" , \"SPL\" ] CONFIG . TASK . SPL . TYPE = \"SPL\" CONFIG . TASK . SPL . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SUCCESS . SUCCESS_DISTANCE = 0.2 CONFIG . MODE = \"train\" This CONFIG object holds very similar values to the ones ENV_ARGS held in the RoboTHOR example. We decided to leave this way of passing in configurations exposed to the user to offer maximum customization of the underlying environment. Finally we need to replace the task sampler and its argument generating functions: # Define Task Sampler from allenact_plugins.habitat_plugin.habitat_task_samplers import PointNavTaskSampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavTaskSampler ( ** kwargs ) def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TRAIN_CONFIGS_PER_PROCESS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . CONFIG . clone () config . defrost () config . DATASET . DATA_PATH = self . VALID_SCENES_PATH config . MODE = \"validate\" config . freeze () return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TEST_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } As we can see this code looks very similar as well, we simply need to pass slightly different parameters. Conclusion # In this tutorial, we learned how to modify our experiment configurations to work with different environments. By providing a high level of modularity and out-of-the-box support for both Habitat and THOR , two of the most popular embodied frameworks out there AllenAct hopes to give researchers the ability to validate their results across many platforms and help guide them towards genuine progress. The source code for this tutorial can be found in /projects/framework_transfer_tutorial .","title":"Swapping environments"},{"location":"tutorials/transfering-to-a-different-environment-framework/#tutorial-swapping-in-a-new-environment","text":"Note The provided paths in this tutorial assume you have installed the full library .","title":"Tutorial: Swapping in a new environment"},{"location":"tutorials/transfering-to-a-different-environment-framework/#introduction","text":"This tutorial was designed as a continuation of the Robothor PointNav Tutorial and explains how to modify the experiment config created in that tutorial to work with the iTHOR and Habitat environments. Cross-platform support is one of the key design goals of allenact . This is achieved through a total decoupling of the environment code from the engine, model and algorithm code, so that swapping in a new environment is as plug and play as possible. Crucially we will be able to run a model on different environments without touching the model code at all, which will allow us to train neural networks in one environment and test them in another.","title":"Introduction"},{"location":"tutorials/transfering-to-a-different-environment-framework/#robothor-to-ithor","text":"Since both the RoboTHOR and the iTHOR environment stem from the same family and are developed by the same organization, switching between the two is incredibly easy. We only have to change the path parameter to point to an iTHOR dataset rather than the RoboTHOR one. # Dataset Parameters TRAIN_DATASET_DIR = \"datasets/ithor-pointnav/train\" VAL_DATASET_DIR = \"datasets/ithor-pointnav/val\" We also have to download the iTHOR-PointNav dataset, following these instructions . We might also want to modify the tag method to accurately reflect our config but this will not change the behavior at all and is merely a bookkeeping convenience. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\"","title":"RoboTHOR to iTHOR"},{"location":"tutorials/transfering-to-a-different-environment-framework/#robothor-to-habitat","text":"To train experiments using the Habitat framework we need to install it following these instructions . Since the roboTHOR and Habitat simulators are sufficiently different and have different parameters to configure this transformation takes a bit more effort, but we only need to modify the environment config and TaskSampler (we have to change the former because the habitat simulator accepts a different format of configuration and the latter because the habitat dataset is formatted differently and thus needs to be parsed differently.) As part of our environment modification, we need to switch from using RoboTHOR sensors to using Habitat sensors. The implementation of sensors we provide offer an uniform interface across all the environments so we simply have to swap out our sensor classes: SENSORS = [ DepthSensorHabitat ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_normalization = True , ), TargetCoordinatesSensorHabitat ( coordinate_dims = 2 ), ] Next we need to define the simulator config: CONFIG = get_habitat_config ( \"configs/gibson.yaml\" ) CONFIG . defrost () CONFIG . NUM_PROCESSES = NUM_PROCESSES CONFIG . SIMULATOR_GPU_IDS = TRAIN_GPUS CONFIG . DATASET . SCENES_DIR = HABITAT_SCENE_DATASETS_DIR CONFIG . DATASET . POINTNAVV1 . CONTENT_SCENES = [ \"*\" ] CONFIG . DATASET . DATA_PATH = TRAIN_SCENES CONFIG . SIMULATOR . AGENT_0 . SENSORS = [ \"RGB_SENSOR\" ] CONFIG . SIMULATOR . RGB_SENSOR . WIDTH = CAMERA_WIDTH CONFIG . SIMULATOR . RGB_SENSOR . HEIGHT = CAMERA_HEIGHT CONFIG . SIMULATOR . TURN_ANGLE = 30 CONFIG . SIMULATOR . FORWARD_STEP_SIZE = 0.25 CONFIG . ENVIRONMENT . MAX_EPISODE_STEPS = MAX_STEPS CONFIG . TASK . TYPE = \"Nav-v0\" CONFIG . TASK . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SENSORS = [ \"POINTGOAL_WITH_GPS_COMPASS_SENSOR\" ] CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . GOAL_FORMAT = \"POLAR\" CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . DIMENSIONALITY = 2 CONFIG . TASK . GOAL_SENSOR_UUID = \"pointgoal_with_gps_compass\" CONFIG . TASK . MEASUREMENTS = [ \"DISTANCE_TO_GOAL\" , \"SUCCESS\" , \"SPL\" ] CONFIG . TASK . SPL . TYPE = \"SPL\" CONFIG . TASK . SPL . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SUCCESS . SUCCESS_DISTANCE = 0.2 CONFIG . MODE = \"train\" This CONFIG object holds very similar values to the ones ENV_ARGS held in the RoboTHOR example. We decided to leave this way of passing in configurations exposed to the user to offer maximum customization of the underlying environment. Finally we need to replace the task sampler and its argument generating functions: # Define Task Sampler from allenact_plugins.habitat_plugin.habitat_task_samplers import PointNavTaskSampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavTaskSampler ( ** kwargs ) def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TRAIN_CONFIGS_PER_PROCESS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . CONFIG . clone () config . defrost () config . DATASET . DATA_PATH = self . VALID_SCENES_PATH config . MODE = \"validate\" config . freeze () return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TEST_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } As we can see this code looks very similar as well, we simply need to pass slightly different parameters.","title":"RoboTHOR to Habitat"},{"location":"tutorials/transfering-to-a-different-environment-framework/#conclusion","text":"In this tutorial, we learned how to modify our experiment configurations to work with different environments. By providing a high level of modularity and out-of-the-box support for both Habitat and THOR , two of the most popular embodied frameworks out there AllenAct hopes to give researchers the ability to validate their results across many platforms and help guide them towards genuine progress. The source code for this tutorial can be found in /projects/framework_transfer_tutorial .","title":"Conclusion"}]}